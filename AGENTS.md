# EMBARGO: LLM-Optimized Codebase Dependency Graph

**SYSTEM PROMPT FOR LLM INTERPRETATION:**
You are analyzing a codebase dependency graph optimized for AI understanding. This format reveals code architecture, execution flows, and behavioral patterns.

## INTERPRETATION KEY

### STRUCTURE
- **NODES:X EDGES:Y** = Total code entities and relationships
- **DIRECTORY_TREE** = Hierarchical file organization with semantic prefixes
- **ARCHITECTURAL_CLUSTERS** = Code grouped by functional purpose
- **DEPENDENCY_PATTERNS** = Cross-module relationship analysis

### BEHAVIORAL NOTATION
- **filename.rs→[...]** = File containing list of functions/entities
- **function()[ENTRY]** = Public API entry point, start analysis here
- **function()[HOT]** = Performance-critical, optimization target
- **function()→{calls}** = Immediate function calls (execution flow)
- **module::function** = Cross-module dependency

### ANALYSIS GUIDANCE
1. **Entry Points**: Start with [ENTRY] functions to understand public APIs
2. **Execution Flow**: Follow →{calls} to trace code execution paths
3. **Hot Paths**: Focus [HOT] functions for performance analysis
4. **Architecture**: Use clusters to understand system organization
5. **Dependencies**: Cross-cluster flows show coupling patterns

### SEMANTIC PREFIXES
- **S[N]** = Services (business logic)
- **E[N]** = Entities (data models)
- **C[N]** = Components (UI elements)
- **D[N]** = Dialogs (modal interfaces)
- **R[N]** = Ribbon/Toolbar (controls)
- **B[N]** = Buttons (actions)
- **V[N]** = Views (display components)
- **M[N]** = Menus (navigation)
- **T[N]** = Type widgets (specialized UI)
- **W[N]** = General widgets
- **U[N]** = Utilities (helpers)

### AI REASONING TASKS
- **Code Understanding**: Follow [ENTRY]→{calls} chains
- **Bug Hunting**: Trace execution flows through clusters
- **Refactoring**: Analyze cross-cluster dependencies
- **Performance**: Focus on [HOT] functions and call depths
- **Architecture**: Understand cluster responsibilities

---

# CODE_GRAPH
NODES:2252 EDGES:1067

## DIRECTORY_TREE
ROOT: torchtitan/
├─ compat/ → U[2]
│  └─ mosaic/ → U[2]
├─ components/ → C[19]
│  ├─ ft/ → C[7]
│  │  ├─ config/ → C[2]
│  │  └─ diloco/ → C[3]
│  └─ quantization/ → C[4]
├─ config/ → U[3]
├─ datasets/ → U[2]
├─ distributed/ → U[7]
├─ experiments/ → E[1] TST[8] U[107]
│  ├─ deepseek_v3/ → U[25]
│  │  ├─ infra/ → U[1]
│  │  ├─ symm_mem_recipes/ → U[4]
│  │  ├─ tokenizers/ → U[1]
│  │  ├─ train_configs/ → U[1]
│  │  └─ unit_testing/ → U[4]
│  ├─ flux/ → TST[3] U[19]
│  │  ├─ dataset/ → U[2]
│  │  ├─ inference/ → U[1]
│  │  ├─ infra/ → U[1]
│  │  ├─ model/ → U[7]
│  │  ├─ scripts/ → U[1]
│  │  └─ tests/ → TST[3]
│  │     ├─ assets/ → TST[1]
│  │     │  └─ cc12m_test/ → TST[1]
│  │     └─ unit_tests/ → TST[1]
│  ├─ forge/ → U[5]
│  ├─ kernels/ → TST[1] U[18]
│  │  ├─ moe/ → TST[1] U[3]
│  │  │  └─ unit_tests/ → TST[1]
│  │  ├─ triton_contiguous_group_gemm/ → U[6]
│  │  └─ triton_mg_group_gemm/ → U[9]
│  │     └─ torchao_pr/ → U[7]
│  ├─ llama4/ → U[7]
│  │  ├─ infra/ → U[1]
│  │  ├─ model/ → U[3]
│  │  └─ scripts/ → U[2]
│  ├─ mosaic/ → E[1] U[5]
│  │  ├─ configs/ → U[1]
│  │  ├─ dataloader/ → U[2]
│  │  └─ models/ → E[1]
│  ├─ multimodal/ → TST[2] U[8]
│  │  ├─ tests/ → TST[2]
│  │  └─ tokenizer/ → U[1]
│  ├─ qwen3/ → U[5]
│  │  ├─ infra/ → U[1]
│  │  └─ model/ → U[3]
│  ├─ simple_fsdp/ → TST[2] U[4]
│  │  └─ tests/ → TST[2]
│  └─ vlm/ → U[11]
│     ├─ assets/ → U[1]
│     ├─ datasets/ → U[5]
│     │  └─ utils/ → U[3]
│     ├─ infra/ → U[1]
│     └─ model/ → U[3]
├─ models/ → E[15]
│  ├─ deepseek_v3/ → E[6]
│  │  ├─ infra/ → E[1]
│  │  └─ model/ → E[4]
│  ├─ llama3/ → E[6]
│  │  ├─ infra/ → E[2]
│  │  └─ model/ → E[3]
│  └─ llama3_ft/ → E[1]
├─ protocols/ → U[5]
└─ tools/ → U[3]

## ARCHITECTURAL_CLUSTERS

### DATA_MODELS
NODES:223 CALL_DEPTH:5

__init__.py→[_get_mosaic_llama3_spec(void),get_train_spec(void),get_train_spec(void),get_train_spec(void)] args.py→[] attention.py→[__init__((self,attn_mask_type: str,fixed_block_size: int | None = None))[CTOR,DUNDER],forward((self,q: torch.Tensor,k: torch.Tensor,v: torch.Tensor,scale: float | None = None,)),__init__((self,attn_mask_type: str))[CTOR,DUNDER],forward((self,q: torch.Tensor,k: torch.Tensor,v: torch.Tensor,scale: float | None = None,)),build_attention((use_flex_attn: bool,attn_mask_type: str,fixed_block_size: int | None = None))[HOT],init_attention_mask((batch: torch.Tensor,eos_id: int | None,cp_mesh: torch.distributed.device_mesh.DeviceMesh | None = None,))] model.py→[forward((self,x: torch.Tensor,freqs_cis: torch.Tensor,))→{repeat_kv,repeat_kv,apply_rotary_emb},forward((self,x: torch.Tensor,freqs_cis: torch.Tensor,))→{apply_rotary_emb,apply_rotary_emb},__init__((self,layer_id: int,model_args: DeepSeekV3ModelArgs))[CTOR,DUNDER]→{math::rope,math::attention},__init__((self,model_args: DeepSeekV3ModelArgs))[CTOR,DUNDER]→{precompute_freqs_cis,lr_scheduler::step},apply_rotary_emb((xq: torch.Tensor,xk: torch.Tensor,freqs_cis: torch.Tensor,))→{reshape_for_broadcast},__init__((self,model_args: TransformerModelArgs))[CTOR,DUNDER]→{attention::build_attention},__init__((self,model_args: DeepSeekV3ModelArgs))[CTOR,DUNDER]→{attention::build_attention},__init__((self,layer_id: int,model_args: TransformerModelArgs))[CTOR,DUNDER]→{math::attention},__init__((self,model_args: TransformerModelArgs))[CTOR,DUNDER]→{lr_scheduler::step},init_weights((self,buffer_device: torch.device | None = None))→{precompute_freqs_cis},_precompute_freqs_cis((self))[HOT]→{precompute_freqs_cis},precompute_freqs_cis((dim: int,end: int,theta: float = 10000.0))[HOT],precompute_freqs_cis((args: DeepSeekV3ModelArgs))[HOT],reshape_for_broadcast((freqs_cis: torch.Tensor,x: torch.Tensor)),repeat_kv((x: torch.Tensor,n_rep: int)),apply_rotary_emb((x: torch.Tensor,freqs_cis: torch.Tensor)),init_weights((self,init_std: float)),__init__((self,dim: int,hidden_dim: int,multiple_of: int,ffn_dim_multiplier: float | None,))[CTOR,DUNDER],forward((self,x)),init_weights((self,init_std: float)),init_weights((self,init_std: float)),forward((self,x: torch.Tensor,freqs_cis: torch.Tensor,)),forward((self,x: torch.Tensor,freqs_cis: torch.Tensor)),init_weights((self)),init_weights((self,buffer_device: torch.device)),init_weights((self,buffer_device: torch.device | None = None,)),forward((self,tokens: torch.Tensor,input_batch: torch.Tensor | None = None,)),forward((self,tokens: torch.Tensor,input_batch: torch.Tensor | None = None,))] moe.py→[__init__((self,dim: int,hidden_dim: int,))[CTOR,DUNDER],forward((self,x: torch.Tensor)),init_weights((self,init_std: float = 0.02)),__init__((self,dim: int,hidden_dim: int,num_experts: int,use_grouped_mm: bool,))[CTOR,DUNDER],forward((self,x: torch.Tensor,num_tokens_per_expert: torch.Tensor,)),init_weights((self,init_std: float)),__init__((self,dim: int,num_experts: int,top_k: int,score_func: Literal["softmax","sigmoid"],route_norm: bool,route_scale: float,))[CTOR,DUNDER],forward((self,x: torch.Tensor,expert_bias: torch.Tensor | None = None)),init_weights((self,init_std: float)),__init__((self,num_experts: int,top_k: int))[CTOR,DUNDER],forward((self,top_scores: torch.Tensor,selected_experts_indices: torch.Tensor,)),__init__((self,moe_args: MoEArgs,dim: int,hidden_dim: int))[CTOR,DUNDER],forward((self,x: torch.Tensor)),init_weights((self,init_std: float,buffer_device: torch.device,))] parallelize.py→[parallelize_llama((model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,))→{apply_ddp,apply_fsdp,apply_compile,apply_tp,activation_checkpoint::apply_ac,tensor_parallel::maybe_enable_async_tp},parallelize_deepseekv3((model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,))→
{apply_ddp,apply_fsdp,apply_compile,apply_non_moe_tp,activation_checkpoint::apply_ac,parallelize::apply_moe_ep_tp},apply_tp((model: nn.Module,tp_mesh: DeviceMesh,loss_parallel: bool,enable_float8_tensorwise_tp: bool,)),apply_non_moe_tp((model: nn.Module,tp_mesh: DeviceMesh,loss_parallel: bool,enable_float8_tensorwise_tp: bool,)),apply_compile((model: nn.Module)),apply_fsdp((model: nn.Module,dp_mesh: DeviceMesh,param_dtype: torch.dtype,reduce_dtype: torch.dtype,pp_enabled: bool,cpu_offload: bool = False,reshard_after_forward_policy: str = "default",)),apply_ddp((model: nn.Module,dp_mesh: DeviceMesh,enable_compile: bool,enable_compiled_autograd: bool,))] pipeline.py→[pipeline_llama((model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,device: torch.device,model_args: BaseModelArgs,parallelize_fn: ParallelizeFunction,loss_fn: LossFunction,))→{pipeline_parallel::build_pipeline_schedule,pipeline_parallel::pipeline_module_split,pipeline_parallel::generate_llm_fqn_per_model_part}] quantization.py→[dequantize_from_fp8((weight: torch.Tensor,scale_inv: torch.Tensor,dtype=torch.bfloat16,BLOCK_SIZE: int = BLOCK_SIZE,))→{calculate_scale_shape,train::main,train::main},calculate_scale_shape((weight: torch.Tensor,BLOCK_SIZE: int = BLOCK_SIZE))] state_dict_adapter.py→[_get_local_experts_weights((self,abstract_key: str,titan_abstract_key: str,layer_id: str,grouped_expert_weight: torch.Tensor,))→{math::rope},_dequantize((self,state_dict: dict[str,Any]))→{quantization::dequantize_from_fp8},_add_quantization_scale_inv_tensors((self,state_dict: dict[str,Any]))→{quantization::calculate_scale_shape},__init__((self,model_args: TransformerModelArgs,hf_assets_path: str | None,))[CTOR,DUNDER],__init__((self,model_args: DeepSeekV3ModelArgs,hf_assets_path: str | None,))[CTOR,DUNDER],_permute((self,w,n_heads_arg,dim1=None,dim2=None)),_reverse_permute((self,w,n_heads_arg,dim1=None,dim2=None)),to_hf((self,state_dict: dict[str,Any])),_calculate_strided_shard_shard_indices((self,strided_shard_dim_degree: int,strided_shard_dim_rank: int,shard_dim_degree: int,shard_dim_rank: int,dim_size_to_split: int,)),from_hf((self,hf_state_dict: dict[str,Any])),_caculate_indices_from_placements((self,dim: int,dim_size: int,dtensor_placements: tuple,device_mesh: DeviceMesh,)),_concatenate_expert_weights_dtensor((self,expert_weights_by_layer: dict[str,dict[str,dict[int,torch.Tensor]]],abstract_key: str,layer_num: str,device_mesh: DeviceMesh,)),_split_experts_weights((self,weight: torch.Tensor,n_experts: int)),_concatenate_expert_weights((self,expert_weights_by_layer: dict[str,dict[str,dict[int,torch.Tensor]]],abstract_key: str,layer_num: str,n_experts: int,)),to_hf((self,state_dict: dict[str,Any])),from_hf((self,hf_state_dict: dict[str,Any]))] 
### TESTS
NODES:89 CALL_DEPTH:5

combine.py→[test_token_combine(void)[TEST]] debug.py→[test_small(void)[TEST]→{debug::verify_results,debug::pytorch_reference,cg_forward::cg_grouped_gemm_forward,debug::create_aligned_test_data},test_medium(void)[TEST]→{debug::verify_results,debug::pytorch_reference,cg_forward::cg_grouped_gemm_forward,debug::create_aligned_test_data},test_large(void)[TEST]→{debug::verify_results,debug::pytorch_reference,cg_forward::cg_grouped_gemm_forward,debug::create_aligned_test_data}] dispatch.py→[test_token_dispatch(void)[TEST]] dsgemm_unit_testing.py→[test_m_grouped_gemm_contiguous_with_empty_groups(void)[TEST]→{dsgemm_unit_testing::compute_reference_with_scaling,dsgemm_utils::get_col_major_tma_aligned_tensor,dsgemm_unit_testing::per_block_cast_to_fp8,dsgemm_unit_testing::per_token_cast_to_fp8,dsgemm_unit_testing::create_m_indices_fast},test_m_grouped_gemm_contiguous_all_empty_but_one(void)[TEST]→{dsgemm_unit_testing::compute_reference_with_scaling,dsgemm_utils::get_col_major_tma_aligned_tensor,dsgemm_unit_testing::per_block_cast_to_fp8,dsgemm_unit_testing::per_token_cast_to_fp8,dsgemm_unit_testing::create_m_indices_fast},test_m_grouped_gemm_contiguous_with_scaling_edge_cases(void)[TEST]→{dsgemm_unit_testing::compute_reference_with_scaling,dsgemm_utils::get_col_major_tma_aligned_tensor,dsgemm_unit_testing::per_block_cast_to_fp8,dsgemm_unit_testing::per_token_cast_to_fp8,dsgemm_unit_testing::create_m_indices_fast}] fast_debug_ao.py→[test_multiple_deepseek_configs(void)[TEST]→{reference_utils::analyze_tensor_differences,reference_utils::analyze_tensor_differences,reference_utils::compute_reference_backward,mg_grouped_gemm::grouped_gemm_backward,reference_utils::analyze_tensor_differences,reference_utils::compute_reference_forward},test_backward_pass(void)[TEST]→{reference_utils::analyze_tensor_differences,reference_utils::analyze_tensor_differences,reference_utils::compute_reference_backward,mg_grouped_gemm::grouped_gemm_backward,mg_grouped_gemm::grouped_gemm_forward},test_forward_pass(void)[TEST]→{reference_utils::analyze_tensor_differences,reference_utils::compute_reference_forward,mg_grouped_gemm::grouped_gemm_forward}] indices.py→[test_with_zero_tokens(void)[TEST]→{indices::generate_permute_indices,indices::generate_permute_indices}] integration_tests.py→[main(void)[ENTRY]→{unit_test_cg::run_tests},main(void)[ENTRY]→{unit_test_cg::run_tests},run_single_test((test_flavor: OverrideDefinitions,full_path: str,output_dir: str))→{train::main},run_tests((args,test_list: list[OverrideDefinitions]))→{run_single_test},build_simple_fsdp_test_list(void)[HOT],build_flux_test_list(void)[HOT]] pack_test_dataset.py→[pack_wds_dataset((tar_destination,source_folder,number_of_samples))→{math::rope,math::rope,math::rope,math::rope}] permute_indices_testing.py→[test_fixed_total_experts_varying_ranks((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_fixed_total_experts_varying_ranks((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_different_block_sizes_with_fixed_experts((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_different_block_sizes_with_fixed_experts((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_edge_cases_with_fixed_experts((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_edge_cases_with_fixed_experts((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_max_blocks_with_large_experts((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_max_blocks_with_large_experts((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_extreme_max_blocks_limit((self))[TEST]→
{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_extreme_max_blocks_limit((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu}] test_create_m_indices.py→[test_create_indices_from_offsets_nosync(void)[TEST]→{dsgemm_utils::create_indices_from_offsets_nosync,dsgemm_utils::create_indices_from_offsets_nosync}] test_flux_dataloader.py→[test_load_dataset((self))[TEST]→{lr_scheduler::step,flux_dataset::build_flux_dataloader,lr_scheduler::step,flux_dataset::build_flux_dataloader,lr_scheduler::step,lr_scheduler::step},setUp((self)),tearDown((self))] test_multimodal_model.py→[test_llama_mm_vision_encoder((self))[TEST]] test_numerics.py→[run_simple_fsdp((self,model,inputs,labels,epoch=20))→{simple_fsdp::data_parallel},init_test((self)),get_input((self)),run_fsdp2((self,model,inputs,labels,epoch=20)),test_replicate_convergence((self))[TEST],test_fullyshard_convergence((self))[TEST],test_hybridshard_convergence((self))[TEST]] test_utils.py→[fixed_init_tensor((shape: torch.Size,min_val: Union[float,int] = 0.0,max_val: Union[float,int] = 1.0,nonlinear: bool = False,dtype: torch.dtype = torch.float,))] unit_test_backwards.py→[test_mg_dx((self))[TEST]→{reference_utils::analyze_tensor_differences,reference_utils::compute_reference_backward,mg_grouped_gemm::grouped_gemm_backward,mg_grouped_gemm::grouped_gemm_forward},test_mg_dw((self))[TEST]→{reference_utils::analyze_tensor_differences,reference_utils::compute_reference_backward,mg_grouped_gemm::grouped_gemm_backward,mg_grouped_gemm::grouped_gemm_forward},test_mg_grouped_gemm_backward_bf16((self))[TEST],test_mg_grouped_gemm_backward_deepseek_shapes((self))[TEST]] unit_test_cg.py→[test_forward_deepseek_shapes((self))[TEST],test_backward_deepseek_shapes((self))[TEST],test_forward_performance_deepseek((self))[TEST],test_backward_performance_deepseek((self))[TEST]] unit_test_forwards.py→[test_mg_grouped_gemm_bf16((self))[TEST],test_mg_grouped_gemm_deepseek_shapes((self))[TEST]] 
### UI_COMPONENTS
NODES:299 CALL_DEPTH:3

__init__.py→[] checkpoint.py→[_get_state_dict((self))→{train::main},load_state_dict((self,state_dict: dict[str,Any]))→{train::main},_find_load_step((self,folder: str = ""))→{train::main},__init__((self,model: nn.Module | list[nn.Module]))[CTOR,DUNDER],state_dict((self)),purge_thread((purge_queue: queue.Queue)),__init__((self,dataloader: BaseDataLoader | None,model_parts: list[nn.Module],optimizers: OptimizersContainer,lr_schedulers: LRSchedulersContainer,states: dict[str,Any],checkpoint_config: CheckpointConfig,sd_adapter: BaseStateDictAdapter | None,base_folder: str = "",ft_manager: FTManager | None = None,))[CTOR,DUNDER],__del__((self))[DUNDER],close((self)),dcp_load((self,state_dict: dict[str,Any],checkpoint_id: str,from_hf: bool,)),maybe_wait_for_staging((self)),_ft_folder((self)),_create_checkpoint_id((self,step: int,folder: str = "")),_ft_save((self,step: int)),_ft_load((self)),_flattened_model_states_sd((self,state_dict: dict[str,Any] | None = None)),_states_to_load((self,model_only: bool)),_save_last_step((self,curr_step: int)),_should_save((self,curr_step: int,last_step: bool = False)),_async_wait((self)),_purge_stale_checkpoints((self))] dataloader.py→[__init__((self,dataset: IterableDataset,dp_rank: int,dp_world_size: int,batch_size: int,collate_fn: Callable | None = None,))[CTOR,DUNDER],state_dict((self)),load_state_dict((self,state_dict: dict[str,Any]))] float8.py→[__init__((self,job_config: JobConfig,parallel_dims: ParallelDims))[CTOR,DUNDER]→{expert_parallel::set_token_group_alignment_size_m,utils::has_cuda_capability},__init__((self,job_config: JobConfig,parallel_dims: ParallelDims))[CTOR,DUNDER]→{utils::has_cuda_capability},_init_filter_fn((self,float8_config: Float8Dense)),convert((self,model: nn.Module)),post_optimizer_hook((self,model: nn.Module | list[nn.Module])),convert((self,model: nn.Module)),post_optimizer_hook((self,model: nn.Module | list[nn.Module]))] job_config.py→[] loss.py→[rescale_accumulated_loss((unwrapped_loss_fn,accumulation_steps))→{rescale_accumulated_loss},cross_entropy_loss((pred: torch.Tensor,labels: torch.Tensor)),build_cross_entropy_loss((job_config: JobConfig))[HOT],__init__((self,unwrapped_loss_fn,accumulation_steps))[CTOR,DUNDER],__call__((self,*args,**kwargs))[DUNDER]] lr_scheduler.py→[__iter__((self))[DUNDER]→{step},__init__((self,optimizers: OptimizersContainer,lr_lambda: Callable))[CTOR,DUNDER],__len__((self))[DUNDER],step((self)),state_dict((self)),load_state_dict((self,state_dict: dict[str,Any])),build_lr_schedulers((optimizers: OptimizersContainer,lr_scheduler_config: LRSchedulerConfig,training_steps: int,))[HOT]] manager.py→[__init__((self,ft_config: FTConfig,))[CTOR,DUNDER],get_dp_info((self,dp_degree: int,dp_rank: int)),maybe_set_all_reduce_hook((self,model_parts: list[torch.nn.Module])),maybe_semi_sync_training((ft_config: FTConfig,ft_manager: FTManager,model: torch.nn.Module,n_layers: int,optimizer: torch.optim.Optimizer,fragment_fn: Optional[Callable[...,list[nn.Module]]] = None,))] metrics.py→[_build_metric_logger((job_config: JobConfig,parallel_dims: ParallelDims,tag: str | None = None))[HOT]→{_get_metrics_rank,lr_scheduler::step},__init__((self,job_config: JobConfig,parallel_dims: ParallelDims,tag: str | None = None,))[CTOR,DUNDER]→{build_device_memory_monitor,_build_metric_logger},ensure_pp_loss_visible((parallel_dims: ParallelDims,job_config: JobConfig,color: Color))→
{lr_scheduler::step},__init__((self,device: str = f"{device_type}:0"))[CTOR,DUNDER],_to_gib((self,memory_in_bytes)),_to_pct((self,memory)),get_peak_stats((self)),reset_peak_stats((self)),build_device_memory_monitor(void)[HOT],log((self,metrics: dict[str,Any],step: int)),close((self)),__init__((self,log_dir: str,tag: str | None = None))[CTOR,DUNDER],log((self,metrics: dict[str,Any],step: int)),close((self)),__init__((self,log_dir: str,job_config: JobConfig,tag: str | None = None))[CTOR,DUNDER],log((self,metrics: dict[str,Any],step: int)),close((self)),__init__((self))[CTOR,DUNDER],add_logger((self,logger_instance: BaseLogger)),log((self,metrics: dict[str,Any],step: int)),close((self)),_get_metrics_rank((parallel_dims: ParallelDims,job_config: JobConfig,)),should_log((self,step: int)),log((self,step: int,global_avg_loss: float,global_max_loss: float,grad_norm: float,extra_metrics: dict[str,Any] | None = None,)),log_validation((self,loss: float,step: int)),close((self)),build_metrics_processor((job_config: JobConfig,parallel_dims: ParallelDims,model_args: "BaseModelArgs | None" = None,tag: str | None = None,))[HOT]] mx.py→[__init__((self,job_config: JobConfig,parallel_dims: ParallelDims))[CTOR,DUNDER]→{expert_parallel::set_token_group_alignment_size_m,utils::has_cuda_capability},__init__((self,job_config: JobConfig,parallel_dims: ParallelDims))[CTOR,DUNDER]→{utils::has_cuda_capability},convert((self,model: nn.Module)),post_optimizer_hook((self,model: nn.Module | list[nn.Module])),convert((self,model: nn.Module)),post_optimizer_hook((self,model: nn.Module | list[nn.Module]))] optimizer.py→[__iter__((self))[DUNDER]→{lr_scheduler::step},state_dict((self))→{train::main},load_state_dict((self,state_dict: dict[str,Any]))→{train::main},__init__((self,model_parts: list[nn.Module],optimizer_cls: type[T],optimizer_kwargs: dict[str,Any],ft_manager: "ft.Manager",use_ft_optimizer: bool = True,))[CTOR,DUNDER]→{train::main},build_optimizers_with_moe_load_balancing((model_parts: list[nn.Module],optimizer_config: OptimizerConfig,parallel_dims: ParallelDims,ft_manager: FTManager | None = None,))[HOT]→{build_optimizers},__init__((self,model_parts: list[nn.Module],optimizer_cls: type[T],optimizer_kwargs: dict[str,Any],))[CTOR,DUNDER],__len__((self))[DUNDER],step((self,*args,**kwargs)),zero_grad((self,*args,**kwargs)),_validate_length((self,expected_length: int)),_post_init((self,all_params: list[nn.Parameter],optimizer_kwargs: dict[str,Any])),__init__((self,model_parts: list[nn.Module],optimizer_cls: type[T],optimizer_kwargs: dict[str,Any],))[CTOR,DUNDER],step((self)),zero_grad((self)),init_cache_state_dict((self)),state_dict((self)),load_state_dict((self,state_dict: dict[str,Any])),step((self,*args,**kwargs)),zero_grad((self,*args,**kwargs)),build_optimizers((model_parts: list[nn.Module],optimizer_config: OptimizerConfig,parallel_dims: ParallelDims,ft_manager: FTManager | None = None,))[HOT]] protocol.py→[] tokenizer.py→[_load_tokenizer_from_path((self,tokenizer_path: str))→{math::rope,math::rope,math::rope,math::rope},_load_config((self,config_path: str))→{math::rope},__init__((self))[CTOR,DUNDER],__init__((self,tokenizer_path: str,))[CTOR,DUNDER],_get_token_from_config((self,config: dict[str,Any],key: str)),_process_special_token((self,token_str: str,token_config: dict,token_id: Optional[int] = None)),_infer_special_tokens((self)),_infer_should_add_bos_eos((self)),encode((self,*args,**kwargs)),get_vocab_size((self)),get_vocab((self)),token_to_id((self,token: str)),id_to_token((self,token_id: int)),build_hf_tokenizer((job_config: JobConfig,))[HOT]] utils.py→[fragment_llm((model: nn.Module,ft_config: FTConfig,n_layers: int,))→{module_split,pipeline_parallel::generate_llm_fqn_per_model_part},module_filter_fn((mod: nn.Module,fqn: str,filter_fqns: list[str])),module_split((model: nn.Module,module_fqns_per_model_fragment: list[list[str]],))] validate.py→
[__init__((self,job_config: JobConfig,dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer,parallel_dims: ParallelDims,loss_fn: LossFunction,validation_context: Generator[None,None,None],maybe_enable_amp: Generator[None,None,None],metrics_processor: MetricsProcessor,pp_schedule: _PipelineSchedule | None = None,pp_has_first_stage: bool | None = None,pp_has_last_stage: bool | None = None,))[CTOR,DUNDER]→{hf_datasets::build_hf_validation_dataloader},build_validator((job_config: JobConfig,dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer,parallel_dims: ParallelDims,loss_fn: LossFunction,validation_context: Generator[None,None,None],maybe_enable_amp: Generator[None,None,None],metrics_processor: MetricsProcessor | None = None,pp_schedule: _PipelineSchedule | None = None,pp_has_first_stage: bool | None = None,pp_has_last_stage: bool | None = None,))[HOT]→{validate},__init__((self,job_config: JobConfig))[CTOR,DUNDER],validate((self,model_parts: list[nn.Module])),should_validate((self,step: int))] 
### UTILITY_LAYER
NODES:1612 CALL_DEPTH:7

__init__.py→[get_train_spec(void),__init__((self,*,input_layout: Placement | None = None,output_layout: Placement | None = None,use_local_output: bool = True,))[CTOR,DUNDER],get_train_spec(void),_apply((self,module: nn.Module,device_mesh: DeviceMesh)),get_train_spec(void),get_train_spec(void),get_train_spec(void)] activation_checkpoint.py→[_apply_ac_to_transformer_block((module: nn.Module,ac_config: ACConfig,*,base_fqn: str | None = None,model_compile_enabled: bool = False,use_flex_attn: bool = False,op_sac_save_list: set[torch._ops.OpOverload] | None = None,))→{_apply_layer_sac,_apply_op_sac,_apply_op_sac_to_transformer_block_with_flex,_apply_full_ac,lr_scheduler::step},_apply_op_sac((module: nn.Module,ac_config: ACConfig,*,base_fqn: str | None = None,op_sac_save_list: set[torch._ops.OpOverload],))→{lr_scheduler::step},_apply_op_sac_to_transformer_block_with_flex((module: nn.Module,ac_config: ACConfig,*,base_fqn: str | None = None,model_compile_enabled: bool = False,op_sac_save_list: set[torch._ops.OpOverload],))→{logging::warn_once},apply_ac((model: nn.Module,ac_config: ACConfig,*,model_compile_enabled: bool = False,use_flex_attn: bool = False,op_sac_save_list: set[torch._ops.OpOverload] | None = None,))→{_apply_ac_to_transformer_block},_apply_layer_sac((module: nn.Module,ac_config: ACConfig)),_apply_full_ac((module: nn.Module,ac_config: ACConfig))] args.py→[] attn_mask_utils.py→[_prepare_4d_causal_attention_mask((attention_mask: Optional[torch.Tensor],input_shape: Union[torch.Size,Tuple,List],inputs_embeds: torch.Tensor,past_key_values_length: int,sliding_window: Optional[int] = None,))] autoencoder.py→[forward((self,x))→{swish,swish},__init__((self,params: AutoEncoderParams))[CTOR,DUNDER]→{tiktoken::decode,tokenizer::encode},forward((self,x: Tensor))→{swish},forward((self,z: Tensor))→{swish},load_ae((ckpt_path: str,autoencoder_params: AutoEncoderParams,device: str | torch.device = "cuda",dtype=torch.bfloat16,random_init=False,))→{lr_scheduler::step},swish((x: Tensor)),__init__((self,in_channels: int))[CTOR,DUNDER],attention((self,h_: Tensor)),forward((self,x: Tensor)),__init__((self,in_channels: int,out_channels: int))[CTOR,DUNDER],__init__((self,in_channels: int))[CTOR,DUNDER],forward((self,x: Tensor)),__init__((self,in_channels: int))[CTOR,DUNDER],forward((self,x: Tensor)),__init__((self,resolution: int,in_channels: int,ch: int,ch_mult: list[int],num_res_blocks: int,z_channels: int,))[CTOR,DUNDER],__init__((self,ch: int,out_ch: int,ch_mult: list[int],num_res_blocks: int,in_channels: int,resolution: int,z_channels: int,))[CTOR,DUNDER],__init__((self,sample: bool = True,chunk_dim: int = 1))[CTOR,DUNDER],forward((self,z: Tensor)),encode((self,x: Tensor)),decode((self,z: Tensor)),forward((self,x: Tensor))] benchmark.py→[benchmark_model_configs(void)→{mg_grouped_gemm::grouped_gemm_forward,mg_grouped_gemm::grouped_gemm_forward,reference_utils::compute_reference_forward,reference_utils::compute_reference_forward},compute_reference_forward((x,w,m_sizes))[HOT],plot_benchmark_results((results)),compare_mg_implementations(void)] benchmark_kernels.py→[print_results_table((results))→{lr_scheduler::step},benchmark_quant_kernels((shapes,dtype=torch.bfloat16,warmup=10,iters=100))] cg_backward.py→[cg_grouped_gemm_backward_weights((grad_output: torch.Tensor,# [M_total,N] inputs: torch.Tensor,# [M_total,K] expert_indices: torch.Tensor,# [M_total] num_experts: int,group_size_m: int = 128,))→{train::main,train::main,train::main},verify_cg_gemm_backward((M_total=1024,N=512,K=512,num_experts=8,group_size_m=128,device="cuda",atol=1e-1,# Absolute tolerance for validation rtol=1e-1,# Relative tolerance for validation))→
{cg_grouped_gemm},cg_grouped_gemm_backward_inputs((grad_output: torch.Tensor,# [M_total,N] expert_weights: torch.Tensor,# [num_experts,N,K] expert_indices: torch.Tensor,# [M_total] group_size_m: int = 128,)),cg_grouped_gemm((inputs: torch.Tensor,expert_weights: torch.Tensor,expert_indices: torch.Tensor,group_size_m: int = 128,)),benchmark_cg_gemm_backward((M_total=1024,N=512,K=512,num_experts=8,group_size_m=128,device="cuda",num_runs=10,))] cg_forward.py→[cg_grouped_gemm_forward((inputs: torch.Tensor,# [M_total,K] expert_weights: torch.Tensor,# [num_experts,N,K] expert_indices: torch.Tensor,# [M_total] group_size_m: int = 128,)),cg_grouped_gemm_forward_dynamic((inputs: torch.Tensor,# [M_total,K] expert_weights: torch.Tensor,# [num_experts,N,K] expert_indices: torch.Tensor,# [M_total] group_size_m: int = 128,)),cg_grouped_gemm((inputs: torch.Tensor,expert_weights: torch.Tensor,expert_indices: torch.Tensor,# use_tma: bool = True,group_size_m: int = 128,))] cg_reference.py→[pytorch_reference((inputs: torch.Tensor,expert_weights: torch.Tensor,expert_indices: torch.Tensor,group_size_m: int = 128,))→{train::main}] check_padding_mm.py→[] checkpoint.py→[load_safetensor_weights((model: torch.nn.Module,weight_map: Dict[str,str],file_location: str,device: torch.device,))→{load_safetensor_file,get_needed_files,lr_scheduler::step,lr_scheduler::step,lr_scheduler::step,lr_scheduler::step},read_weights_from_json((file_path: str))→{lr_scheduler::step,math::rope},load_weights_from_hf((model: torch.nn.Module,distribution: str,device: torch.device,))→{load_safetensor_weights,get_hf_weight_map_and_path},get_hf_weight_map_and_path((model_id: str,))→{read_weights_from_json},get_needed_files((state_dict: Dict[str,torch.Tensor],weight_map: Dict[str,str]))→{lr_scheduler::step},load_safetensor_file((full_path: str,device: torch.device))] combine.py→[__init__((self,group_name: str,align: int,in_len,out_len,token_shape,num_ranks,num_local_experts,dtype,device: torch.device,))[CTOR,DUNDER],forward((self,inp: torch.Tensor,out: torch.Tensor,in_splits_offsets: torch.Tensor,out_splits_offsets: torch.Tensor,))] config.py→[] convert_hf_to_dcp_with_gpus.py→[_create_fqn_mappings((self,state_dict: dict[str,torch.Tensor]))→{convert_to_titan_fqns,lr_scheduler::step,lr_scheduler::step,lr_scheduler::step,lr_scheduler::step,lr_scheduler::step},_verify_state_dict((state_dict: dict[str,torch.Tensor],path: str,rank: int))→{lr_scheduler::step,math::rope},convert_to_titan_fqns((fqn: str))→{extract_layer_number},_load_metadata((self))→{math::rope},_get_load_assignments((self,state_dict: dict[str,Any]))→{convert_to_hf_shape},_create_verified_state_dict((pg: dist.ProcessGroup,mesh: DeviceMesh))→{simple_fsdp::_distribute_dtensor},extract_layer_number((s)),convert_to_hf_shape((fqn: str,titan_fqns: list[str],dtensor: DTensor)),convert_to_titan_tensors((fqn: str,full_tensor: torch.Tensor)),__init__((self,process_group: dist.ProcessGroup,path: str,token: Optional[str] = None,loader_every_n_ranks: int = 8,))[CTOR,DUNDER],convert((self,state_dict: dict[str,torch.Tensor])),_load_round((self,assignment: _Assignment)),_reshard_send((self,assignment: _Assignment,loaded_state_dict: dict[str,torch.Tensor],)),_reshard_receive((self,assignment: _Assignment,state_dict: dict[str,torch.Tensor])),_reshard((self,result: dict[str,torch.Tensor],state_dict: dict[str,torch.Tensor],))] convert_meta_to_dcp_with_gpus.py→[_create_fqn_mappings((self,state_dict: dict[str,torch.Tensor]))→{lr_scheduler::step,lr_scheduler::step,lr_scheduler::step,lr_scheduler::step,lr_scheduler::step,lr_scheduler::step},_create_verified_state_dict((pg: dist.ProcessGroup,mesh: DeviceMesh))→
{simple_fsdp::_distribute_dtensor},convert_to_titan_fqns((fqn: str)),get_shard_dim((fqn: str)),split_fused_qkv((shards: list[torch.Tensor])),__init__((self,process_group: dist.ProcessGroup,path: str,loader_every_n_ranks: int = 8,))[CTOR,DUNDER],convert((self,state_dict: dict[str,torch.Tensor])),_get_file_path((self,loader_id: int)),_load_metadata((self)),_get_load_assignments((self,state_dict: dict[str,torch.Tensor])),_load_round((self,assignment: _Assignment)),_reshard_send((self,assignment: _Assignment,loaded_state_dict: dict[str,torch.Tensor],)),_reshard_receive((self,assignment: _Assignment,state_dict: dict[str,torch.Tensor])),_reshard((self,results: list[dict[str,torch.Tensor]],state_dict: dict[str,torch.Tensor],)),_verify_state_dict((state_dict: dict[str,torch.Tensor],path: str,rank: int))] custom_args.py→[] dataloader.py→[build_mosaic_dataloader((*,dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer | None,job_config: JobConfig,))[HOT]→{_maybe_extract_hf_tokenizer,math::rope},__iter__((self))[DUNDER]→{lr_scheduler::step},titan_collate_fn((batch: list[Any]))→{math::rope},__init__((self,data_spec: Any))[CTOR,DUNDER],state_dict((self)),load_state_dict((self,state_dict: dict[str,Any])),__init__((self,*args: Any,**kwargs: Any))[CTOR,DUNDER],__getitem__((self,idx: int))[DUNDER],_maybe_extract_hf_tokenizer((tokenizer: BaseTokenizer | Any | None)),state_dict((self,num_samples: int | None = None,from_beginning: bool = True)),load_state_dict((self,obj: dict[str,Any])),__init__((self,dataset: StatefulStreamingTextDataset,dp_rank: int,dp_world_size: int,batch_size: int,collate_fn: Callable | None = None,num_workers: int = 0,prefetch_factor: int | None = 2,pin_memory: bool = True,persistent_workers: bool = True,drop_last: bool = True,))[CTOR,DUNDER],state_dict((self)),load_state_dict((self,state_dict: dict[str,Any])),build_mosaic_dataloader((*,dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer,job_config: MosaicJobConfig,))[HOT]] debug.py→[benchmark_performance(void)→{pytorch_reference,pytorch_reference,create_aligned_test_data,cg_forward::cg_grouped_gemm_forward,cg_forward::cg_grouped_gemm_forward},run_all_tests(void)→{benchmark_performance,debug::test_large,debug::test_medium,debug::test_small},pytorch_reference((inputs: torch.Tensor,expert_weights: torch.Tensor,expert_indices: torch.Tensor,group_size_m: int = 128,))→{train::main},create_aligned_test_data((batch_size: int,seq_len: int,hidden_dim: int,output_dim: int,num_experts: int,group_size_m: int = 128,device: str = "cuda",dtype: torch.dtype = torch.float16,)),verify_results((output_triton: torch.Tensor,output_reference: torch.Tensor,rtol: float = 1e-2,atol: float = 1e-2,))] dispatch.py→[__init__((self,group_name: str,align: int,in_len,out_len,token_shape,num_ranks,num_local_experts,dtype,device: torch.device,))[CTOR,DUNDER],forward((self,inp: torch.Tensor,out: torch.Tensor,in_splits: torch.Tensor,out_splits_offsets: torch.Tensor,))] download.py→[print_usage(void)] download_autoencoder.py→[hf_download((repo_id: str,file_path: str,local_dir: str,hf_token: Optional[str] = None))] dsgemm_kernels.py→[groupwise_activation_quant((x: torch.Tensor,block_size: int = 128,switching_size=2048,))→{train::main,train::main}] dsgemm_unit_testing.py→[create_m_indices_fast((m_sizes: torch.Tensor)),per_token_cast_to_fp8((x: torch.Tensor)),per_block_cast_to_fp8((x: torch.Tensor)),compute_reference_with_scaling((lhs: torch.Tensor,lhs_scales: torch.Tensor,rhs: torch.Tensor,rhs_scales: torch.Tensor,m_indices: torch.Tensor,num_groups: int,))[HOT]] dsgemm_utils.py→[make_grouped_layout((num_groups: int,x: torch.Tensor,y: torch.Tensor))→{get_col_major_tma_aligned_tensor,dsgemm_unit_testing::per_token_cast_to_fp8,dsgemm_unit_testing::per_block_cast_to_fp8,dsgemm_unit_testing::per_token_cast_to_fp8},construct_grouped((num_groups: int,x: torch.Tensor,y: torch.Tensor,is_masked: bool))→
{get_col_major_tma_aligned_tensor,dsgemm_unit_testing::per_token_cast_to_fp8,dsgemm_unit_testing::per_block_cast_to_fp8,dsgemm_unit_testing::per_token_cast_to_fp8},prepare_fp8_input((x))→{get_col_major_tma_aligned_tensor,dsgemm_unit_testing::per_token_cast_to_fp8},per_block_cast_to_fp8((x: torch.Tensor))→{ceil_div,ceil_div},prepare_fp8_weight((w))→{dsgemm_unit_testing::per_block_cast_to_fp8},get_tma_aligned_size((x: int,element_size: int))→{ceil_div},get_col_major_tma_aligned_tensor((x: torch.Tensor))→{get_tma_aligned_size},compare_fp8_tensors((a: torch.Tensor,b: torch.Tensor)),create_indices_from_offsets_nosync((m_offsets: torch.Tensor)),create_m_indices_from_offsets((m_offsets: torch.Tensor)),create_m_indices_from_sizes((m_sizes: torch.Tensor)),get_m_indices((num_groups: int,m: int)),set_num_sms((num_sms: int)),get_num_sms(void),ceil_div((x: int,y: int)),get_m_alignment_for_contiguous_layout(void),per_token_cast_to_fp8((x: torch.Tensor))] engine.py→[close((self))] example_train.py→[batch_generator((self,data_iterable: Iterable[tuple[dict[str,torch.Tensor],torch.Tensor]]))→{lr_scheduler::step},forward_backward_step((self,input_dict: dict[str,torch.Tensor],labels: torch.Tensor))→{attention::init_attention_mask},train_step((self,data_iterator: Iterable[tuple[dict[str,torch.Tensor],torch.Tensor]])),state_dict((self)),load_state_dict((self,state_dict: dict[str,Any])),close((self))] expert_parallel.py→[_partition_fn((self,name,module,device_mesh))→{simple_fsdp::_distribute_dtensor,simple_fsdp::_distribute_dtensor,simple_fsdp::_distribute_dtensor},_partition_fn_2d((self,name,mod,ep_tp_mesh))→{simple_fsdp::_distribute_dtensor,simple_fsdp::_distribute_dtensor,simple_fsdp::_distribute_dtensor},set_token_group_alignment_size_m((alignment_size: ValidTokenGroupAlignmentSize,)),_apply((self,module: nn.Module,device_mesh: DeviceMesh)),__init__((self))[CTOR,DUNDER],_token_dispatch((self,mod,inputs,device_mesh)),_token_combine((self,mod,routed_output,device_mesh)),_apply((self,module: nn.Module,device_mesh: DeviceMesh)),__init__((self,tp_mesh: DeviceMesh,ep_mesh: DeviceMesh,))[CTOR,DUNDER],_token_dispatch((self,mod,inputs,device_mesh)),_token_combine((self,mod,routed_output,device_mesh)),_apply((self,module: nn.Module,device_mesh: DeviceMesh)),expert_parallel((func: Callable)),__init__((self))[CTOR,DUNDER],_prepare_inputput_fn((self,mod,inputs,device_mesh)),_prepare_output_fn((self,mod,outputs,device_mesh)),_apply((self,module: nn.Module,device_mesh: DeviceMesh))] fast_debug_ao.py→[] flux_dataset.py→[_get_data_iter((self))→{lr_scheduler::step,lr_scheduler::step,lr_scheduler::step},_cc12m_wds_data_processor((sample: dict[str,Any],t5_tokenizer: FluxTokenizer,clip_tokenizer: FluxTokenizer,output_size: int = 256,))→{_process_cc12m_image},_coco_data_processor((sample: dict[str,Any],t5_tokenizer: FluxTokenizer,clip_tokenizer: FluxTokenizer,output_size: int = 256,))→{_process_cc12m_image},__init__((self,dataset_name: str,dataset_path: Optional[str],t5_tokenizer: BaseTokenizer,clip_tokenizer: BaseTokenizer,job_config: Optional[JobConfig] = None,dp_rank: int = 0,dp_world_size: int = 1,infinite: bool = False,))[CTOR,DUNDER]→{hf_datasets::_validate_dataset},build_flux_dataloader((dp_world_size: int,dp_rank: int,job_config: JobConfig,# This parameter is not used,keep it for compatibility tokenizer: FluxTokenizer | None,infinite: bool = True,))[HOT]→{tokenizer::build_flux_tokenizer},build_flux_validation_dataloader((dp_world_size: int,dp_rank: int,job_config: JobConfig,# This parameter is not used,keep it for compatibility tokenizer: BaseTokenizer | None,generate_timestamps: bool = True,infinite: bool = False,))[HOT]→
{tokenizer::build_flux_tokenizer},_process_cc12m_image((img: PIL.Image.Image,output_size: int = 256,)),_validate_dataset((dataset_name: str,dataset_path: Optional[str] = None)),__iter__((self))[DUNDER],load_state_dict((self,state_dict)),state_dict((self)),__init__((self,dataset_name: str,dataset_path: Optional[str],t5_tokenizer: BaseTokenizer,clip_tokenizer: BaseTokenizer,job_config: Optional[JobConfig] = None,dp_rank: int = 0,dp_world_size: int = 1,generate_timesteps: bool = True,infinite: bool = False,))[CTOR,DUNDER],__iter__((self))[DUNDER]] generate.py→[create_model((dist_config: DistConfig))→{checkpoint::load_weights_from_hf},decode((tokenizer,x))→{colorize_chat},colorize_chat((text,user_color=None,assistant_color=None,output_color=None)),create_dist_config((mesh: DeviceMesh)),time_generation((func))] group_gemms.py→[execute((self,contig_tokens,m_sizes,m_offsets,module))→{cg_forward::cg_grouped_gemm_forward,cg_forward::cg_grouped_gemm_forward,cg_forward::cg_grouped_gemm_forward},execute((self,contig_tokens,m_sizes,m_offsets,module))→{mg_grouped_gemm::grouped_gemm_forward,mg_grouped_gemm::grouped_gemm_forward,mg_grouped_gemm::grouped_gemm_forward},__init__((self,custom_activation))[CTOR,DUNDER],arrange_expert_weights((self,all_weights,submod_name,module)),execute((self,contig_tokens,m_sizes,m_offsets,module)),arrange_expert_weights((self,all_weights,submod_name,module)),execute((self,contig_tokens,m_sizes,m_offsets,module)),arrange_expert_weights((self,all_weights,submod_name,module)),arrange_expert_weights((self,all_weights,submod_name,module)),execute((self,contig_tokens,m_sizes,m_offsets,module)),arrange_expert_weights((self,all_weights,submod_name,module)),arrange_expert_weights((self,all_weights,submod_name,module)),execute((self,contig_tokens,m_sizes,m_offsets,module)),__init__((self,custom_activation,use_triton_quant=True))[CTOR,DUNDER],arrange_expert_weights((self,all_weights,submod_name,module)),execute((self,contig_tokens,m_sizes,m_offsets,module))] hf_datasets.py→[_get_data_iter((self))→{lr_scheduler::step,lr_scheduler::step,lr_scheduler::step},__init__((self,dataset_name: str,dataset_path: str | None,tokenizer: BaseTokenizer,seq_len: int = 2048,dp_rank: int = 0,dp_world_size: int = 1,infinite: bool = False,))[CTOR,DUNDER]→{_validate_dataset},_load_c4_dataset((dataset_path: str,split: str)),_process_c4_text((sample: dict[str,Any])),_validate_dataset((dataset_name: str,dataset_path: str | None = None)),__iter__((self))[DUNDER],load_state_dict((self,state_dict)),state_dict((self)),build_hf_dataloader((dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer,job_config: JobConfig,infinite: bool = True,))[HOT],build_hf_validation_dataloader((dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer,job_config: JobConfig,infinite: bool = False,))[HOT]] hf_embedder.py→[__init__((self,version: str,random_init=False,**hf_kwargs))[CTOR,DUNDER],forward((self,batch_tokens: Tensor))] hf_tokenizer.py→[remove_notset_root_handlers(void),__init__((self,tokenizer))[CTOR,DUNDER],encode((self,text,bos=False,eos=False,**kwargs)),__getattr__((self,name))[DUNDER],get_hf_tokenizer((model_id: str))] indices.py→[generate_permute_indices((tokens_per_expert_group: torch.Tensor,experts_per_rank: int,num_ranks: int,max_len: int,alignment: int,use_cpu: bool = False,))→{fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},simple_test(void)→{generate_permute_indices,generate_permute_indices},fill_indices_wrapper((tokens_per_expert_group: torch.Tensor,start_index_values: torch.Tensor,write_offsets: torch.Tensor,experts_per_rank: int,num_ranks: int,max_len: int,block_size: int = 128,max_blocks: int = 1024,# cap on total number of blocks to launch))→{train::main},fill_indices_cpu((tokens_per_expert_group: torch.Tensor,start_index_values: torch.Tensor,write_offsets: torch.Tensor,experts_per_rank: int,num_ranks: int,max_len: int,))→{train::main}] infer.py→[] job_config.py→[] layers.py→
[forward((self,ids: Tensor))→{math::rope},forward((self,x: Tensor,pe: Tensor))→{math::attention},forward((self,img: Tensor,txt: Tensor,vec: Tensor,pe: Tensor))→{math::attention},forward((self,x: Tensor,vec: Tensor,pe: Tensor))→{math::attention},__init__((self,dim: int,theta: int,axes_dim: list[int]))[CTOR,DUNDER],timestep_embedding((t: Tensor,dim,max_period=10000,time_factor: float = 1000.0)),__init__((self,in_dim: int,hidden_dim: int))[CTOR,DUNDER],init_weights((self,init_std: float = 0.02)),forward((self,x: Tensor)),__init__((self,dim: int))[CTOR,DUNDER],init_weights((self)),forward((self,q: Tensor,k: Tensor,v: Tensor)),__init__((self,dim: int,num_heads: int = 8,qkv_bias: bool = False))[CTOR,DUNDER],init_weights((self)),__init__((self,dim: int,double: bool))[CTOR,DUNDER],init_weights((self)),forward((self,vec: Tensor)),__init__((self,hidden_size: int,num_heads: int,mlp_ratio: float,qkv_bias: bool = False))[CTOR,DUNDER],init_weights((self)),__init__((self,hidden_size: int,num_heads: int,mlp_ratio: float = 4.0,qk_scale: float | None = None,))[CTOR,DUNDER],init_weights((self)),__init__((self,hidden_size: int,patch_size: int,out_channels: int))[CTOR,DUNDER],init_weights((self)),forward((self,x: Tensor,vec: Tensor))] logging.py→[init_logger(void),warn_once((logger: logging.Logger,msg: str))] loss.py→[mse_loss((pred: torch.Tensor,labels: torch.Tensor)),build_mse_loss((job_config: JobConfig))[HOT]] manager.py→[_dict_to_dataclass((self,cls,data: dict[str,Any]))→{train::close,lr_scheduler::step,lr_scheduler::step},_maybe_load_toml((self,args: list[str]))→{math::rope},__init__((self,config_cls: Type[JobConfig] = JobConfig))[CTOR,DUNDER],parse_args((self,args: list[str] = sys.argv[1:])),_maybe_add_custom_args((self,args: list[str],toml_values: dict[str,Any] | None)),_validate_config((self))] math.py→[attention((q: Tensor,k: Tensor,v: Tensor,pe: Tensor))→{apply_rope},rope((pos: Tensor,dim: int,theta: int)),apply_rope((xq: Tensor,xk: Tensor,freqs_cis: Tensor))] mg_grouped_gemm.py→[grouped_gemm_backward((grad_output: torch.Tensor,x: torch.Tensor,w: torch.Tensor,m_sizes: torch.Tensor,use_tma: bool = True,tma_size: int = 128,))→{grouped_gemm_dw_tma,grouped_gemm_dx_tma},grouped_gemm_forward((x: torch.Tensor,w: torch.Tensor,m_sizes: torch.Tensor,tma_size: int = 128,using_fp8: bool = False,)),grouped_gemm_dx_tma((grad_output: torch.Tensor,w: torch.Tensor,m_sizes: torch.Tensor,num_sms: int = 132,tma_size: int = 128,)),grouped_gemm_dw_tma((x: torch.Tensor,grad_output: torch.Tensor,m_sizes: torch.Tensor,num_sms: int = 132,tma_size: int = 128,)),mg_grouped_gemm((x: torch.Tensor,w: torch.Tensor,m_sizes: torch.Tensor,use_tma: bool = True,tma_size: int = 128,using_fp8: bool = False,))] mm_collator.py→[padded_collate((batch: List[Dict[str,List[int]]],padding_idx: int = 0,ignore_idx: int = -100,))] mm_collator_nld.py→[] mm_dataset.py→[_process_obelics_sample((sample: dict[str,Any],image_token: str = "<|image|>"))→{train::main,utils::load_image},_get_data_iter((self))→{lr_scheduler::step,lr_scheduler::step},__init__((self,dataset_name: str,dataset_path: Optional[str],tokenizer: BaseTokenizer,image_token: str = "<|image|>",tile_size: int = 448,max_num_tiles: int = 4,seq_len: int = 2048,dp_rank: int = 0,dp_world_size: int = 1,infinite: bool = False,))[CTOR,DUNDER]→{_validate_mm_dataset},__iter__((self))[DUNDER]→{lr_scheduler::step},_load_obelics_dataset((dataset_path: str)),_validate_mm_dataset((dataset_name: str,dataset_path: str = None)),load_state_dict((self,state_dict)),state_dict((self)),build_mm_dataloader((dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer,job_config: JobConfig,infinite: bool = True,))[HOT]] mm_datasets.py→[_process_mm_sample((texts: list[str] | str,images: list[bytes] | bytes,tokenizer: BaseTokenizer,patch_size: int,max_patch_per_image: int,spatial_merge_size: int,special_tokens: SpecialTokens,))→
{text::process_text_with_images,image::calculate_image_tokens,image::process_image},_get_data_iter((self))→{lr_scheduler::step,lr_scheduler::step,lr_scheduler::step},_process_obelics_sample((sample: dict[str,Any],tokenizer: HuggingFaceTokenizer,patch_size: int,spatial_merge_size: int,max_patch_per_image: int,special_tokens: SpecialTokens,))→{_process_mm_sample},_process_cc12_wd_sample((sample: dict[str,Any],tokenizer: BaseTokenizer,patch_size: int,spatial_merge_size: int,max_patch_per_image: int,special_tokens: SpecialTokens,))→{_process_mm_sample},__init__((self,dataset_name: str,dataset_path: str | None,tokenizer: BaseTokenizer,batch_size: int,seq_len: int,patch_size: int,spatial_merge_size: int,max_patches_per_image: int,max_images_per_batch: int,packing_buffer_size: int,special_tokens: SpecialTokens,dp_rank: int = 0,dp_world_size: int = 1,infinite: bool = False,))[CTOR,DUNDER]→{mm_dataset::_validate_mm_dataset},_validate_mm_dataset((dataset_name: str,dataset_path: str | None = None)),__iter__((self))[DUNDER],load_state_dict((self,state_dict)),state_dict((self)),build_mm_dataloader((dp_world_size: int,dp_rank: int,tokenizer: HuggingFaceTokenizer,job_config: JobConfig,infinite: bool = True,))[HOT]] model.py→[yarn_find_correction_range((low_rot,high_rot,dim,base=10000,max_position_embeddings=2048))→{yarn_find_correction_dim,yarn_find_correction_dim,train::main,train::main},_set_cos_sin_cache((self,seq_len,device,dtype))→{yarn_get_mscale,yarn_get_mscale,yarn_linear_ramp_mask,yarn_find_correction_range},apply_rotary_emb((xq: torch.Tensor,xk: torch.Tensor,rope_cache: torch.Tensor))→{rotate_half,rotate_half,model::reshape_for_broadcast},forward((self,x: torch.Tensor,freqs_cis: torch.Tensor,))→{model::repeat_kv,model::repeat_kv,model::apply_rotary_emb},forward((self,x: torch.Tensor,rope_cache: torch.Tensor,))→{model::repeat_kv,model::repeat_kv,model::apply_rotary_emb},forward((self,x: torch.Tensor,freqs_cis: torch.Tensor,))→{model::repeat_kv,model::repeat_kv,model::apply_rotary_emb},forward((self,x: torch.Tensor,freqs_cis: torch.Tensor,))→{model::repeat_kv,model::repeat_kv,model::apply_rotary_emb},__init__((self,layer_id: int,model_args: TransformerModelArgs,))[CTOR,DUNDER]→{math::rope,math::attention},__init__((self,layer_id: int,model_args: Qwen3ModelArgs))[CTOR,DUNDER]→{math::rope,math::attention},apply_rotary_pos_emb((q,k,cos,sin,position_ids,unsqueeze_dim=1))→{rotate_half,rotate_half},__init__((self,config))[CTOR,DUNDER]→{get_group,lr_scheduler::step},forward((self,hidden_states: torch.Tensor,attention_mask: Optional[torch.Tensor] = None,position_ids: Optional[torch.LongTensor] = None,))→{apply_rotary_pos_emb,attn_mask_utils::_prepare_4d_causal_attention_mask},__init__((self,config: ModelArgs,layer_idx: int))[CTOR,DUNDER]→{math::rope,math::attention},forward((self,x: torch.Tensor,encoder_input: torch.Tensor,mask: Optional[torch.Tensor] = None,))→{model::repeat_kv,model::repeat_kv},apply_rotary_emb((xq: torch.Tensor,xk: torch.Tensor,freqs_cis: torch.Tensor,))→{model::reshape_for_broadcast},forward((self,tokens: torch.Tensor,pixel_values: torch.Tensor,grid_thw: torch.Tensor,special_tokens: SpecialTokens,input_batch: torch.Tensor | None = None,))→{_scatter_img_tokens},forward((self,img: Tensor,img_ids: Tensor,txt: Tensor,txt_ids: Tensor,timesteps: Tensor,y: Tensor,))→{layers::timestep_embedding},__init__((self,model_args: Qwen3ModelArgs))[CTOR,DUNDER]→{attention::build_attention},__init__((self,model_args: TransformerModelArgs,use_rope: bool = True,fixed_block_size: int | None = None,))[CTOR,DUNDER]→{attention::build_attention},apply_rotary_emb((xq: torch.Tensor,xk: torch.Tensor,freqs_cis: torch.Tensor,))→{model::reshape_for_broadcast},__init__((self,model_args: Qwen3ModelArgs))[CTOR,DUNDER]→{lr_scheduler::step},__init__((self,model_args: TransformerModelArgs))[CTOR,DUNDER]→{lr_scheduler::step},_precompute_rope_cache((self))[HOT]→
{precompute_rope_cache},_precompute_freqs_cis((self))[HOT]→{model::precompute_freqs_cis},__init__((self,model_args: ModelArgs,attn_scale: Optional[nn.Module] = None,mlp_scale: Optional[nn.Module] = None,))[CTOR,DUNDER]→{math::attention},moe_on_device((self,x,topk_ids,topk_weight))→{indices::generate_permute_indices},__init__((self,config: ModelArgs,layer_idx: Optional[int] = None))[CTOR,DUNDER]→{yarn_get_mscale},__init__((self,config: ModelArgs))[CTOR,DUNDER]→{lr_scheduler::step},_precompute_freqs_cis((self,model_args))[HOT]→
{model::precompute_freqs_cis},__init__((self,model_args: TransformerModelArgs))[CTOR,DUNDER],init_weights((self,*args,**kwargs)),_scatter_img_tokens((h_BSD,tokens_BS,i_NLD,i_mask_NL,img_id)),precompute_freqs_cis((dim: int,end: int,theta: float = 10000.0))[HOT],precompute_rope_cache((dim: int,max_seq_len: int,base: float = 1_000_000.0))[HOT],__init__((self,model_args: FluxModelArgs))[CTOR,DUNDER],__init__((self,in_dim: int,out_dim: int))[CTOR,DUNDER],rotate_half((x: torch.Tensor)),reshape_for_broadcast((freqs_cis: torch.Tensor,x: torch.Tensor)),forward((self,x_NLD: torch.Tensor)),__init__((self,model_args: BaseModelArgs))[CTOR,DUNDER],reshape_for_broadcast((rope_cache: torch.Tensor,x: torch.Tensor)),init_weights((self)),__init__((self,model_args: Llama3Siglip2ModelArgs))[CTOR,DUNDER],__init__((self,*args: Any,**kwargs: Any))[CTOR,DUNDER],get_group((dim_name: Optional[str] = None)),init_weights((self,buffer_device=None)),forward((self,x: torch.Tensor)),__init__((self,hidden_size,eps=1e-6))[CTOR,DUNDER],forward((self,hidden_states)),init_weights((self,buffer_device=None)),precompute_freqs_cis((dim: int,end: int,theta: float = 10000.0))[HOT],__init__((self,dim,max_position_embeddings=2048,base=10000,device=None))[CTOR,DUNDER],repeat_kv((x: torch.Tensor,n_rep: int)),repeat_kv((x: torch.Tensor,n_rep: int)),_set_cos_sin_cache((self,seq_len,device,dtype)),reshape_for_broadcast((freqs_cis: torch.Tensor,x: torch.Tensor)),forward((self,x,seq_len=None)),__init__((self,dim,max_position_embeddings=2048,base=10000,device=None,scaling_factor=1.0,))[CTOR,DUNDER],_set_cos_sin_cache((self,seq_len,device,dtype)),__init__((self,dim,max_position_embeddings=2048,base=10000,device=None,scaling_factor=1.0,))[CTOR,DUNDER],init_weights((self,init_std: float)),init_weights((self,init_std: float)),repeat_kv((x: torch.Tensor,num_rep: int)),_set_cos_sin_cache((self,seq_len,device,dtype)),__init__((self,model_args: ModelArgs))[CTOR,DUNDER],yarn_find_correction_dim((num_rotations,dim,base=10000,max_position_embeddings=2048)),yarn_get_mscale((scale=1,mscale=1)),yarn_linear_ramp_mask((min,max,dim)),init_weights((self,init_std: float)),__init__((self,dim: int,hidden_dim: int,multiple_of: int,ffn_dim_multiplier: float | None,))[CTOR,DUNDER],__init__((self,dim,max_position_embeddings=2048,base=10000,device=None,scaling_factor=1.0,original_max_position_embeddings=4096,beta_fast=32,beta_slow=1,mscale=1,mscale_all_dim=0,))[CTOR,DUNDER],__init__((self,dim: int,hidden_dim: int,))[CTOR,DUNDER],forward((self,x)),forward((self,x)),init_weights((self,init_std: float)),init_weights((self,init_std: float)),__init__((self,dim: int,hidden_dim: int,multiple_of: int,ffn_dim_multiplier: Optional[float],activation: nn.Module = nn.SiLUvoid,))[CTOR,DUNDER],rotate_half((x)),forward((self,x: torch.Tensor,rope_cache: torch.Tensor,)),forward((self,x)),init_weights((self,init_std: float)),__init__((self))[CTOR,DUNDER],forward((self,x: torch.Tensor)),init_weights((self,buffer_device: torch.device)),forward((self,x: torch.Tensor,freqs_cis: torch.Tensor,)),__init__((self,config,hidden_size=None,intermediate_size=None))[CTOR,DUNDER],__init__((self,max_num_tiles: int,emb_dim: int,))[CTOR,DUNDER],init_weights((self,buffer_device: torch.device)),forward((self,x)),forward((self,x: torch.Tensor,aspect_ratio: torch.Tensor)),__init__((self,config))[CTOR,DUNDER],init_weights((self,buffer_device: torch.device | None = None,)),reset_parameters((self)),forward((self,hidden_states)),__init__((self,emb_dim: int,tile_size: int,patch_size: int))[CTOR,DUNDER],init_weights((self,buffer_device: torch.device | None = None,)),forward((self,x: torch.Tensor,*args: Tuple[Any])),forward((self,tokens: torch.Tensor,input_batch: torch.Tensor | None = None,)),__init__((self,max_num_tiles: int,emb_dim: int,tile_size: int,patch_size: int))[CTOR,DUNDER],forward((self,tokens: torch.Tensor,input_batch: torch.Tensor | None = None,)),forward((self,x: torch.Tensor,aspect_ratio: torch.Tensor)),__init__((self,in_channels: int,ou
t_channels: int,kernel_size: int,stride: int,bias: bool = False,))[CTOR,DUNDER],forward((self,x: torch.Tensor)),combine_experts((self,submod_name: str)),forward((self,x: torch.Tensor,mask: Optional[torch.Tensor] = None,)),setup_symm_mem((self,dtype: torch.dtype,device: torch.device)),__init__((self,emb_dim: int))[CTOR,DUNDER],forward((self,x: torch.Tensor)),get_send_buf((self)),get_gather_buf((self)),forward((self,hidden_states)),moe_forward((self,x,topk_ids,topk_weight)),sort_tokens((self,x,topk_ids,topk_weights)),__init__((self,model_args: ModelArgs,))[CTOR,DUNDER],_run_group_gemm((self,contig_tokens,m_sizes,m_offsets)),forward((self,images: torch.Tensor,aspect_ratio: Optional[torch.Tensor] = None)),__init__((self,model_args: ModelArgs,))[CTOR,DUNDER],forward((self,x: torch.Tensor,hidden_states: Optional[List[torch.Tensor]] = None,)),_init_rope((self)),__init__((self,model_args: ModelArgs))[CTOR,DUNDER],forward((self,images: torch.Tensor,aspect_ratio: Optional[torch.Tensor] = None)),__init__((self,dim: int,hidden_dim: int,multiple_of: int,ffn_dim_multiplier: Optional[float],))[CTOR,DUNDER],forward((self,x)),init_weights((self,init_std: float)),__init__((self,model_args: ModelArgs))[CTOR,DUNDER],init_weights((self,init_std: float)),forward((self,hidden_states: torch.Tensor,attention_mask: Optional[torch.Tensor] = None,position_ids: Optional[torch.LongTensor] = None,)),__init__((self,model_args: ModelArgs))[CTOR,DUNDER],init_weights((self,init_std: float)),__init__((self,model_args: ModelArgs,))[CTOR,DUNDER],forward((self,x: torch.Tensor,freqs_cis: torch.Tensor,**kwargs: Dict,)),__init__((self,model_args: ModelArgs,))[CTOR,DUNDER],_skip_mask((self,mask: Optional[torch.Tensor])),_init_weights((self,module)),forward((self,x: torch.Tensor,*,encoder_input: Optional[torch.Tensor] = None,encoder_mask: Optional[torch.Tensor] = None,**kwargs: Dict,)),forward((self,tokens: torch.Tensor,attention_mask: Optional[torch.Tensor] = None,position_ids: Optional[torch.LongTensor] = None,)),__init__((self,config))[CTOR,DUNDER],forward((self,tokens: torch.Tensor,attention_mask: Optional[torch.Tensor] = None,position_ids: Optional[torch.LongTensor] = None,)),__init__((self,layer: nn.Module,fusion_layer: nn.Module,fusion_first: bool = True))[CTOR,DUNDER],forward((self,x: torch.Tensor,**kwargs: Dict)),__init__((self,vocab_size: int,fusion_vocab_size: int,embed_dim: int))[CTOR,DUNDER],prepare_inputs_for_generation((self,input_ids,past_key_values=None,attention_mask=None,**kwargs,)),forward((self,input: torch.Tensor)),__init__((self,model_args: ModelArgs))[CTOR,DUNDER],setup_symm_mem((self,dtype: torch.dtype,device: torch.device)),forward((self,tokens: torch.Tensor,*,encoder_input: Optional[torch.Tensor] = None,encoder_mask: Optional[torch.Tensor] = None,))] model_args.py→[] model_config.py→[] model_converter.py→[__init__((self,job_config: JobConfig,parallel_dims: ParallelDims))[CTOR,DUNDER],convert((self,model: nn.Module)),post_optimizer_hook((self,model: Union[nn.Module,List[nn.Module]])),register_model_converter((converter_cls: type[ModelConverter],name: str)),__init__((self,job_config: JobConfig,parallel_dims: ParallelDims))[CTOR,DUNDER],convert((self,model: nn.Module)),post_optimizer_hook((self,model: Union[nn.Module,List[nn.Module]])),build_model_converters((job_config: JobConfig,parallel_dims: ParallelDims))[HOT]] moe_kernels.py→[generate_permute_indices((tokens_per_expert_group: torch.Tensor,experts_per_rank: int,num_ranks: int,max_len: int,alignment: int,use_cpu: bool = False,))→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},simple_test(void)→{indices::generate_permute_indices,indices::generate_permute_indices},fill_indices_wrapper((tokens_per_expert_group: torch.Tensor,start_index_values: torch.Tensor,write_offsets: torch.Tensor,experts_per_rank: int,num_ranks: int,max_len: int,block_size: int = 128,max_blocks: int = 1024,# cap on total number of blocks to launch))→
{train::main},fill_indices_cpu((tokens_per_expert_group: torch.Tensor,start_index_values: torch.Tensor,write_offsets: torch.Tensor,experts_per_rank: int,num_ranks: int,max_len: int,))→{train::main}] parallel_dims.py→[] parallelize.py→[parallelize_vlm((model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,))→{parallelize::apply_ddp,parallelize::apply_fsdp,parallelize::apply_compile,parallelize::apply_compile,activation_checkpoint::apply_ac,activation_checkpoint::apply_ac},parallelize_qwen3((model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,))→{apply_moe_ep_tp,parallelize::apply_ddp,parallelize::apply_fsdp,parallelize::apply_compile,activation_checkpoint::apply_ac,parallelize::apply_non_moe_tp},parallelize_llama((model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,))→{apply_moe_ep_tp,parallelize::apply_ddp,parallelize::apply_fsdp,parallelize::apply_compile,activation_checkpoint::apply_ac,tensor_parallel::maybe_enable_async_tp},parallelize_llama((model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,))→{simple_fsdp::data_parallel,activation_checkpoint::apply_ac,tensor_parallel::maybe_enable_async_tp,parallelize::apply_tp},parallelize_flux((model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,))→{parallelize::apply_fsdp,activation_checkpoint::apply_ac},apply_moe_ep_tp((model: nn.Module,tp_mesh: DeviceMesh | None,ep_mesh: DeviceMesh | None,ep_tp_mesh: DeviceMesh | None,etp_enabled: bool,))→{expert_parallel::expert_parallel,expert_parallel::expert_parallel},apply_fsdp((model: nn.Module,dp_mesh: DeviceMesh,param_dtype: torch.dtype,reduce_dtype: torch.dtype,cpu_offload: bool = False,)),apply_ac((model: nn.Module,ac_config)),apply_fsdp((model: nn.Module,dp_mesh: DeviceMesh,param_dtype: torch.dtype,reduce_dtype: torch.dtype,pp_enabled: bool,cpu_offload: bool = False,reshard_after_forward_policy: str = "default",)),parallelize_encoders((t5_model: nn.Module,clip_model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,)),apply_non_moe_tp((model: nn.Module,tp_mesh: DeviceMesh,loss_parallel: bool,enable_float8_tensorwise_tp: bool,enable_async_tp: bool,)),apply_non_moe_tp((model: nn.Module,tp_mesh: DeviceMesh,loss_parallel: bool,enable_float8_tensorwise_tp: bool,)),apply_fsdp((model: nn.Module,dp_mesh: DeviceMesh,param_dtype: torch.dtype,reduce_dtype: torch.dtype,pp_enabled: bool,cpu_offload: bool = False,reshard_after_forward_policy: str = "default",ep_degree: int = 1,dp_mod_ep_mesh: DeviceMesh | None = None,gradient_divide_factor: int | None = None,)),apply_compile((model: nn.Module))] parallelize_deepseek.py→[get_group((dim_name: Optional[str] = None)),parallelize_deepseek((# model: nn.Module,world_mesh: DeviceMesh,device: torch.device,model_args,rank: int,# parallel_dims: ParallelDims,# job_config: JobConfig,))] permute_indices_testing.py→[fill_indices_cpu((tokens_per_expert_group: torch.Tensor,start_index_values: torch.Tensor,write_offsets: torch.Tensor,experts_per_rank: int,num_ranks: int,max_len: int,))→{train::main},fill_indices_cpu((tokens_per_expert_group: torch.Tensor,start_index_values: torch.Tensor,write_offsets: torch.Tensor,experts_per_rank: int,num_ranks: int,max_len: int,))→{train::main},setUp((self)),setUp((self)),create_test_data((self,experts_per_rank: int,num_ranks: int,token_range: Tuple[int,int] = (1,16),alignment: int = 32,)),create_test_data((self,experts_per_rank: int,num_ranks: int,token_range: Tuple[int,int] = (1,16),alignment: int = 32,))] pipeline_parallel.py→[build_pipeline_schedule((job_config: JobConfig,stages: list[PipelineStage],loss_fn: Callable))[HOT]→
{loss::rescale_accumulated_loss},stage_ids_this_rank((pp_rank: int,pp_size: int,num_stages: int,style: str = "loop")),generate_llm_fqn_per_model_part((num_stages: int,num_layers: int,input_weight: int = 1,output_weight: int = 1,)),pipeline_module_split((whole_model: nn.Module,pp_mesh: DeviceMesh,pp_schedule: str,device: torch.device,module_names_per_stage: list[list[str]],))] profiling.py→[] reference_utils.py→[compute_reference_backward((x,w,m_sizes,grad_output))[HOT]→{compute_reference_forward},compute_reference_forward((x,w,m_sizes))[HOT],analyze_tensor_differences((actual,expected,name))] sampling.py→[denoise((device: torch.device,dtype: torch.dtype,model: FluxModel,img_width: int,img_height: int,denoising_steps: int,clip_encodings: torch.Tensor,t5_encodings: torch.Tensor,enable_classifier_free_guidance: bool = False,empty_t5_encodings: torch.Tensor | None = None,empty_clip_encodings: torch.Tensor | None = None,classifier_free_guidance_scale: float | None = None,))→{get_schedule,utils::unpack_latents,utils::pack_latents,utils::create_position_encoding_for_latents,utils::generate_noise_latent},generate_image((device: torch.device,dtype: torch.dtype,job_config: JobConfig,model: FluxModel,prompt: str | list[str],autoencoder: AutoEncoder,t5_tokenizer: BaseTokenizer,clip_tokenizer: BaseTokenizer,t5_encoder: FluxEmbedder,clip_encoder: FluxEmbedder,))→{denoise,utils::preprocess_data,utils::preprocess_data},get_schedule((num_steps: int,image_seq_len: int,base_shift: float = 0.5,max_shift: float = 1.15,shift: bool = True,))→{time_shift,get_lin_function},time_shift((mu: float,sigma: float,t: Tensor)),get_lin_function((x1: float = 256,y1: float = 0.5,x2: float = 4096,y2: float = 1.15)),save_image((name: str,output_dir: str,x: torch.Tensor,add_sampling_metadata: bool,prompt: str,))] siglip2.py→[forward((self,pixels_NLD: torch.Tensor,grid_hw: torch.Tensor))→{resize_positional_embeddings},__init__((self,args: Siglip2ModelArgs))[CTOR,DUNDER]→{attention::build_attention},__init__((self,args: Siglip2ModelArgs))[CTOR,DUNDER]→{math::attention},__init__((self,args: Siglip2ModelArgs))[CTOR,DUNDER]→{lr_scheduler::step},forward((self,pixel_values_NLD: torch.FloatTensor,pixel_masks_NL: torch.BoolTensor,grid_hw: torch.LongTensor,))→{attention::init_attention_mask},resize_positional_embeddings((pos_embs_HWD: torch.Tensor,spatial_shapes_N2: torch.Tensor,max_length: int,)),__init__((self,args: Siglip2ModelArgs))[CTOR,DUNDER],init_weights((self)),forward((self,x: torch.Tensor)),init_weights((self)),__init__((self,args: Siglip2ModelArgs))[CTOR,DUNDER],forward((self,x: torch.Tensor)),init_weights((self)),forward((self,x: torch.Tensor)),init_weights((self)),init_weights((self))] simpleMoE.py→[measure_performance((model: nn.Module,batch_size: int,seq_len: int,vocab_size: int,num_batches: int,device: str,))→{generate_sample_data,generate_sample_data,generate_sample_data},forward_mg_gemm((self,x: torch.Tensor))→{mg_grouped_gemm::grouped_gemm_forward,mg_grouped_gemm::grouped_gemm_forward},train_epoch((model: nn.Module,optimizer: torch.optim.Optimizer,batch_size: int,seq_len: int,vocab_size: int,num_batches: int,device: str,load_balance_coef: float = 0.01,))→{compute_load_balancing_loss,generate_sample_data},compare_methods((args))→{measure_performance,measure_performance},train_model((args))→{evaluate,train_epoch},evaluate((model: nn.Module,batch_size: int,seq_len: int,vocab_size: int,num_batches: int,device: str,))→
{generate_sample_data},__init__((self,input_dim: int,num_experts: int,top_k: int = 2))[CTOR,DUNDER],forward((self,x: torch.Tensor)),__init__((self,input_dim: int,hidden_dim: int,output_dim: int))[CTOR,DUNDER],forward((self,x: torch.Tensor)),__init__((self,input_dim: int,hidden_dim: int,output_dim: int,num_experts: int,top_k: int = 2,use_mg_gemm: bool = False,))[CTOR,DUNDER],forward_manual_loop((self,x: torch.Tensor)),forward((self,x: torch.Tensor)),__init__((self,vocab_size: int,embed_dim: int,hidden_dim: int,num_experts: int,top_k: int = 2,use_mg_gemm: bool = False,))[CTOR,DUNDER],forward((self,x: torch.Tensor)),compute_load_balancing_loss((router_logits: torch.Tensor,num_experts: int))[HOT],generate_sample_data((batch_size: int,seq_len: int,vocab_size: int,device: str = "cuda"))] simple_fsdp.py→[_register_parametrization((module: nn.Module,param_names: List[str],parametrization: nn.Module))→{math::rope},data_parallel((model,device_mesh,mode="replicate",ac_mode: str = "none",mp_policy: Optional[MixedPrecisionPolicy] = None,))→{_register_parametrization},_distribute_dtensor((tensor: DTensor,device_mesh: DeviceMesh,placements: Sequence[Placement],)),fsdp_policy(void),__init__((self,device_mesh,param_sharding,mode,regional_ac,mp_policy,))[CTOR,DUNDER],replicate_compute((self,x))[HOT],forward((self,x))] state_dict_adapter.py→[__init__((self,model_args: FluxModelArgs,hf_assets_path: str | None))[CTOR,DUNDER]→{math::rope},__init__((self,model_args: BaseModelArgs,hf_assets_path: str | None,))[CTOR,DUNDER]→{math::rope},__init__((self,model_args: TransformerModelArgs,hf_assets_path: str | None))[CTOR,DUNDER],__init__((self,model_args: Qwen3ModelArgs,hf_assets_path: str | None))[CTOR,DUNDER],to_hf((self,state_dict: dict[str,Any])),to_hf((self,state_dict: dict[str,Any])),from_hf((self,hf_state_dict: dict[str,Any])),from_hf((self,hf_state_dict: dict[str,Any])),_swap_scale_shift((self,weight)),to_hf((self,state_dict: dict[str,Any])),from_hf((self,hf_state_dict: dict[str,Any]))] tensor_parallel.py→[maybe_enable_async_tp((job_config: JobConfig,tp_mesh: DeviceMesh))] test_create_m_indices.py→[] tiktoken.py→[encode((self,s: str,*,bos: bool,eos: bool,allowed_special: Optional[Union[Literal["all"],AbstractSet[str]]] = None,disallowed_special: Optional[Union[Literal["all"],Collection[str]]] = None,))→{lr_scheduler::step,math::rope},encode_multimodal((self,sample: Mapping[str,Any]))→{lr_scheduler::step},__init__((self,model_path: str))[CTOR,DUNDER],decode((self,t: Sequence[int])),build_tiktoken_tokenizer((job_config: JobConfig))[HOT]] tma_autotuning.py→[__init__((self,tma_size: int = 128))[CTOR,DUNDER],init_tma_descriptor((self,name: str)),fill_1d_tma_descriptor((self,name: str,ptr: int,dim: int,block_dim: int,element_size: int)),fill_2d_tma_descriptor((self,name: str,ptr: int,dim1: int,dim0: int,block_dim1: int,block_dim0: int,element_size: int,)),get_tma_descriptor_kernel_param((self,name: str)),early_config_prune((configs,named_args,dtsize=None,dtype=None,**kwargs))] tma_cuda_autotune.py→[early_config_prune((configs,args,**kwargs))→{train::main},__init__((self,tma_size: int = 128))[CTOR,DUNDER],init_tma_descriptor((self,name: str)),fill_1d_tma_descriptor((self,name: str,ptr: int,dim: int,block_dim: int,element_size: int)),fill_2d_tma_descriptor((self,name: str,ptr: int,dim1: int,dim0: int,block_dim1: int,block_dim0: int,element_size: int,)),get_tma_descriptor_kernel_param((self,name: str))] tokenizer.py→
[__init__((self,model_path: str = "t5-small",max_length: int = 77,**hf_kwargs))[CTOR,DUNDER],_pad_and_chunk_tokens((self,tokens: List[int],max_length: int,pad_token: int)),build_mosaic_tokenizer((job_config: MosaicJobConfig,))[HOT],get_vocab_size((self)),encode((self,text: str | list[str])),decode((self,t: List[int])),__init__((self,model_path: str = "t5-small",max_length: int = 77,**hf_kwargs))[CTOR,DUNDER],get_vocab_size((self)),encode((self,s: str | list[str],)),decode((self,t: List[int])),build_flux_tokenizer((job_config: JobConfig))[HOT]] train.py→[main(void)[ENTRY]→{train_spec::register_train_spec,__init__::get_train_spec,__init__::get_train_spec,logging::init_logger},forward_backward_step((self,input_dict: dict[str,torch.Tensor],labels: torch.Tensor))→{utils::unpack_latents,utils::pack_latents,utils::create_position_encoding_for_latents,utils::preprocess_data},__init__((self,job_config: JobConfig))[CTOR,DUNDER]→{parallelize::parallelize_encoders,autoencoder::load_ae},batch_generator((self,data_iterable: Iterable[tuple[dict[str,torch.Tensor],torch.Tensor]]))→{lr_scheduler::step},forward_backward_step((self,input_dict: dict[str,torch.Tensor],labels: torch.Tensor))→{attention::init_attention_mask},train_step((self,data_iterator: Iterable[tuple[dict[str,torch.Tensor],torch.Tensor]])),should_continue_training((self)),state_dict((self)),load_state_dict((self,state_dict: dict[str,Any])),close((self))] train_ds_dev.py→[run_full_model((mesh: DeviceMesh,))] train_ds_real.py→[run_full_model((config: JobConfig,))→{next_batch,lr_scheduler::step,lr_scheduler::build_lr_schedulers,optimizer::build_optimizers,metrics::build_metrics_processor,hf_datasets::build_hf_dataloader},cross_entropy_loss((pred: torch.Tensor,labels: torch.Tensor)),next_batch((data_iterator: Iterable,metrics_processor))] train_spec.py→[get_train_spec((name: str))→{_transform_train_spec,_transform_train_spec},register_llama3_mosaic(void)→{register_train_spec},_transform_train_spec((original_spec: TrainSpec)),register_train_spec((train_spec: ForgeTrainSpec)),register_train_spec((train_spec: TrainSpec)),get_train_spec((name: str))] transform.py→[__call__((self,image: torch.Tensor))[DUNDER]→{utils::tile_crop,utils::resize_with_pad,utils::get_canvas_best_fit},__init__((self,*,image_mean: Optional[List[float]] = None,image_std: Optional[List[float]] = None,possible_resolutions: Optional[List[Tuple[int,int]]] = None,tile_size: int = 224,max_num_tiles: Optional[int] = 4,dtype: torch.dtype = torch.bfloat16,resample: str = "bilinear",resize_to_max_canvas: bool = False,))[CTOR,DUNDER]→{utils::find_supported_resolutions}] triton_barrier.py→[] triton_on_device_all_to_all_v.py→[_on_device_all_to_all_v((output: torch.Tensor,output_splits: torch.Tensor,input: torch.Tensor,input_splits: torch.Tensor,group: dist.ProcessGroup = dist.group.WORLD,BLOCKS_PER_REMOTE_RANK=8,UNROLL_FACTOR: int = 8,BLOCK_SIZE: int = 16384,))] triton_utils.py→[] unit_test_backwards.py→[_run_grouped_gemm_backward_test((self,shape: Tuple[int,int,int,int],device: torch.device,dtype: torch.dtype = torch.bfloat16,atol: float = 1e-5,rtol: float = 1.6e-2,))→{reference_utils::analyze_tensor_differences,reference_utils::analyze_tensor_differences,reference_utils::compute_reference_backward,mg_grouped_gemm::grouped_gemm_backward,reference_utils::analyze_tensor_differences,reference_utils::compute_reference_forward},setUp((self))] unit_test_cg.py→[benchmark_forward((self,M_total,K,N,num_experts,group_size_m,num_runs=10))→{train::main,cg_forward::cg_grouped_gemm_forward,cg_forward::cg_grouped_gemm_forward},benchmark_backward((self,M_total,K,N,num_experts,group_size_m,num_runs=5))→{train::main,cg_backward::cg_grouped_gemm,cg_backward::cg_grouped_gemm},verify_forward((self,M_total,K,N,num_experts,group_size_m,print_stats=False))→{train::main,cg_forward::cg_grouped_gemm_forward},verify_backward((self,M_total,K,N,num_experts,group_size_m,print_stats=False))→
{train::main,cg_backward::cg_grouped_gemm},run_tests((run_benchmarks=False))] unit_test_forwards.py→[_run_grouped_gemm_test((self,shape: Tuple[int,int,int,int],device: torch.device,dtype: torch.dtype = torch.bfloat16,atol: float = 1e-5,rtol: float = 1.6e-2,))→{mg_grouped_gemm::grouped_gemm_forward},setUp((self))] utils.py→[resize_with_pad((image: torch.Tensor,target_size: Tuple[int,int],resample: torchvision.transforms.InterpolationMode,max_size: Optional[int] = None,))→{_pad_image_top_left,_get_max_res_without_distortion,train::main,train::main,train::main,train::main},set_determinism((world_mesh: DeviceMesh | None,device: torch.device,seed: int | None = None,deterministic: bool = False,distinct_seed_mesh_dim: str = "pp",))→{lr_scheduler::step,lr_scheduler::step},_get_max_res_without_distortion((image_size: Tuple[int,int],target_size: Tuple[int,int],))→{train::main,train::main},dist_max((x: torch.Tensor,mesh: DeviceMesh,extra_pg: dist.ProcessGroup | None = None,))→{_dist_reduce},dist_sum((x: torch.Tensor,mesh: DeviceMesh,extra_pg: dist.ProcessGroup | None = None,))→{_dist_reduce},dist_mean((x: torch.Tensor,mesh: DeviceMesh,extra_pg: dist.ProcessGroup | None = None,))→{_dist_reduce},_get_factors((n: int))→{lr_scheduler::step},find_supported_resolutions((max_num_tiles: int,tile_size: int))→{_get_factors},preprocess_data((# arguments from the recipe device: torch.device,dtype: torch.dtype,*,# arguments from the config autoencoder: Optional[AutoEncoder],clip_encoder: FluxEmbedder,t5_encoder: FluxEmbedder,batch: dict[str,Tensor],)),has_cuda_capability((major: int,minor: int)),tile_crop((image: torch.Tensor,tile_size: int)),_dist_reduce((x: torch.Tensor,reduceOp: str,mesh: DeviceMesh,extra_pg: dist.ProcessGroup | None,)),get_device_info(void),__init__((self,gc_freq: int = 1000,debug: bool = False))[CTOR,DUNDER],run((self,step_count: int)),generate_noise_latent((bsz: int,height: int,width: int,device: str | torch.device,dtype: torch.dtype,seed: int | None = None,)),get_peak_flops((device_name: str)),create_position_encoding_for_latents((bsz: int,latent_height: int,latent_width: int,position_dim: int = 3)),pack_latents((x: Tensor)),_pad_image_top_left((image: torch.Tensor,target_size: Tuple[int,int],)),unpack_latents((x: Tensor,latent_height: int,latent_width: int)),check_if_feature_in_pytorch((feature_name: str,pull_request: str,min_nightly_version: Optional[str] = None,)),create_context_parallel_ctx((cp_mesh: DeviceMesh,cp_buffers: list[torch.Tensor],cp_seq_dims: list[int],cp_no_restore_buffers: set[torch.Tensor],cp_rotate_method: str,)),get_train_context((enable_loss_parallel: bool,enable_compiled_autograd: bool)),_round_up((x: int,y: int)),maybe_enable_amp((parallel_dims: ParallelDims,mixed_precision_param: str,device_type: torch.device)),init_distributed((comm_config: CommConfig,enable_cpu_backend: bool = False,base_folder: str = "")),get_canvas_best_fit((image: torch.Tensor,possible_resolutions: torch.Tensor,resize_to_max_canvas: bool)),set_pg_timeouts((timeout,world_mesh)),load_image((image_loc: Union[Path,str]))] validate.py→[__init__((self,job_config: JobConfig,dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer,parallel_dims: ParallelDims,loss_fn: LossFunction,validation_context: Generator[None,None,None],maybe_enable_amp: Generator[None,None,None],metrics_processor: MetricsProcessor | None = None,pp_schedule: _PipelineSchedule | None = None,pp_has_first_stage: bool | None = None,pp_has_last_stage: bool | None = None,))[CTOR,DUNDER]→
{tokenizer::build_flux_tokenizer,flux_dataset::build_flux_validation_dataloader},flux_init((self,device: torch.device,_dtype: torch.dtype,autoencoder: AutoEncoder,t5_encoder: FluxEmbedder,clip_encoder: FluxEmbedder,)),build_flux_validator((job_config: JobConfig,dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer,parallel_dims: ParallelDims,loss_fn: LossFunction,validation_context: Generator[None,None,None],maybe_enable_amp: Generator[None,None,None],metrics_processor: MetricsProcessor | None = None,pp_schedule: _PipelineSchedule | None = None,pp_has_first_stage: bool | None = None,pp_has_last_stage: bool | None = None,))[HOT]] 
### UTILS
NODES:29 CALL_DEPTH:5

image.py→[_smart_resize((height: int,width: int,factor: int,# should be equal patch_size * merge_size max_patch_per_image: int,min_patch_per_image: int = 1,))→{train::main,train::main,train::main,train::main},_resize_image_by_patch_count((image: Image.Image,max_patch_per_image: int,patch_size: int,merge_size: int,min_patch_per_image: int = 1,))→{_smart_resize,_smart_resize,_smart_resize,train::main},process_image((image: str | bytes | Image.Image,patch_size: int = 16,merge_size: int = 1,max_patch_per_image: int = 256,min_patch_per_image: int = 1,))→{_resize_image_by_patch_count},calculate_image_tokens((image: Image.Image | torch.Tensor,patch_size: int,spatial_merge_size: int,)),convert_to_patches((pixel_values: torch.Tensor,patch_size: int,temporal_patch_size: int = 1,)),pad_patches((patches: torch.Tensor,grids: torch.Tensor,max_patches: int,)),pad_empty_images_to_target_batch_size((patches: torch.Tensor,grids: torch.Tensor,max_images: int,))] packing.py→[__init__((self,max_seq_length: int,buffer_size: int = 100,batch_size: int = 8,))[CTOR,DUNDER],_pack_buffered_samples((self)),add_sample((self,sample: dict[str,Any])),has_batch_ready((self)),get_next_batch((self))] text.py→[pad_text_batch((input_ids: torch.Tensor,labels: torch.Tensor,seq_len: int,padding_idx: int = 0,ignore_idx: int = -100,)),pad_input_ids_and_labels_to_target_batch_size((input_ids: torch.Tensor,labels: torch.Tensor,target_batch_size: int,padding_idx: int = 0,ignore_idx: int = -100,)),process_text_with_images((text: list[str],image_tokens: list[tuple[int,int,int]],# [(total,width,height),...] tokenizer,special_tokens,add_eos: bool = True,))] 

## DEPENDENCY_PATTERNS

### EDGE_PATTERNS
Call: 502 edges
Contains: 565 edges

### CROSS_CLUSTER_FLOW
DATA_MODELS→UI_COMPONENTS: 2
DATA_MODELS→UTILITY_LAYER: 14
TESTS→UI_COMPONENTS: 4
TESTS→UTILITY_LAYER: 84
UI_COMPONENTS→UTILITY_LAYER: 19
UTILITY_LAYER→DATA_MODELS: 41
UTILITY_LAYER→TESTS: 23
UTILITY_LAYER→UI_COMPONENTS: 57
UTILITY_LAYER→UTILS: 3
UTILS→UTILITY_LAYER: 5


# EMBARGO: LLM-Optimized Codebase Dependency Graph

**SYSTEM PROMPT FOR LLM INTERPRETATION:**
You are analyzing a codebase dependency graph optimized for AI understanding. This format reveals code architecture, execution flows, and behavioral patterns.

## INTERPRETATION KEY

### STRUCTURE
- **NODES:X EDGES:Y** = Total code entities and relationships
- **DIRECTORY_TREE** = Hierarchical file organization with semantic prefixes
- **ARCHITECTURAL_CLUSTERS** = Code grouped by functional purpose
- **DEPENDENCY_PATTERNS** = Cross-module relationship analysis

### BEHAVIORAL NOTATION
- **filename.rs→[...]** = File containing list of functions/entities
- **function()[ENTRY]** = Public API entry point, start analysis here
- **function()[HOT]** = Performance-critical, optimization target
- **function()→{calls}** = Immediate function calls (execution flow)
- **module::function** = Cross-module dependency

### ANALYSIS GUIDANCE
1. **Entry Points**: Start with [ENTRY] functions to understand public APIs
2. **Execution Flow**: Follow →{calls} to trace code execution paths
3. **Hot Paths**: Focus [HOT] functions for performance analysis
4. **Architecture**: Use clusters to understand system organization
5. **Dependencies**: Cross-cluster flows show coupling patterns

### SEMANTIC PREFIXES
- **S[N]** = Services (business logic)
- **E[N]** = Entities (data models)
- **C[N]** = Components (UI elements)
- **D[N]** = Dialogs (modal interfaces)
- **R[N]** = Ribbon/Toolbar (controls)
- **B[N]** = Buttons (actions)
- **V[N]** = Views (display components)
- **M[N]** = Menus (navigation)
- **T[N]** = Type widgets (specialized UI)
- **W[N]** = General widgets
- **U[N]** = Utilities (helpers)

### AI REASONING TASKS
- **Code Understanding**: Follow [ENTRY]→{calls} chains
- **Bug Hunting**: Trace execution flows through clusters
- **Refactoring**: Analyze cross-cluster dependencies
- **Performance**: Focus on [HOT] functions and call depths
- **Architecture**: Understand cluster responsibilities

---

# CODE_GRAPH
NODES:4618 EDGES:2799

## DIRECTORY_TREE
ROOT: ./
├─ private_photon/ → E[36] U[121]
│  ├─ notebooks/ → U[1]
│  ├─ photon/ → E[36] U[114]
│  │  ├─ callbacks/ → U[2]
│  │  ├─ clients/ → U[6]
│  │  ├─ composer_callbacks/ → U[6]
│  │  ├─ composer_trainer_utils/ → U[3]
│  │  ├─ conf/ → U[2]
│  │  ├─ dataset/ → U[14]
│  │  │  └─ constants/ → U[6]
│  │  ├─ eval_light/ → U[5]
│  │  ├─ initialization_utils/ → U[1]
│  │  ├─ messages/ → U[3]
│  │  ├─ metrics/ → U[2]
│  │  ├─ models/ → E[36]
│  │  │  ├─ deptoe/ → E[9]
│  │  │  ├─ embeddings/ → E[1]
│  │  │  ├─ ffns/ → E[10]
│  │  │  ├─ mpt/ → E[6]
│  │  │  ├─ rope/ → E[2]
│  │  │  └─ switch_head/ → E[3]
│  │  ├─ moe_utils/ → U[3]
│  │  ├─ node_manager/ → U[3]
│  │  ├─ optimizers/ → U[6]
│  │  ├─ pytorch_utils/ → U[5]
│  │  ├─ schedulers/ → U[2]
│  │  ├─ server/ → U[19]
│  │  │  └─ fit_eval/ → U[5]
│  │  ├─ shm/ → U[1]
│  │  ├─ storage/ → U[4]
│  │  ├─ strategy_functional/ → U[7]
│  │  ├─ tools/ → U[3]
│  │  └─ worker/ → U[2]
│  └─ scripts/ → U[6]
│     ├─ amd/ → U[4]
│     │  └─ helpers/ → U[1]
│     └─ deptoe/ → U[2]
│        └─ scaling_exp/ → U[2]
│           └─ helpers/ → U[1]
├─ scripts/ → U[7]
│  ├─ checkpoint_conversion/ → U[3]
│  ├─ estimate/ → U[1]
│  └─ generate/ → U[2]
├─ tests/ → TST[16]
│  ├─ assets/ → TST[1]
│  ├─ integration_tests/ → TST[6]
│  └─ unit_tests/ → TST[9]
└─ torchtitan/ → C[19] E[15] TST[8] U[124]
   ├─ components/ → C[19]
   │  ├─ ft/ → C[7]
   │  │  ├─ config/ → C[2]
   │  │  └─ diloco/ → C[3]
   │  └─ quantization/ → C[4]
   ├─ config/ → U[3]
   ├─ datasets/ → U[2]
   ├─ distributed/ → U[7]
   ├─ experiments/ → TST[8] U[102]
   │  ├─ deepseek_v3/ → U[25]
   │  │  ├─ infra/ → U[1]
   │  │  ├─ symm_mem_recipes/ → U[4]
   │  │  ├─ tokenizers/ → U[1]
   │  │  ├─ train_configs/ → U[1]
   │  │  └─ unit_testing/ → U[4]
   │  ├─ flux/ → TST[3] U[19]
   │  │  ├─ dataset/ → U[2]
   │  │  ├─ inference/ → U[1]
   │  │  ├─ infra/ → U[1]
   │  │  ├─ model/ → U[7]
   │  │  ├─ scripts/ → U[1]
   │  │  └─ tests/ → TST[3]
   │  │     ├─ assets/ → TST[1]
   │  │     │  └─ cc12m_test/ → TST[1]
   │  │     └─ unit_tests/ → TST[1]
   │  ├─ forge/ → U[5]
   │  ├─ kernels/ → TST[1] U[18]
   │  │  ├─ moe/ → TST[1] U[3]
   │  │  │  └─ unit_tests/ → TST[1]
   │  │  ├─ triton_contiguous_group_gemm/ → U[6]
   │  │  └─ triton_mg_group_gemm/ → U[9]
   │  │     └─ torchao_pr/ → U[7]
   │  ├─ llama4/ → U[7]
   │  │  ├─ infra/ → U[1]
   │  │  ├─ model/ → U[3]
   │  │  └─ scripts/ → U[2]
   │  ├─ multimodal/ → TST[2] U[8]
   │  │  ├─ tests/ → TST[2]
   │  │  └─ tokenizer/ → U[1]
   │  ├─ qwen3/ → U[5]
   │  │  ├─ infra/ → U[1]
   │  │  └─ model/ → U[3]
   │  ├─ simple_fsdp/ → TST[2] U[4]
   │  │  └─ tests/ → TST[2]
   │  └─ vlm/ → U[11]
   │     ├─ assets/ → U[1]
   │     ├─ datasets/ → U[5]
   │     │  └─ utils/ → U[3]
   │     ├─ infra/ → U[1]
   │     └─ model/ → U[3]
   ├─ models/ → E[15]
   │  ├─ deepseek_v3/ → E[6]
   │  │  ├─ infra/ → E[1]
   │  │  └─ model/ → E[4]
   │  ├─ llama3/ → E[6]
   │  │  ├─ infra/ → E[2]
   │  │  └─ model/ → E[3]
   │  └─ llama3_ft/ → E[1]
   ├─ protocols/ → U[5]
   └─ tools/ → U[3]

## ARCHITECTURAL_CLUSTERS

### DATA_MODELS
NODES:596 CALL_DEPTH:7

__init__.py→[get_train_spec(void),get_train_spec(void),get_train_spec(void)] args.py→[] attention.py→[__init__((self,attn_mask_type: str,fixed_block_size: int | None = None))[CTOR,DUNDER],forward((self,q: torch.Tensor,k: torch.Tensor,v: torch.Tensor,scale: float | None = None,)),__init__((self,attn_mask_type: str))[CTOR,DUNDER],forward((self,q: torch.Tensor,k: torch.Tensor,v: torch.Tensor,scale: float | None = None,)),build_attention((use_flex_attn: bool,attn_mask_type: str,fixed_block_size: int | None = None))[HOT],init_attention_mask((batch: torch.Tensor,eos_id: int | None,cp_mesh: torch.distributed.device_mesh.DeviceMesh | None = None,))] builder.py→[build_sigma_moe((# noqa: PLR0913,PLR0917 d_model: int,ff_n_experts: int,n_repeats: int,ff_expert_size: int,ff_n_active_experts: int,v_dim: int | None = None,mup_config: dict | None = None,ffn_act_fn: dict | None = None,expert_dropout: float = 0.0,init_std: float = 0.02,r_std_multiplier: float = 1.0,e_std_multiplier: float = 1.0,*,expansion_ratio: float | None = None,# noqa: ARG001 device: str | None = None,# noqa: ARG001 bias: bool = False,# noqa: ARG001 fc_type: str = "torch",# noqa: ARG001 cvmm_kernel: bool = False,deepgemm_kernel: bool = False,h100_fp8_train: bool = False,))[HOT],sigma_moe_init_sp((module: torch.nn.Module,init_fn_: Callable,**kwargs: Any,# noqa: ANN401))] configuration_deptoe.py→[__init__((# noqa: PLR0913,PLR0917 self,d_model: int = 2048,n_heads: int = 16,n_layers: int = 24,max_seq_len: int = 2048,vocab_size: int = 50368,embedding_dropout: float = 0.0,init_device: str = "cpu",logit_scale: float | str | None = None,embedding_fraction: float = 1.0,out_norm_type: str = "low_precision_layernorm",out_norm_eps: float = 1e-05,init_config: dict | None = None,final_logit_softcapping: float | None = None,# SwitchHeadConfig parameters attn_heads_experts: int = 1,attn_heads_dropout: float = 0.0,attn_heads_projection_size: int = 64,attn_heads_experts_dropout: float = 0.0,attn_heads_n_active_experts: int = 1,# SigmaMoE parameters ff_n_experts: int = 1,ff_expert_size: int = 64,ff_n_active_experts: int = 1,# MoEUT regularization parameters entropy_reg: float = 0.01,att_entropy_reg: float = 0.001,# UT number of repeats n_repeats: int = 1,# RoPE parameters rope_base: int = 10000,*,use_flash_attn: bool = True,attn_xpos_scale_base: int | None = None,use_torch_apply_rotary_emb: bool = False,use_peri_ln: bool = False,use_embedding_peri_ln: bool = False,attn_cvmm_kernel: bool = False,ff_cvmm_kernel: bool = False,rope_interleaved: bool = False,use_cache: bool = False,tie_word_embeddings: bool = True,no_bias: bool = True,# MuP parameters,mup_config: dict[str,Any] | None = None,**kwargs: Any,# noqa: ANN401))[CTOR,DUNDER]→{runtime_estimator::init,math::rope,runtime_estimator::init},_set_config_defaults((self,config: dict[str,Any],config_defaults: dict[str,Any],)),validate_config((self))] cvmm_triton_kernel.py→[cvmm((x: torch.Tensor,sel: torch.Tensor | CVMMSel,keys: torch.Tensor,))→{cvmm_prepare_sel,optimizer_states_setters::_cast},cvmm_triton_backward((# noqa: PLR0913,PLR0917 x: torch.Tensor,sel_index: torch.Tensor,sel: torch.Tensor,grads: torch.Tensor,n_experts: int,key_dtype: torch.dtype,op_float16: bool,# noqa: ARG001,FBT001 out_index: torch.Tensor,))→{optimizer_states_setters::_cast},is_cvmm_available(void),_safe_autotune((# pyright: ignore[reportUnusedFunction] *args: object,**kwargs: object,)),_safe_jit((fn: Callable[...,Any])),cvmm_prepare_sel((sel: torch.Tensor)),cvmm_prepare_sel2((sel: torch.Tensor,w: torch.Tensor | None = None))] cvmm_triton_kernel_fp8_activations.py→[cvmm_fp8((x: torch.Tensor,sel: torch.Tensor | CVMMSel,keys: torch.Tensor,fp8_format: Literal["e4m3","e5m2"] = "e4m3",))→{optimizer_states_setters::_cast,cvmm_triton_kernel::cvmm_prepare_sel},_quantize_per_row_int8((x: torch.Tensor,eps: float = 1e-8,))] cvmm_triton_kernel_orig.py→[cvmm_triton_backward((# noqa: PLR0913,PLR0917 x: torch.Tensor,sel_index: torch.Tensor,sel: torch.Tensor,grads: torch.Tensor,n_experts: int,key_dtype: torch.dtype,op_dtype: torch.dtype,out_index: torch.Tensor,))→{dtype_to_type_id,dtype_to_type_id},cvmm((x: torch.Tensor,sel: torch.Tensor | CVMMSel,keys: torch.Tensor,))→{optimizer_states_setters::_cast,cvmm_triton_kernel::cvmm_prepare_sel},get_dtype(void),dtype_to_type_id((dtype: torch.dtype)),cvmm_prepare_sel((sel: torch.Tensor)),cvmm_prepare_sel2((sel: torch.Tensor,w: torch.Tensor | None = None))] deepgemm_moe.py→[moe_forward_deepgemm((# noqa: PLR0912,PLR0913,PLR0914,PLR0917 x: torch.Tensor,sel_index: torch.Tensor,sel_weight: torch.Tensor,up_proj: torch.Tensor,down_proj: torch.Tensor,activation: Callable[[torch.Tensor],torch.Tensor] = F.silu,*,out_dtype: torch.dtype = torch.bfloat16,))→{_get_buffer,_buffer_pool_enabled,_get_buffer,_buffer_pool_enabled,_cast_tokens_fp8,_get_down_weights_fp8_cached},_dense_single_expert_forward((x: torch.Tensor,up_weight: torch.Tensor,down_weight: torch.Tensor,activation: Callable[[torch.Tensor],torch.Tensor],out_dtype: torch.dtype,))→{_cast_tokens_fp8,_get_buffer,_buffer_pool_enabled,_get_down_weights_fp8_cached,_get_buffer,_buffer_pool_enabled},_dense_all_experts_forward((# noqa: PLR0914 x: torch.Tensor,sel_weight: torch.Tensor,weights: tuple[torch.Tensor,torch.Tensor],activation: Callable[[torch.Tensor],torch.Tensor],out_dtype: torch.dtype,))→{_get_buffer,_buffer_pool_enabled,_get_buffer,_buffer_pool_enabled,_cast_tokens_fp8,_get_down_weights_fp8_cached},_build_grouped_layout((# noqa: PLR0914 sel_index: torch.Tensor,sel_weight: torch.Tensor,n_experts: int,))[HOT]→{runtime_estimator::init,runtime_estimator::init,runtime_estimator::init,runtime_estimator::init,dsgemm_utils::get_m_alignment_for_contiguous_layout},_cast_grouped_weights_fp8((w_bf16: torch.Tensor,))→{_use_ue8m0,dsgemm_unit_testing::per_block_cast_to_fp8,dsgemm_utils::ceil_div,dsgemm_utils::ceil_div},_build_all_experts_layout((sel_weight: torch.Tensor))[HOT]→{runtime_estimator::init,runtime_estimator::init,dsgemm_utils::get_m_alignment_for_contiguous_layout},dense_forward_deepgemm((# noqa: PLR0913 x: torch.Tensor,sel_weight: torch.Tensor | None,up_proj: torch.Tensor,down_proj: torch.Tensor,activation: Callable[[torch.Tensor],torch.Tensor] = F.silu,*,out_dtype: torch.dtype = torch.bfloat16,))→{_dense_all_experts_forward,_dense_single_expert_forward,is_deepgemm_available},_cast_tokens_fp8((a_bf16: torch.Tensor))→{_use_ue8m0,dsgemm_unit_testing::per_token_cast_to_fp8},_get_up_weights_fp8_cached((up_bf16: torch.Tensor,))→{_cast_grouped_weights_fp8},_get_down_weights_fp8_cached((down_bf16: torch.Tensor,))→{_cast_grouped_weights_fp8},_use_ue8m0(void),is_deepgemm_available(void),_buffer_pool_enabled(void),_get_buffer((shape: tuple[int,...],dtype: torch.dtype,device: torch.device,*,zero: bool = False,)),clear_fp8_weight_cache(void)] deptoe.py→[forward((# noqa: PLR0913 self,input_ids: torch.LongTensor | None = None,attention_mask: torch.ByteTensor | None = None,*,return_dict: bool | None = None,output_attentions: bool | None = None,output_hidden_states: bool | None = None,use_cache: bool | None = None,))→{optimizer_states_setters::_cast,optimizer_states_setters::_cast,optimizer_states_setters::_cast,optimizer_states_setters::_cast,optimizer_states_setters::_cast,optimizer_states_setters::_cast},__init__((self,config: DEPToEConfig))[CTOR,DUNDER]→{lr_scheduler::step,metrics::log,metrics::log,math::rope,optimizer_states_setters::_cast},apply_embedding_layer((self,input_ids: torch.LongTensor | None))→{optimizer_states_setters::_cast},reset_parameters((self)),construct_blocks((self,config: DEPToEConfig)),get_input_embeddings((self)),set_input_embeddings((# type: ignore[reportIncompatibleMethodOverride] self,value: SharedEmbedding | nn.Embedding,)),_forward_checks((self,attention_mask: torch.ByteTensor | None = None,input_ids: torch.LongTensor | None = None,*,return_dict: bool | None = None,output_attentions: bool | None = None,use_cache: bool | None = None,))] deptoe_block.py→[forward((self,x: torch.Tensor,past_key_value: tuple[torch.Tensor,torch.Tensor] | None = None,attention_mask: torch.ByteTensor | None = None,*,output_attentions: bool = False,))→{optimizer_states_setters::_cast,optimizer_states_setters::_cast},__init__((# noqa: PLR0913,PLR0917 self,d_model: int,n_heads: int,n_repeats: int = 1,n_blocks: int = 1,attn_heads_experts: int = 1,attn_heads_dropout: float = 0.0,attn_heads_projection_size: int = 64,attn_heads_experts_dropout: float = 0.0,attn_heads_n_active_experts: int = 1,rope_base: int = 10000,ff_n_experts: int = 1,ff_expert_size: int = 64,ff_n_active_experts: int = 1,ff_expert_dropout: float = 0.0,device: str | None = None,*,use_flash_attn: bool = True,use_peri_ln: bool = False,rope_interleaved: bool = False,ff_cvmm_kernel: bool = False,ff_deepgemm_kernel: bool = False,ff_h100_fp8_train: bool = False,attn_cvmm_kernel: bool = False,init_std: float = 0.02,no_bias: bool = False,mup_config: MuPConfig | None = None,))[CTOR,DUNDER],__init__((self,config: DEPToEBlockConfig,rope_module: RotaryPositionalEncoding | None = None,))[CTOR,DUNDER]] deptoe_causal_lm.py→[] deptoe_for_causal_lm.py→[forward((# noqa: PLR0913 self,input_ids: torch.LongTensor | None = None,attention_mask: torch.ByteTensor | None = None,labels: torch.LongTensor | None = None,*,return_dict: bool | None = None,output_attentions: bool | None = None,output_hidden_states: bool | None = None,use_cache: bool | None = None,))→{optimizer_states_setters::_cast,optimizer_states_setters::_cast,optimizer_states_setters::_cast,optimizer_states_setters::_cast,optimizer_states_setters::_cast,optimizer_states_setters::_cast},activation_checkpointing_fn((self,module: nn.Module))→{optimizer_states_setters::_cast,runtime_estimator::init,optimizer_states_setters::_cast,runtime_estimator::init},get_optimizer_param_groups((# noqa: C901,PLR0915,PLR0914,PLR0912 self,optimizer_config: dict[str,Any],))→{permute_indices_testing::setUp,optimizer_states_setters::_cast},__init__((self,config: DEPToEConfig))[CTOR,DUNDER]→{optimizer_states_setters::_cast},set_output_embeddings((self,new_embeddings: SharedEmbedding | nn.Embedding | nn.Linear,))→{math::rope},prepare_inputs_for_generation((self,input_ids: torch.Tensor,past_key_values: Cache | None = None,attention_mask: torch.LongTensor | None = None,inputs_embeds: torch.FloatTensor | None = None,cache_position: torch.LongTensor | None = None,# noqa: ARG002 **kwargs: Any,# noqa: ANN401))→{math::rope},reset_parameters((self)),param_init_fn((self,module: nn.Module)),get_input_embeddings((self)),set_input_embeddings((# type: ignore[reportIncompatibleMethodOverride] self,value: SharedEmbedding | nn.Embedding,)),get_output_embeddings((self,)),tie_weights((self)),set_decoder((self,decoder: DEPToEModel)),get_decoder((self)),_reorder_cache((# noqa: PLR6301 self,past_key_values: list[tuple[torch.Tensor,torch.Tensor]],beam_idx: torch.LongTensor,))] forward_cvmm.py→[forward_cvmm_kernel((# noqa: PLR0913,PLR0917 x: torch.Tensor,sel_val: torch.Tensor,sel_index: torch.Tensor,up_proj: torch.nn.Parameter,down_proj: torch.nn.Parameter,activation: Callable[[torch.Tensor],torch.Tensor],))→{cvmm_triton_kernel::cvmm,cvmm_triton_kernel::cvmm,cvmm_triton_kernel::cvmm_prepare_sel2}] forward_cvmm_h100_fp8.py→[forward_cvmm_fp8_kernel((# noqa: PLR0913,PLR0917 x: torch.Tensor,sel_val: torch.Tensor,sel_index: torch.Tensor,up_proj: torch.nn.Parameter,down_proj: torch.nn.Parameter,activation: Callable[[torch.Tensor],torch.Tensor],))→{cvmm_triton_kernel_fp8_activations::cvmm_fp8,cvmm_triton_kernel_fp8_activations::cvmm_fp8,cvmm_triton_kernel::cvmm_prepare_sel2}] forward_gemm.py→[forward_deepgemm((# noqa: PLR0913,PLR0917 x: torch.Tensor,sel_val: torch.Tensor,sel_index: torch.Tensor | None,up_proj: torch.Tensor,down_proj: torch.Tensor,activation: Callable[[torch.Tensor],torch.Tensor],))→{deepgemm_moe::moe_forward_deepgemm,deepgemm_moe::dense_forward_deepgemm}] forward_no_kernel.py→[forward_sparse((# noqa: PLR0913,PLR0914,PLR0917 x: torch.Tensor,sel_val: torch.Tensor,sel_index: torch.Tensor,up_proj: torch.nn.Parameter,down_proj: torch.nn.Parameter,activation: Callable[[torch.Tensor],torch.Tensor],))→{runtime_estimator::init,runtime_estimator::init},forward_dense_single_expert((x: torch.Tensor,up_proj: torch.nn.Parameter,down_proj: torch.nn.Parameter,activation: Callable[[torch.Tensor],torch.Tensor],)),forward_dense_all_experts((x: torch.Tensor,sel_val: torch.Tensor,up_proj: torch.Tensor,down_proj: torch.Tensor,activation: Callable[[torch.Tensor],torch.Tensor],))] forward_router.py→[forward_router((# noqa: PLR0913 x: torch.Tensor,expert_sel: torch.nn.Parameter,n_active_experts: int,sel_input: torch.Tensor | None = None,expert_dropout: float = 0.0,*,is_training: bool = True,))→{comms_ray_batch_variants::load}] init_config.py→[] model.py→[forward((self,x: torch.Tensor,freqs_cis: torch.Tensor,))→{repeat_kv,repeat_kv,apply_rotary_emb},forward((self,x: torch.Tensor,freqs_cis: torch.Tensor,))→{apply_rotary_emb,apply_rotary_emb},__init__((self,dim: int,hidden_dim: int,multiple_of: int,ffn_dim_multiplier: float | None,))[CTOR,DUNDER]→{runtime_estimator::init,runtime_estimator::init},__init__((self,layer_id: int,model_args: DeepSeekV3ModelArgs))[CTOR,DUNDER]→{math::rope,math::attention},__init__((self,model_args: DeepSeekV3ModelArgs))[CTOR,DUNDER]→{precompute_freqs_cis,lr_scheduler::step},apply_rotary_emb((xq: torch.Tensor,xk: torch.Tensor,freqs_cis: torch.Tensor,))→{reshape_for_broadcast},__init__((self,model_args: TransformerModelArgs))[CTOR,DUNDER]→{attention::build_attention},__init__((self,model_args: DeepSeekV3ModelArgs))[CTOR,DUNDER]→{attention::build_attention},__init__((self,layer_id: int,model_args: TransformerModelArgs))[CTOR,DUNDER]→{math::attention},__init__((self,model_args: TransformerModelArgs))[CTOR,DUNDER]→{lr_scheduler::step},init_weights((self,buffer_device: torch.device | None = None))→{precompute_freqs_cis},_precompute_freqs_cis((self))[HOT]→{precompute_freqs_cis},precompute_freqs_cis((dim: int,end: int,theta: float = 10000.0))[HOT],precompute_freqs_cis((args: DeepSeekV3ModelArgs))[HOT],reshape_for_broadcast((freqs_cis: torch.Tensor,x: torch.Tensor)),repeat_kv((x: torch.Tensor,n_rep: int)),apply_rotary_emb((x: torch.Tensor,freqs_cis: torch.Tensor)),init_weights((self,init_std: float)),forward((self,x)),init_weights((self,init_std: float)),init_weights((self,init_std: float)),forward((self,x: torch.Tensor,freqs_cis: torch.Tensor,)),forward((self,x: torch.Tensor,freqs_cis: torch.Tensor)),init_weights((self)),init_weights((self,buffer_device: torch.device)),init_weights((self,buffer_device: torch.device | None = None,)),forward((self,tokens: torch.Tensor,input_batch: torch.Tensor | None = None,)),forward((self,tokens: torch.Tensor,input_batch: torch.Tensor | None = None,))] moe.py→[__init__((self,dim: int,hidden_dim: int,))[CTOR,DUNDER],forward((self,x: torch.Tensor)),init_weights((self,init_std: float = 0.02)),__init__((self,dim: int,hidden_dim: int,num_experts: int,use_grouped_mm: bool,))[CTOR,DUNDER],forward((self,x: torch.Tensor,num_tokens_per_expert: torch.Tensor,)),init_weights((self,init_std: float)),__init__((self,dim: int,num_experts: int,top_k: int,score_func: Literal["softmax","sigmoid"],route_norm: bool,route_scale: float,))[CTOR,DUNDER],forward((self,x: torch.Tensor,expert_bias: torch.Tensor | None = None)),init_weights((self,init_std: float)),__init__((self,num_experts: int,top_k: int))[CTOR,DUNDER],forward((self,top_scores: torch.Tensor,selected_experts_indices: torch.Tensor,)),__init__((self,moe_args: MoEArgs,dim: int,hidden_dim: int))[CTOR,DUNDER],forward((self,x: torch.Tensor)),init_weights((self,init_std: float,buffer_device: torch.device,))] mpt_for_question_answering.py→[forward((# noqa: PLR0914 self,batch: MutableMapping,))→{optimizer_states_setters::_cast,optimizer_states_setters::_cast,optimizer_states_setters::_cast,optimizer_states_setters::_cast,numerical_tests_example::loss_fn,numerical_tests_example::loss_fn},eval_forward((self,batch: MutableMapping,outputs: QuestionAnsweringModelOutput | None = None,))→{math::rope,math::rope},__init__((self,model: ComposerMPTCausalLM,hidden_size: int,dropout_rate: float = 0.1,train_metrics: list[Metric] | None = None,eval_metrics: list[Metric] | None = None,))[CTOR,DUNDER]→{optimizer_states_setters::_cast}] mpt_for_sequence_classification.py→[forward((# noqa: C901,PLR0912,PLR0914 self,batch: MutableMapping,))→{optimizer_states_setters::_cast,optimizer_states_setters::_cast,numerical_tests_example::loss_fn,numerical_tests_example::loss_fn,loss::cross_entropy_loss,numerical_tests_example::loss_fn},__init__((# noqa: PLR0913,PLR0917 self,model: ComposerMPTCausalLM,num_labels: int,hidden_size: int,pad_token_id: int | None,problem_type: str | None = None,train_metrics: list[Metric] | None = None,eval_metrics: list[Metric] | None = None,))[CTOR,DUNDER]→{optimizer_states_setters::_cast},eval_forward((self,batch: MutableMapping,outputs: SequenceClassifierOutputWithPast | None = None,))] mpt_for_token_classification.py→[forward((self,batch: MutableMapping,))→{optimizer_states_setters::_cast,optimizer_states_setters::_cast,optimizer_states_setters::_cast,numerical_tests_example::loss_fn,optimizer_states_setters::_cast,loss::cross_entropy_loss},__init__((# noqa: PLR0913,PLR0917 self,model: ComposerMPTCausalLM,num_labels: int,hidden_size: int,classifier_dropout: float | None = None,hidden_dropout: float | None = None,train_metrics: list[Metric] | None = None,eval_metrics: list[Metric] | None = None,))[CTOR,DUNDER]→{optimizer_states_setters::_cast}] mpt_mup.py→[sigma_moe_n_active_params((model: ComposerMPTMuPCausalLM))→{utils::dtensor_safe_check_numel,runtime_estimator::init,utils::dtensor_safe_check_numel,utils::dtensor_safe_check_numel,runtime_estimator::init,optimizer_states_setters::_cast},get_optimizer_param_groups((# noqa: PLR0914,C901,PLR0915,PLR0912 self,optimizer_config: dict[str,Any],))→{permute_indices_testing::setUp,permute_indices_testing::setUp,permute_indices_testing::setUp,permute_indices_testing::setUp,permute_indices_testing::setUp,permute_indices_testing::setUp},param_init_fn((self,module: nn.Module))→{math::rope,optimizer_states_setters::_cast,math::rope},convert_sigma_moe_config_to_dense((sigma_moe_block_args: dict[str,Any],))→{runtime_estimator::init,runtime_estimator::init},__init__((self,*args: Any,# noqa: ANN401 n_non_expert_layers: int = 0,mup_config: dict | None = None,use_peri_norm: bool = True,use_embedding_norm: bool = True,**kwargs: Any,# noqa: ANN401))[CTOR,DUNDER]→{comms_ray_batch_variants::load,optimizer_states_setters::_cast},construct_blocks((self,config: MPTConfig))→{convert_sigma_moe_config_to_dense,permute_indices_testing::setUp},__init__((self,config: MPTMuPConfig))[CTOR,DUNDER]→{optimizer_states_setters::_cast,optimizer_states_setters::_cast},__init__((self,config: MPTMuPConfig))[CTOR,DUNDER]→{optimizer_states_setters::_cast},extract_block_args((self,block_args: dict[str,Any])),forward((self,*args: Any,# noqa: ANN401 **kwargs: Any,# noqa: ANN401)),reset_parameters((self))] mup_completep_block.py→[__init__((# noqa: PLR0913 self,depth_multiplier: float = 1.0,depth_alpha_exp: float = 1.0,norm_type: str = "low_precision_layernorm",norm_eps: float = 1e-05,*,use_peri_norm: bool = False,depth_alpha_enabled: bool = False,device: str | None = None,**kwargs: Any,# noqa: ANN401))[CTOR,DUNDER],forward((# noqa: PLR0913,PLR0917 self,x: torch.Tensor,past_key_value: tuple[torch.Tensor,torch.Tensor] | None = None,attn_bias: torch.Tensor | None = None,rotary_emb_w_meta_info: dict | None = None,attention_mask: torch.ByteTensor | None = None,is_causal: bool = True,# noqa: FBT001,FBT002 output_attentions: bool = False,# noqa: FBT001,FBT002 alibi_slopes: torch.Tensor | None = None,flash_attn_padding_info: dict[str,torch.Tensor] | None = None,prev_layer_key_value: tuple[torch.Tensor,torch.Tensor] | None = None,key_value_states: torch.Tensor | None = None,pos_id_within_seq: torch.Tensor | None = None,))] mup_config.py→[] mup_shared_embeddings.py→[__init__((# noqa: PLR0913 self,vocab_size: int,d_model: int,*args: Any,# noqa: ANN401 scale: float = 1.0,use_embedding_norm: bool = False,norm_type: str = "low_precision_layernorm",norm_eps: float = 1e-05,device: str | None = None,**kwargs: Any,# noqa: ANN401))[CTOR,DUNDER],forward((self,input: Tensor,unembed: bool = False))] parallelize.py→[parallelize_llama((model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,))→{apply_ddp,apply_fsdp,apply_compile,apply_tp,activation_checkpoint::apply_ac,tensor_parallel::maybe_enable_async_tp},parallelize_deepseekv3((model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,))→{apply_ddp,apply_fsdp,apply_compile,apply_non_moe_tp,activation_checkpoint::apply_ac,parallelize::apply_moe_ep_tp},apply_tp((model: nn.Module,tp_mesh: DeviceMesh,loss_parallel: bool,enable_float8_tensorwise_tp: bool,)),apply_non_moe_tp((model: nn.Module,tp_mesh: DeviceMesh,loss_parallel: bool,enable_float8_tensorwise_tp: bool,)),apply_compile((model: nn.Module)),apply_fsdp((model: nn.Module,dp_mesh: DeviceMesh,param_dtype: torch.dtype,reduce_dtype: torch.dtype,pp_enabled: bool,cpu_offload: bool = False,reshard_after_forward_policy: str = "default",)),apply_ddp((model: nn.Module,dp_mesh: DeviceMesh,enable_compile: bool,enable_compiled_autograd: bool,))] pipeline.py→[pipeline_llama((model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,device: torch.device,model_args: BaseModelArgs,parallelize_fn: ParallelizeFunction,loss_fn: LossFunction,))→{pipeline_parallel::build_pipeline_schedule,pipeline_parallel::pipeline_module_split,pipeline_parallel::generate_llm_fqn_per_model_part}] quantization.py→[dequantize_from_fp8((weight: torch.Tensor,scale_inv: torch.Tensor,dtype=torch.bfloat16,BLOCK_SIZE: int = BLOCK_SIZE,))→{calculate_scale_shape,integration_tests::main,integration_tests::main},calculate_scale_shape((weight: torch.Tensor,BLOCK_SIZE: int = BLOCK_SIZE))] rope_encoding.py→[_update_cos_sin_cache((self,sequence_length: int,device: torch.device | None = None,dtype: torch.dtype | None = None,))→{optimizer_states_setters::_cast,optimizer_states_setters::_cast,optimizer_states_setters::_cast,runtime_estimator::init},__init__((self,attn_heads_projection_size: int,base: int = 10000,scale_base: int | None = None,*,interleaved: bool = False,use_torch_apply_rotary_emb: bool = False,))[CTOR,DUNDER]→{optimizer_states_setters::_cast,comms_ray_batch_variants::load},apply_rotary_emb_torch((# noqa: PLR0913 x: torch.Tensor,cos: torch.Tensor,sin: torch.Tensor,seqlen_offsets: int | torch.Tensor = 0,cu_seqlens: torch.Tensor | None = None,*,interleaved: bool = False,inplace: bool = False,))→{model::rotate_half,runtime_estimator::init},get((self,x: torch.Tensor,)),forward((self,q: torch.Tensor,k: torch.Tensor,pos_offset: int = 0,))] sigma_moe.py→[forward((self,x: torch.Tensor,))→{forward_no_kernel::forward_sparse,forward_no_kernel::forward_dense_all_experts,forward_cvmm::forward_cvmm_kernel,forward_cvmm_h100_fp8::forward_cvmm_fp8_kernel,forward_gemm::forward_deepgemm,forward_router::forward_router},__init__((self,config: SigmaMoEConfig,))[CTOR,DUNDER]→{cvmm_triton_kernel::is_cvmm_available,deepgemm_moe::is_deepgemm_available,deepgemm_moe::is_deepgemm_available,cvmm_triton_kernel::is_cvmm_available},get_reg_loss((self))→{utils::entropy_reg},extra_repr((self)),_reset_parameters_sp((self)),_reset_parameters_mup((self)),reset_parameters((self))] state_dict_adapter.py→[_get_local_experts_weights((self,abstract_key: str,titan_abstract_key: str,layer_id: str,grouped_expert_weight: torch.Tensor,))→{math::rope},_dequantize((self,state_dict: dict[str,Any]))→{quantization::dequantize_from_fp8},_add_quantization_scale_inv_tensors((self,state_dict: dict[str,Any]))→{quantization::calculate_scale_shape},__init__((self,model_args: TransformerModelArgs,hf_assets_path: str | None,))[CTOR,DUNDER],__init__((self,model_args: DeepSeekV3ModelArgs,hf_assets_path: str | None,))[CTOR,DUNDER],_permute((self,w,n_heads_arg,dim1=None,dim2=None)),_reverse_permute((self,w,n_heads_arg,dim1=None,dim2=None)),to_hf((self,state_dict: dict[str,Any])),_calculate_strided_shard_shard_indices((self,strided_shard_dim_degree: int,strided_shard_dim_rank: int,shard_dim_degree: int,shard_dim_rank: int,dim_size_to_split: int,)),from_hf((self,hf_state_dict: dict[str,Any])),_caculate_indices_from_placements((self,dim: int,dim_size: int,dtensor_placements: tuple,device_mesh: DeviceMesh,)),_concatenate_expert_weights_dtensor((self,expert_weights_by_layer: dict[str,dict[str,dict[int,torch.Tensor]]],abstract_key: str,layer_num: str,device_mesh: DeviceMesh,)),_split_experts_weights((self,weight: torch.Tensor,n_experts: int)),_concatenate_expert_weights((self,expert_weights_by_layer: dict[str,dict[str,dict[int,torch.Tensor]]],abstract_key: str,layer_num: str,n_experts: int,)),to_hf((self,state_dict: dict[str,Any])),from_hf((self,hf_state_dict: dict[str,Any]))] switch_head_core.py→[_forward_simplified((# noqa: PLR0913 self,q_src: torch.Tensor,k_src: torch.Tensor,v_src: torch.Tensor,mask: AttentionMask | None,kv_cache: KVCache = None,*,output_attentions: bool = False,))→{optimizer_states_setters::_cast,optimizer_states_setters::_cast,optimizer_states_setters::_cast},_forward_cvmm_kernel((# noqa: PLR0913,PLR0914 self,q_src: torch.Tensor,k_src: torch.Tensor,v_src: torch.Tensor,mask: AttentionMask | None,kv_cache: KVCache = None,*,output_attentions: bool = False,))→{optimizer_states_setters::_cast,optimizer_states_setters::_cast,optimizer_states_setters::_cast},_forward_no_kernel((# noqa: PLR0913 self,q_src: torch.Tensor,k_src: torch.Tensor,v_src: torch.Tensor,mask: AttentionMask | None,kv_cache: KVCache = None,*,output_attentions: bool = False,))→{optimizer_states_setters::_cast,optimizer_states_setters::_cast,optimizer_states_setters::_cast},_masked_projection((# noqa: PLR0914 self,x: torch.Tensor,scoring_tensor: torch.Tensor,scoring_function: torch.nn.Parameter,linear_projector: torch.nn.Parameter,*,is_v: bool = True,))→{runtime_estimator::init,comms_ray_batch_variants::load},get_sel((self,t: torch.Tensor,w: torch.Tensor,))→{comms_ray_batch_variants::load},attend((self,v: torch.Tensor,k: torch.Tensor,q: torch.Tensor,mask: torch.Tensor | None,*,output_attentions: bool = False,))→{optimizer_states_setters::_cast},get_reg_loss((self))→{utils::entropy_reg},__init__((self,config: SwitchHeadConfig,rope_module: RotaryPositionalEncoding | None = None,))[CTOR,DUNDER],extra_repr((self)),_reset_parameters_sp((self)),_reset_parameters_mup((self)),reset_parameters((self)),_reset_experts_assignments_counters((self)),get_experts_scores_statistics((self,experts_scores: torch.Tensor,*,is_v: bool = True,)),_get_attention_scores((# noqa: PLR0913 self,q: torch.Tensor,k: torch.Tensor,v: torch.Tensor,pos_offset: int,mask: AttentionMask | None,*,output_attentions: bool = False,)),forward((self,x: torch.Tensor,mask: AttentionMask | None,kv_cache: KVCache = None,*,output_attentions: bool = False,))] utils.py→[entropy_reg((sel: torch.Tensor,dim: int))→{entropy_l,log_mean},compute_loss_from_logits((outputs: CausalLMOutputWithPast,labels: torch.Tensor,loss_fn: nn.Module,*,shift_labels: bool,))[HOT]→{get_targets,numerical_tests_example::loss_fn},log_num_params((model: ComposerModel,logged_cfg: dict[str,Any]))→{metrics::log},log_mean((x: torch.Tensor,dim: int = 0)),dtensor_safe_check_numel((tensor: torch.Tensor | DTensor)),entropy_l((input_tensor: torch.Tensor)),get_targets((labels: torch.Tensor))] 
### TESTS
NODES:280 CALL_DEPTH:5

__init__.py→[] combine.py→[test_token_combine(void)[TEST]] debug.py→[test_small(void)[TEST]→{debug::verify_results,debug::pytorch_reference,cg_forward::cg_grouped_gemm_forward,debug::create_aligned_test_data},test_medium(void)[TEST]→{debug::verify_results,debug::pytorch_reference,cg_forward::cg_grouped_gemm_forward,debug::create_aligned_test_data},test_large(void)[TEST]→{debug::verify_results,debug::pytorch_reference,cg_forward::cg_grouped_gemm_forward,debug::create_aligned_test_data}] dispatch.py→[test_token_dispatch(void)[TEST]] dsgemm_unit_testing.py→[test_m_grouped_gemm_contiguous_with_empty_groups(void)[TEST]→{dsgemm_unit_testing::compute_reference_with_scaling,dsgemm_utils::get_col_major_tma_aligned_tensor,dsgemm_unit_testing::per_block_cast_to_fp8,dsgemm_unit_testing::per_token_cast_to_fp8,dsgemm_unit_testing::create_m_indices_fast,runtime_estimator::init},test_m_grouped_gemm_contiguous_all_empty_but_one(void)[TEST]→{dsgemm_unit_testing::compute_reference_with_scaling,dsgemm_utils::get_col_major_tma_aligned_tensor,dsgemm_unit_testing::per_block_cast_to_fp8,dsgemm_unit_testing::per_token_cast_to_fp8,dsgemm_unit_testing::create_m_indices_fast,runtime_estimator::init},test_m_grouped_gemm_contiguous_with_scaling_edge_cases(void)[TEST]→{dsgemm_unit_testing::compute_reference_with_scaling,dsgemm_utils::get_col_major_tma_aligned_tensor,dsgemm_unit_testing::per_block_cast_to_fp8,dsgemm_unit_testing::per_token_cast_to_fp8,dsgemm_unit_testing::create_m_indices_fast,runtime_estimator::init}] extend_jobconfig_example.py→[] fast_debug_ao.py→[test_multiple_deepseek_configs(void)[TEST]→{reference_utils::analyze_tensor_differences,reference_utils::analyze_tensor_differences,reference_utils::compute_reference_backward,mg_grouped_gemm::grouped_gemm_backward,reference_utils::analyze_tensor_differences,reference_utils::compute_reference_forward},test_backward_pass(void)[TEST]→{reference_utils::analyze_tensor_differences,reference_utils::analyze_tensor_differences,reference_utils::compute_reference_backward,mg_grouped_gemm::grouped_gemm_backward,mg_grouped_gemm::grouped_gemm_forward},test_forward_pass(void)[TEST]→{reference_utils::analyze_tensor_differences,reference_utils::compute_reference_forward,mg_grouped_gemm::grouped_gemm_forward}] features.py→[build_features_test_list(void)[HOT]] ft.py→[main(void)[ENTRY]→{build_ft_test_list,unit_test_cg::run_tests},run_single_test((test_flavor: OverrideDefinitions,full_path: str,output_dir: str))→{integration_tests::main},run_tests((args,test_list: list[OverrideDefinitions]))→{integration_tests::run_single_test},build_ft_test_list(void)[HOT],_run_cmd((cmd))] h100.py→[build_h100_tests_list(void)[HOT]] indices.py→[test_with_zero_tokens(void)[TEST]→{indices::generate_permute_indices,indices::generate_permute_indices}] integration_tests.py→[main(void)[ENTRY]→{unit_test_cg::run_tests},run_single_test((test_flavor: OverrideDefinitions,full_path: str,output_dir: str))→{main,run_tests::_run_cmd},run_tests((args,test_list: list[OverrideDefinitions]))→{run_single_test},main(void)→{unit_test_cg::run_tests},build_simple_fsdp_test_list(void)[HOT],build_flux_test_list(void)[HOT]] models.py→[build_model_tests_list(void)[HOT]] pack_test_dataset.py→[pack_wds_dataset((tar_destination,source_folder,number_of_samples))→{math::rope,math::rope,math::rope,math::rope}] permute_indices_testing.py→[test_fixed_total_experts_varying_ranks((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_fixed_total_experts_varying_ranks((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_different_block_sizes_with_fixed_experts((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_different_block_sizes_with_fixed_experts((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_edge_cases_with_fixed_experts((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_edge_cases_with_fixed_experts((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_max_blocks_with_large_experts((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_max_blocks_with_large_experts((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_extreme_max_blocks_limit((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_extreme_max_blocks_limit((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu}] run_tests.py→[main(void)[ENTRY]→{unit_test_cg::run_tests},run_single_test((test_flavor: OverrideDefinitions,full_path: str,output_dir: str))→{_run_cmd,integration_tests::main},run_tests((args,test_list: list[OverrideDefinitions]))→{integration_tests::run_single_test},_run_cmd((cmd))] test_activation_checkpoint.py→[test_correctness((self))[TEST]→{activation_checkpoint::apply_ac,activation_checkpoint::apply_ac,activation_checkpoint::apply_ac},__init__((self))[CTOR,DUNDER],forward((self,x)),__init__((self))[CTOR,DUNDER],forward((self,x)),test_flops((self))[TEST],test_mem((self))[TEST]] test_checkpoint.py→[__init__((self))[CTOR,DUNDER],state_dict((self)),load_state_dict((self,sd: dict)),init_cache_state_dict((self)),__init__((self))[CTOR,DUNDER],state_dict((self)),load_state_dict((self,sd: dict)),__init__((self))[CTOR,DUNDER],state_dict((self)),load_state_dict((self,sd: dict)),__init__((self))[CTOR,DUNDER],__init__((self))[CTOR,DUNDER],fake_async_save((*args,**kwargs)),__init__((self,job))[CTOR,DUNDER],setUp((self)),tearDown((self)),fake_save((self,state_dict: dict,checkpoint_id: str,storage_writer=None)),fake_load((self,states: dict,checkpoint_id=None)),test_load_returns_false_when_no_checkpoint_folder((self))[TEST]] test_create_m_indices.py→[test_create_indices_from_offsets_nosync(void)[TEST]→{dsgemm_utils::create_indices_from_offsets_nosync,dsgemm_utils::create_indices_from_offsets_nosync}] test_dataset_checkpointing.py→[_build_dataloader((self,dataset_name,batch_size,seq_len,world_size,rank))[HOT]→{hf_datasets::build_hf_dataloader,lr_scheduler::step,lr_scheduler::step},test_c4_resumption((self))[TEST]→{lr_scheduler::step,lr_scheduler::step},setUp((self)),tearDown((self))] test_download_hf_assets.py→[_call_download_hf_assets((self,**kwargs))→{download_hf_assets::download_hf_assets},test_invalid_repo_id_format((self))[TEST]→{lr_scheduler::step},test_unknown_asset_type((self))[TEST]→{lr_scheduler::step},setUp((self)),tearDown((self)),_setup_mocks((self,mock_download,mock_list_files,repo_files=None)),_get_downloaded_files((self,mock_download)),_assert_files_downloaded((self,mock_download,expected_files))] test_flux_dataloader.py→[test_load_dataset((self))[TEST]→{lr_scheduler::step,flux_dataset::build_flux_dataloader,lr_scheduler::step,flux_dataset::build_flux_dataloader,lr_scheduler::step,lr_scheduler::step},setUp((self)),tearDown((self))] test_job_config.py→[test_parse_module_fqns_per_model_part((self))[TEST]→{math::rope,math::rope,math::rope},test_parse_exclude_from_loading((self))[TEST]→{math::rope,math::rope},test_job_config_invalid_field((self))[TEST]→{math::rope},test_command_line_args((self))[TEST],test_job_config_file((self))[TEST],test_job_file_does_not_exist((self))[TEST],test_empty_config_file((self))[TEST],test_job_config_file_cmd_overrides((self))[TEST],test_job_config_model_converters_split((self))[TEST],test_print_help((self))[TEST],test_extend_jobconfig_directly((self))[TEST],test_custom_parser((self))[TEST]] test_lr_scheduler.py→[create_job_config((self,training_steps=10,warmup_steps=None,decay_ratio=None,decay_type=None,min_lr_factor=None,))→{lr_scheduler::step,lr_scheduler::step,lr_scheduler::step,lr_scheduler::step},setUp((self))→{lr_scheduler::step},test_linear_warmup_decay((self))[TEST]→{lr_scheduler::build_lr_schedulers},test_warmup_stable_decay((self))[TEST]→{lr_scheduler::build_lr_schedulers},test_min_lr((self))[TEST]→{lr_scheduler::build_lr_schedulers},test_warmup_exceeds_training((self))[TEST]→{lr_scheduler::build_lr_schedulers},test_warmup_stable_only((self))[TEST]→{lr_scheduler::build_lr_schedulers},test_warmup_plus_decay_exceeds_training((self))[TEST]→{lr_scheduler::build_lr_schedulers}] test_model_converter.py→[test_build_model_converters_empty_list(void)[HOT,TEST]→{build_parallel_dims,model_converter::build_model_converters},test_build_model_converters_float8_converter(void)[HOT,TEST]→{build_parallel_dims,model_converter::build_model_converters},build_parallel_dims((job_config,world_size))[HOT]] test_multimodal_model.py→[test_llama_mm_vision_encoder((self))[TEST]] test_numerics.py→[run_simple_fsdp((self,model,inputs,labels,epoch=20))→{simple_fsdp::data_parallel},init_test((self)),get_input((self)),run_fsdp2((self,model,inputs,labels,epoch=20)),test_replicate_convergence((self))[TEST],test_fullyshard_convergence((self))[TEST],test_hybridshard_convergence((self))[TEST]] test_tokenizer.py→[setUp((self)),tearDown((self)),_compare_tokenizers((self,our_tokenizer,reference_tokenizer,test_repo_id))] test_train_spec.py→[test_register_train_spec((self))[TEST]→{__init__::get_train_spec,__init__::get_train_spec,train_spec::register_train_spec},test_optim_hook((self))[TEST]→{__init__::get_train_spec,train_spec::register_train_spec},fake_build_optimizers_with_hook((model_parts: list[nn.Module],optimizer_config: OptimizerConfig,parallel_dims: ParallelDims,ft_manager: FTManager,optimizer_hook,))[HOT]→{fake_build_optimizers},__init__((self,model_args: BaseModelArgs))[CTOR,DUNDER],forward((self,x: torch.Tensor)),init_weights((self,buffer_device: torch.device | None = None)),fake_build_optimizers((model_parts: list[nn.Module],optimizer_config: OptimizerConfig,parallel_dims: ParallelDims,ft_manager: FTManager,))[HOT]] test_utils.py→[fixed_init_tensor((shape: torch.Size,min_val: Union[float,int] = 0.0,max_val: Union[float,int] = 1.0,nonlinear: bool = False,dtype: torch.dtype = torch.float,))] unit_test_backwards.py→[test_mg_dx((self))[TEST]→{reference_utils::analyze_tensor_differences,reference_utils::compute_reference_backward,mg_grouped_gemm::grouped_gemm_backward,mg_grouped_gemm::grouped_gemm_forward},test_mg_dw((self))[TEST]→{reference_utils::analyze_tensor_differences,reference_utils::compute_reference_backward,mg_grouped_gemm::grouped_gemm_backward,mg_grouped_gemm::grouped_gemm_forward},test_mg_grouped_gemm_backward_bf16((self))[TEST],test_mg_grouped_gemm_backward_deepseek_shapes((self))[TEST]] unit_test_cg.py→[test_forward_deepseek_shapes((self))[TEST],test_backward_deepseek_shapes((self))[TEST],test_forward_performance_deepseek((self))[TEST],test_backward_performance_deepseek((self))[TEST]] unit_test_forwards.py→[test_mg_grouped_gemm_bf16((self))[TEST],test_mg_grouped_gemm_deepseek_shapes((self))[TEST]] 
### UI_COMPONENTS
NODES:299 CALL_DEPTH:3

__init__.py→[] checkpoint.py→[_find_load_step((self,folder: str = ""))→{integration_tests::main,runtime_estimator::init,runtime_estimator::init},_get_state_dict((self))→{integration_tests::main},load_state_dict((self,state_dict: dict[str,Any]))→{integration_tests::main},_purge_stale_checkpoints((self))→{runtime_estimator::init},__init__((self,model: nn.Module | list[nn.Module]))[CTOR,DUNDER],state_dict((self)),purge_thread((purge_queue: queue.Queue)),__init__((self,dataloader: BaseDataLoader | None,model_parts: list[nn.Module],optimizers: OptimizersContainer,lr_schedulers: LRSchedulersContainer,states: dict[str,Any],checkpoint_config: CheckpointConfig,sd_adapter: BaseStateDictAdapter | None,base_folder: str = "",ft_manager: FTManager | None = None,))[CTOR,DUNDER],__del__((self))[DUNDER],close((self)),dcp_load((self,state_dict: dict[str,Any],checkpoint_id: str,from_hf: bool,)),maybe_wait_for_staging((self)),_ft_folder((self)),_create_checkpoint_id((self,step: int,folder: str = "")),_ft_save((self,step: int)),_ft_load((self)),_flattened_model_states_sd((self,state_dict: dict[str,Any] | None = None)),_states_to_load((self,model_only: bool)),_save_last_step((self,curr_step: int)),_should_save((self,curr_step: int,last_step: bool = False)),_async_wait((self))] dataloader.py→[__init__((self,dataset: IterableDataset,dp_rank: int,dp_world_size: int,batch_size: int,collate_fn: Callable | None = None,))[CTOR,DUNDER],state_dict((self)),load_state_dict((self,state_dict: dict[str,Any]))] float8.py→[__init__((self,job_config: JobConfig,parallel_dims: ParallelDims))[CTOR,DUNDER]→{expert_parallel::set_token_group_alignment_size_m,utils::has_cuda_capability},__init__((self,job_config: JobConfig,parallel_dims: ParallelDims))[CTOR,DUNDER]→{utils::has_cuda_capability},_init_filter_fn((self,float8_config: Float8Dense)),convert((self,model: nn.Module)),post_optimizer_hook((self,model: nn.Module | list[nn.Module])),convert((self,model: nn.Module)),post_optimizer_hook((self,model: nn.Module | list[nn.Module]))] job_config.py→[] loss.py→[rescale_accumulated_loss((unwrapped_loss_fn,accumulation_steps))→{rescale_accumulated_loss},cross_entropy_loss((pred: torch.Tensor,labels: torch.Tensor)),build_cross_entropy_loss((job_config: JobConfig))[HOT],__init__((self,unwrapped_loss_fn,accumulation_steps))[CTOR,DUNDER],__call__((self,*args,**kwargs))[DUNDER]] lr_scheduler.py→[__iter__((self))[DUNDER]→{step},build_lr_schedulers((optimizers: OptimizersContainer,lr_scheduler_config: LRSchedulerConfig,training_steps: int,))[HOT]→{runtime_estimator::init},__init__((self,optimizers: OptimizersContainer,lr_lambda: Callable))[CTOR,DUNDER],__len__((self))[DUNDER],step((self)),state_dict((self)),load_state_dict((self,state_dict: dict[str,Any]))] manager.py→[__init__((self,ft_config: FTConfig,))[CTOR,DUNDER],get_dp_info((self,dp_degree: int,dp_rank: int)),maybe_set_all_reduce_hook((self,model_parts: list[torch.nn.Module])),maybe_semi_sync_training((ft_config: FTConfig,ft_manager: FTManager,model: torch.nn.Module,n_layers: int,optimizer: torch.optim.Optimizer,fragment_fn: Optional[Callable[...,list[nn.Module]]] = None,))] metrics.py→[_build_metric_logger((job_config: JobConfig,parallel_dims: ParallelDims,tag: str | None = None))[HOT]→{_get_metrics_rank,lr_scheduler::step},__init__((self,job_config: JobConfig,parallel_dims: ParallelDims,tag: str | None = None,))[CTOR,DUNDER]→{build_device_memory_monitor,_build_metric_logger},ensure_pp_loss_visible((parallel_dims: ParallelDims,job_config: JobConfig,color: Color))→{lr_scheduler::step},__init__((self,device: str = f"{device_type}:0"))[CTOR,DUNDER],_to_gib((self,memory_in_bytes)),_to_pct((self,memory)),get_peak_stats((self)),reset_peak_stats((self)),build_device_memory_monitor(void)[HOT],log((self,metrics: dict[str,Any],step: int)),close((self)),__init__((self,log_dir: str,tag: str | None = None))[CTOR,DUNDER],log((self,metrics: dict[str,Any],step: int)),close((self)),__init__((self,log_dir: str,job_config: JobConfig,tag: str | None = None))[CTOR,DUNDER],log((self,metrics: dict[str,Any],step: int)),close((self)),__init__((self))[CTOR,DUNDER],add_logger((self,logger_instance: BaseLogger)),log((self,metrics: dict[str,Any],step: int)),close((self)),_get_metrics_rank((parallel_dims: ParallelDims,job_config: JobConfig,)),should_log((self,step: int)),log((self,step: int,global_avg_loss: float,global_max_loss: float,grad_norm: float,extra_metrics: dict[str,Any] | None = None,)),log_validation((self,loss: float,step: int)),close((self)),build_metrics_processor((job_config: JobConfig,parallel_dims: ParallelDims,model_args: "BaseModelArgs | None" = None,tag: str | None = None,))[HOT]] mx.py→[__init__((self,job_config: JobConfig,parallel_dims: ParallelDims))[CTOR,DUNDER]→{expert_parallel::set_token_group_alignment_size_m,utils::has_cuda_capability},__init__((self,job_config: JobConfig,parallel_dims: ParallelDims))[CTOR,DUNDER]→{utils::has_cuda_capability},convert((self,model: nn.Module)),post_optimizer_hook((self,model: nn.Module | list[nn.Module])),convert((self,model: nn.Module)),post_optimizer_hook((self,model: nn.Module | list[nn.Module]))] optimizer.py→[__iter__((self))[DUNDER]→{lr_scheduler::step},state_dict((self))→{integration_tests::main},load_state_dict((self,state_dict: dict[str,Any]))→{integration_tests::main},__init__((self,model_parts: list[nn.Module],optimizer_cls: type[T],optimizer_kwargs: dict[str,Any],ft_manager: "ft.Manager",use_ft_optimizer: bool = True,))[CTOR,DUNDER]→{integration_tests::main},build_optimizers_with_moe_load_balancing((model_parts: list[nn.Module],optimizer_config: OptimizerConfig,parallel_dims: ParallelDims,ft_manager: FTManager | None = None,))[HOT]→{build_optimizers},__init__((self,model_parts: list[nn.Module],optimizer_cls: type[T],optimizer_kwargs: dict[str,Any],))[CTOR,DUNDER],__len__((self))[DUNDER],step((self,*args,**kwargs)),zero_grad((self,*args,**kwargs)),_validate_length((self,expected_length: int)),_post_init((self,all_params: list[nn.Parameter],optimizer_kwargs: dict[str,Any])),__init__((self,model_parts: list[nn.Module],optimizer_cls: type[T],optimizer_kwargs: dict[str,Any],))[CTOR,DUNDER],step((self)),zero_grad((self)),init_cache_state_dict((self)),state_dict((self)),load_state_dict((self,state_dict: dict[str,Any])),step((self,*args,**kwargs)),zero_grad((self,*args,**kwargs)),build_optimizers((model_parts: list[nn.Module],optimizer_config: OptimizerConfig,parallel_dims: ParallelDims,ft_manager: FTManager | None = None,))[HOT]] protocol.py→[] tokenizer.py→[_load_tokenizer_from_path((self,tokenizer_path: str))→{math::rope,math::rope,math::rope,math::rope},_load_config((self,config_path: str))→{math::rope},_infer_special_tokens((self))→{runtime_estimator::init},__init__((self))[CTOR,DUNDER],__init__((self,tokenizer_path: str,))[CTOR,DUNDER],_get_token_from_config((self,config: dict[str,Any],key: str)),_process_special_token((self,token_str: str,token_config: dict,token_id: Optional[int] = None)),_infer_should_add_bos_eos((self)),encode((self,*args,**kwargs)),get_vocab_size((self)),get_vocab((self)),token_to_id((self,token: str)),id_to_token((self,token_id: int)),build_hf_tokenizer((job_config: JobConfig,))[HOT]] utils.py→[fragment_llm((model: nn.Module,ft_config: FTConfig,n_layers: int,))→{module_split,pipeline_parallel::generate_llm_fqn_per_model_part},module_filter_fn((mod: nn.Module,fqn: str,filter_fqns: list[str])),module_split((model: nn.Module,module_fqns_per_model_fragment: list[list[str]],))] validate.py→[__init__((self,job_config: JobConfig,dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer,parallel_dims: ParallelDims,loss_fn: LossFunction,validation_context: Generator[None,None,None],maybe_enable_amp: Generator[None,None,None],metrics_processor: MetricsProcessor,pp_schedule: _PipelineSchedule | None = None,pp_has_first_stage: bool | None = None,pp_has_last_stage: bool | None = None,))[CTOR,DUNDER]→{hf_datasets::build_hf_validation_dataloader},build_validator((job_config: JobConfig,dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer,parallel_dims: ParallelDims,loss_fn: LossFunction,validation_context: Generator[None,None,None],maybe_enable_amp: Generator[None,None,None],metrics_processor: MetricsProcessor | None = None,pp_schedule: _PipelineSchedule | None = None,pp_has_first_stage: bool | None = None,pp_has_last_stage: bool | None = None,))[HOT]→{validate},__init__((self,job_config: JobConfig))[CTOR,DUNDER],validate((self,model_parts: list[nn.Module])),should_validate((self,step: int))] 
### UTILITY_LAYER
NODES:3408 CALL_DEPTH:10

__init__.py→[get_train_spec(void),__init__((self,*,input_layout: Placement | None = None,output_layout: Placement | None = None,use_local_output: bool = True,))[CTOR,DUNDER],get_train_spec(void),_apply((self,module: nn.Module,device_mesh: DeviceMesh)),get_train_spec(void),get_train_spec(void),get_train_spec(void)] _generation.py→[logits_to_probs((logits: torch.Tensor,temperature: float = 1.0,top_k: Optional[int] = None,))→{comms_ray_batch_variants::load,integration_tests::main,integration_tests::main},generate_next_token((model,x: torch.Tensor,*,temperature: float = 1.0,top_k: Optional[int] = None,rng: Optional[torch.Generator] = None,))→{multinomial_sample_one,logits_to_probs},multinomial_sample_one((probs: torch.Tensor,rng: Optional[torch.Generator] = None))] activation_checkpoint.py→[_apply_ac_to_transformer_block((module: nn.Module,ac_config: ACConfig,*,base_fqn: str | None = None,model_compile_enabled: bool = False,use_flex_attn: bool = False,op_sac_save_list: set[torch._ops.OpOverload] | None = None,))→{_apply_layer_sac,_apply_op_sac,_apply_op_sac_to_transformer_block_with_flex,_apply_full_ac,permute_indices_testing::setUp},_apply_layer_sac((module: nn.Module,ac_config: ACConfig))→{runtime_estimator::init},_apply_op_sac((module: nn.Module,ac_config: ACConfig,*,base_fqn: str | None = None,op_sac_save_list: set[torch._ops.OpOverload],))→{permute_indices_testing::setUp},_apply_op_sac_to_transformer_block_with_flex((module: nn.Module,ac_config: ACConfig,*,base_fqn: str | None = None,model_compile_enabled: bool = False,op_sac_save_list: set[torch._ops.OpOverload],))→{logging::warn_once},apply_ac((model: nn.Module,ac_config: ACConfig,*,model_compile_enabled: bool = False,use_flex_attn: bool = False,op_sac_save_list: set[torch._ops.OpOverload] | None = None,))→{_apply_ac_to_transformer_block},_apply_full_ac((module: nn.Module,ac_config: ACConfig))] adopt.py→[_multi_tensor_adopt((# noqa: C901,PLR0917,PLR0913,PLR0915,PLR0912 params: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | None,has_complex: bool,beta1: float,beta2: float,lr: float | Tensor,clip_lambda: Callable[[Number | Tensor | Any],float] | None,weight_decay: float,decouple: bool,eps: float,maximize: bool,capturable: bool,differentiable: bool,))→{optimizer_states_setters::_cast,optimizer_states_setters::_cast,optimizer_states_setters::_cast,optimizer_states_setters::_cast,optimizer_states_setters::_cast},_default_clip_lambda((step: Number | Tensor))→{math::rope,runtime_estimator::init,runtime_estimator::init},_single_tensor_adopt((# noqa: PLR0917,PLR0913 params: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | None,decouple: bool,clip_lambda: Callable[[Number | Tensor | Any],float] | None,beta1: float,beta2: float,lr: float | Tensor,weight_decay: float,eps: float,maximize: bool,capturable: bool,differentiable: bool,has_complex: bool,# noqa: ARG001)),_fused_adopt((# noqa: PLR0917,PLR0913 params: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | None,has_complex: bool,# Needed for consistency. beta1: float,beta2: float,lr: float | Tensor,clip_lambda: Callable[[int],float] | None,weight_decay: float,decouple: bool,eps: float,maximize: bool,capturable: bool,# Needed for consistency. differentiable: bool,))] aggregation.py→[_aggregate_helper((prev_tuple: tuple[np.ndarray,int],param: np.ndarray,num_examples: int,))→{comms_ray_batch_variants::load,comms_ray_batch_variants::load},submit_aggregation_tasks_to_executor((executor: ThreadPoolExecutor,accumulator: dict[tuple[str,ModelStateNames],tuple[np.ndarray,int]],aggregation_queue: Queue[tuple[np.ndarray,int,str,ModelStateNames]],futures: dict[tuple[str,ModelStateNames],Future[tuple[np.ndarray,int]]],))→{_remove_futures_done,_remove_futures_done},wait_for_all_futures((accumulator: dict[tuple[str,ModelStateNames],tuple[np.ndarray,int]],futures: dict[tuple[str,ModelStateNames],Future[tuple[np.ndarray,int]]],))→{_remove_futures_done},_remove_futures_done((accumulator: dict[tuple[str,ModelStateNames],tuple[np.ndarray,int]],futures: dict[tuple[str,ModelStateNames],Future[tuple[np.ndarray,int]]],)),weighted_average((metrics: list[tuple[int,dict]],))] args.py→[] attn_mask_utils.py→[_prepare_4d_causal_attention_mask((attention_mask: Optional[torch.Tensor],input_shape: Union[torch.Size,Tuple,List],inputs_embeds: torch.Tensor,past_key_values_length: int,sliding_window: Optional[int] = None,))] autoencoder.py→[forward((self,x))→{swish,swish},__init__((self,params: AutoEncoderParams))[CTOR,DUNDER]→{tiktoken::decode,tokenizer::encode},forward((self,x: Tensor))→{swish},forward((self,z: Tensor))→{swish},load_ae((ckpt_path: str,autoencoder_params: AutoEncoderParams,device: str | torch.device = "cuda",dtype=torch.bfloat16,random_init=False,))→{lr_scheduler::step},swish((x: Tensor)),__init__((self,in_channels: int))[CTOR,DUNDER],attention((self,h_: Tensor)),forward((self,x: Tensor)),__init__((self,in_channels: int,out_channels: int))[CTOR,DUNDER],__init__((self,in_channels: int))[CTOR,DUNDER],forward((self,x: Tensor)),__init__((self,in_channels: int))[CTOR,DUNDER],forward((self,x: Tensor)),__init__((self,resolution: int,in_channels: int,ch: int,ch_mult: list[int],num_res_blocks: int,z_channels: int,))[CTOR,DUNDER],__init__((self,ch: int,out_ch: int,ch_mult: list[int],num_res_blocks: int,in_channels: int,resolution: int,z_channels: int,))[CTOR,DUNDER],__init__((self,sample: bool = True,chunk_dim: int = 1))[CTOR,DUNDER],forward((self,z: Tensor)),encode((self,x: Tensor)),decode((self,z: Tensor)),forward((self,x: Tensor))] base_schema.py→[register_config((name: "str"))] benchmark.py→[benchmark_model_configs(void)→{mg_grouped_gemm::grouped_gemm_forward,mg_grouped_gemm::grouped_gemm_forward,reference_utils::compute_reference_forward,reference_utils::compute_reference_forward},compute_reference_forward((x,w,m_sizes))[HOT],plot_benchmark_results((results)),compare_mg_implementations(void)] benchmark_kernels.py→[print_results_table((results))→{lr_scheduler::step},benchmark_quant_kernels((shapes,dtype=torch.bfloat16,warmup=10,iters=100))] broadcast.py→[build_broadcast_recorddict((transmission_mask: tuple[tuple[bool,...],list[str],list[str]],batch_id: int = 0,))[HOT]→{math::rope,lr_scheduler::step,runtime_estimator::init}] broadcast_utils.py→[compose_broadcast_messages((msg_str: str,server_load_balancer: ServerLoadBalancer,server_state: ServerState,comm_stack: CommStack,transmission_mask: tuple[tuple[bool,...],list[str],list[str]],))→{lr_scheduler::step,comms::offload_recorddict_parameters_to_remote,lr_scheduler::step,broadcast::build_broadcast_recorddict,lr_scheduler::step,utils::compress_with_strict},broadcast_parameters_to_nodes((server_load_balancer: ServerLoadBalancer,server_state: ServerState,comm_stack: CommStack,))→{compose_broadcast_messages,permute_indices_testing::setUp,metrics::log,s3_checkpoint_loading::delete_past_communication_states,metrics::log,generic::build_success_recorddict}] build_optimizer.py→[build_optimizer((model: torch.nn.Module,name: str,optimizer_config: dict[str,Any],))[HOT]] centralised_train.py→[main(void)[ENTRY]→{initialize_checkpoint_state,initialize_checkpoint_state,metrics::log,metrics::log,sharding::dump_model_parameters_to_file,composer_trainer_getters::get_parameters_from_trainer},initialize_checkpoint_state((trainer: Trainer,checkpoint_path: str,))→{utils::create_tensors_dict,composer_trainer_setters::set_trainer_tensors_dict,s3_helper::list_objects,s3_checkpoint_loading::get_checkpoint_files_for_path,helper::retrieve_model_states_and_types,utils::get_n_experts_from_model}] cg_backward.py→[cg_grouped_gemm_backward_weights((grad_output: torch.Tensor,# [M_total,N] inputs: torch.Tensor,# [M_total,K] expert_indices: torch.Tensor,# [M_total] num_experts: int,group_size_m: int = 128,))→{integration_tests::main,integration_tests::main,integration_tests::main},verify_cg_gemm_backward((M_total=1024,N=512,K=512,num_experts=8,group_size_m=128,device="cuda",atol=1e-1,# Absolute tolerance for validation rtol=1e-1,# Relative tolerance for validation))→{cg_grouped_gemm},cg_grouped_gemm_backward_inputs((grad_output: torch.Tensor,# [M_total,N] expert_weights: torch.Tensor,# [num_experts,N,K] expert_indices: torch.Tensor,# [M_total] group_size_m: int = 128,)),cg_grouped_gemm((inputs: torch.Tensor,expert_weights: torch.Tensor,expert_indices: torch.Tensor,group_size_m: int = 128,)),benchmark_cg_gemm_backward((M_total=1024,N=512,K=512,num_experts=8,group_size_m=128,device="cuda",num_runs=10,))] cg_forward.py→[cg_grouped_gemm_forward((inputs: torch.Tensor,# [M_total,K] expert_weights: torch.Tensor,# [num_experts,N,K] expert_indices: torch.Tensor,# [M_total] group_size_m: int = 128,)),cg_grouped_gemm_forward_dynamic((inputs: torch.Tensor,# [M_total,K] expert_weights: torch.Tensor,# [num_experts,N,K] expert_indices: torch.Tensor,# [M_total] group_size_m: int = 128,)),cg_grouped_gemm((inputs: torch.Tensor,expert_weights: torch.Tensor,expert_indices: torch.Tensor,# use_tma: bool = True,group_size_m: int = 128,))] cg_reference.py→[pytorch_reference((inputs: torch.Tensor,expert_weights: torch.Tensor,expert_indices: torch.Tensor,group_size_m: int = 128,))→{integration_tests::main}] check_padding_mm.py→[] checkpoint.py→[load_safetensor_weights((model: torch.nn.Module,weight_map: Dict[str,str],file_location: str,device: torch.device,))→{load_safetensor_file,get_needed_files,permute_indices_testing::setUp,permute_indices_testing::setUp,permute_indices_testing::setUp,lr_scheduler::step},read_weights_from_json((file_path: str))→{lr_scheduler::step,math::rope},load_weights_from_hf((model: torch.nn.Module,distribution: str,device: torch.device,))→{load_safetensor_weights,get_hf_weight_map_and_path},get_hf_weight_map_and_path((model_id: str,))→{read_weights_from_json},get_needed_files((state_dict: Dict[str,torch.Tensor],weight_map: Dict[str,str]))→{permute_indices_testing::setUp},load_safetensor_file((full_path: str,device: torch.device))] checkpoint_tool.py→[main((argv: list[str] | None = None))[ENTRY]→{_cmd_inspect_round,_cmd_list_rounds,server_state_tool::build_arg_parser},inspect_round((run_root: str,rnd: int))→{_join,_count_safetensor_shards,_join,_count_safetensor_shards,_join,_count_safetensor_shards},read_state_summary((run_root: str,rnd: int))→{_exists,_join,pickle_compat::custom_pickle_load,lr_scheduler::step},_cmd_list_rounds((args: argparse.Namespace))→{list_rounds,metrics::log,metrics::log},_cmd_inspect_round((args: argparse.Namespace))→{read_state_summary,inspect_round,metrics::log},_join((run_root: str,*parts: str))→{lr_scheduler::step},_count_safetensor_shards((url_dir: str))→{lr_scheduler::step},list_rounds((run_root: str))→{s3_helper::obtain_sorted_runs},_exists((url: str)),build_arg_parser(void)[HOT]] checkpoint_upgrade.py→[main((argv: list[str] | None = None))[ENTRY]→{upgrade_all,upgrade_round,server_state_tool::build_arg_parser,metrics::log,metrics::log},_attempt_wandb_alignment((server_round_dir: str,names: list[str],arrays: list[np.ndarray],wandb_base: str,))→{_align_to_expected,_expected_params_from_cfg,_fetch_wandb_config,_basename,lr_scheduler::step,metrics::log},_write_safetensors((params_dir: str,mapping: dict[str,np.ndarray],cfg: DictConfig | None,))→{_join_to_path,_write_bytes,lr_scheduler::step,metrics::log,metrics::log,model_parameters_setters::set_model_tensors_dict},_maybe_load_legacy_params((server_round_dir: str))→{_remote_paths_exists,_remote_paths_exists,_join_to_path,_remote_paths_exists,_join_to_path,_join_to_path},_ensure_params_safetensors((server_round_dir: str,*,wandb_base: str | None = None,dry_run: bool = False,))→{_write_safetensors,_default_mapping,_attempt_wandb_alignment,_maybe_load_legacy_params,_remote_paths_exists,_join_to_path},_ensure_state((server_round_dir: str,round_num: int,*,dry_run: bool = False,))→{_write_bytes,_remote_paths_exists,_join_to_path,metrics::log,lr_scheduler::step,metrics::log},upgrade_round((run_root: str,round_num: int,*,wandb_base: str | None = None,dry_run: bool = False,))→{_ensure_state,_ensure_params_safetensors,_join_to_path,lr_scheduler::step},upgrade_all((run_root: str,*,wandb_base: str | None = None,dry_run: bool = False,))→{upgrade_round,metrics::log,metrics::log,s3_helper::obtain_sorted_runs},_fetch_wandb_config((wandb_base: str,run_uuid: str))→{_parse_wandb_base,lr_scheduler::step,metrics::log},_align_to_expected((legacy_names: list[str],legacy_arrays: list[np.ndarray],expected_names: list[str],expected_shapes: list[tuple[int,...]],))→{_reshape_like,_reshape_like},_join_to_path((*parts: str))→{lr_scheduler::step},_expected_params_from_cfg((cfg: DictConfig,))→{utils::get_random_model_states_from_config},_reshape_like((arr: np.ndarray,target_shape: Sequence[int]))→{runtime_estimator::init},_remote_paths_exists((url: str)),_write_bytes((url: str,data: bytes)),_basename((path: str)),_parse_wandb_base((wandb_base: str)),_default_mapping((names: list[str],arrays: list[np.ndarray],)),build_arg_parser(void)[HOT]] client_app.py→[] client_sampler.py→[sample_clients((self,rng: random.Random,))→{runtime_estimator::init},__init__((self,function_name: str,))[CTOR,DUNDER],__init__((self,total_number_of_clients: int,number_of_clients_per_round: int,dropout_ratio: float,dropout_function_name: str,))[CTOR,DUNDER]] combine.py→[__init__((self,group_name: str,align: int,in_len,out_len,token_shape,num_ranks,num_local_experts,dtype,device: torch.device,))[CTOR,DUNDER],forward((self,inp: torch.Tensor,out: torch.Tensor,in_splits_offsets: torch.Tensor,out_splits_offsets: torch.Tensor,))] comms.py→[offload_to_shm((parameters: NDArrays,recorddict: RecordDict,msg_str: str,))→{lr_scheduler::step,lr_scheduler::step,utils::set_parameters_in_place,utils::get_parameters_shm,metrics::log,lr_scheduler::step},load_from_shm_parameters((recorddict: RecordDict,msg_str: str,params_garbage_queue: Queue[ray.ObjectRef | str] | None = None,))→{utils::get_parameters_shm,metrics::log,lr_scheduler::step,metrics::log,utils::get_parameters_shm,lr_scheduler::step},offload_to_s3((parameters: NDArrays,remote_uploader_downloader: RemoteUploaderDownloader,recorddict: RecordDict,msg_str: str,))→{metrics::log,lr_scheduler::step,io::upload_file_to_s3,sharding::dump_model_parameters_to_file,s3_helper::extract_s3_comm_config_from_configrecord},offload_recorddict_parameters_to_remote((parameters: NDArrays,remote_uploader_downloader: RemoteUploaderDownloader | None,outgoing_recorddict: RecordDict,msg_str: str,comm_stack: CommStack,))→{offload_to_shm,offload_to_s3,comms_ray_batch_variants::offload_to_ray,comms_ray_batch_variants::offload_to_ray,comms_ray_batch_variants::offload_to_ray_legacy},load_from_s3_parameters((remote_uploader_downloader: RemoteUploaderDownloader,recorddict: RecordDict,msg_str: str,))→{sharding::load_model_states_from_file,metrics::log,lr_scheduler::step,io::download_file_from_s3,s3_helper::extract_s3_comm_config_from_configrecord},load_recorddict_parameters_from_remote((remote_uploader_downloader: RemoteUploaderDownloader | None,incoming_message: Message,msg_str: str,comm_stack: CommStack,params_garbage_queue: Queue[ray.ObjectRef | str] | None = None,))→{load_from_shm_parameters,load_from_s3_parameters,comms_ray_batch_variants::load_from_ray_parameters_with_strategy,metrics::log}] comms_ray_batch_variants.py→[load_from_ray_parameters_with_strategy((recorddict: RecordDict,msg_str: str,ray_gc_queue: Queue[ray.ObjectRef | str] | None = None,))→{load,load,lr_scheduler::step,metrics::log},load((obj_refs: list[ray.ObjectRef],ray_gc_queue: Queue[ray.ObjectRef | str] | None,packed_meta: list[list[dict]] | None = None,))→{resolve_meta,integration_tests::main},offload_to_ray_legacy((parameters: list[np.ndarray],recorddict: RecordDict,msg_str: str,))→{integration_tests::main},offload_to_ray((parameters: list[np.ndarray],recorddict: RecordDict,msg_str: str,*,is_packed: bool = False,))→{build_size_batches},__init__((self,strategy_name: str))[CTOR,DUNDER],build_size_batches((arrays: list[np.ndarray],*,target_batch_bytes: int = PHOTON_RAY_TARGET_BATCH_BYTES,min_batches_to_merge: int = PHOTON_RAY_MIN_BATCHES_TO_MERGE,max_batches: int = PHOTON_RAY_MAX_BATCHES,))[HOT],resolve_meta((ready_meta: list[list[dict]],arrays_fp32: list[np.ndarray],tensor_dict: dict[int,np.ndarray],))[HOT]] composer_trainer_getters.py→[get_optimizer_states_from_trainer((trainer: Trainer,layer_names: list[str],layer_types: list[ModelStateNames],))→{extract_optimizer_states,metrics::log,utils::split_experts_opt_dict_tensors,metrics::log,utils::get_n_experts_from_model,metrics::log},calculate_trainer_post_training_l2_norms((model_states: dict[ModelStateNames,NDArrays],train_metrics: dict[str,Scalar],payload: NDArrays,fit_config: FitConfig,))→{utils::compress_with_strict,shared_utils::get_ndarrays_and_names_from_payload,shared_utils::calculate_l2_norms_per_state_and_mode,shared_utils::filter_expert_parameters,shared_utils::filter_expert_parameters},get_trainer_model_states((trainer: Trainer,fit_config: FitConfig,train_metrics: dict[str,Scalar],))→{get_optimizer_states_from_trainer,get_parameters_from_trainer,shared_utils::filter_expert_parameters,shared_utils::modify_aggregation_mask_with_frozen_layers},get_parameters_from_trainer((trainer: Trainer,parameter_names: Iterable[str] | None = None,))→{utils::split_experts_layers_tensors,utils::get_n_experts_from_model,model_parameters_getters::get_tensors_dict},update_client_state((client_state: ClientState,trainer: Trainer,fit_config: FitConfig,))→{math::rope,utils::is_literal_for_ast},get_wte_parameters_from_trainer((trainer: Trainer))→{model_parameters_getters::get_wte_parameters_from_model},extract_optimizer_states((opt_state_dict: dict[str,dict[str,torch.Tensor]],layer_names: list[str],layer_types: list[ModelStateNames],accumulator_per_type: dict[ModelStateNames,NDArrays],))] composer_trainer_setters.py→[set_trainer_states_for_fit((# noqa: PLR0914 payload: NDArrays,trainer: Trainer,configs: tuple[FitConfig,DictConfig],client_state_struct: ClientState,train_metrics: dict[str,Scalar],))→{set_trainer_tensors_dict,set_trainer_optimizer_states,utils::create_tensors_dict,utils::set_frozen_layers_tensors_dict,metrics::log,shared_utils::get_ndarrays_and_names_from_payload},set_trainer_states_for_eval((payload: NDArrays,trainer: Trainer,eval_config: EvaluateConfig,))→{set_trainer_tensors_dict,utils::merge_experts_layers_tensors,utils::get_n_experts_from_model,utils::create_tensors_dict,shared_utils::get_ndarrays_and_names_from_payload,shared_utils::filter_expert_parameters},set_trainer_optimizer_states((momentum_vectors_set: tuple[NDArrays,list[str],list[str]],trainer: Trainer,fit_config: FitConfig,))→{utils::invert_opt_state_dict_keys,optimizer_states_setters::set_optimizers_states,utils::merge_experts_opt_dict_tensors,utils::get_n_experts_from_model,optimizer_states_setters::_cast,math::rope},set_trainer_tensors_dict((trainer: Trainer,tensors_dict: OrderedDict[str,torch.Tensor],))→{model_parameters_setters::set_model_tensors_dict,utils::merge_experts_layers_tensors,utils::get_n_experts_from_model}] configs.py→[typed_field_validator((field: str,/,*field_names: str,mode: FieldValidatorModes = "after",)),get_photon_fit_config_fn((cfg: BaseConfig,aggregation_mask: Callable[ [str | int,int],tuple[tuple[bool,...],list[str],list[str]],],layer_names_and_types: tuple[tuple[str,ModelStateNames],...],)),get_photon_evaluate_config_fn((cfg: BaseConfig,))] constants.py→[] convert_dataset_hf.py→[main((args: Namespace))[ENTRY]→{runtime_estimator::init,metrics::log,runtime_estimator::init,metrics::log,metrics::log,samples_generators::generate_samples_from_dataloader},parse_args(void)→{metrics::log,permute_indices_testing::setUp}] convert_from_hf.py→[] convert_hf_to_dcp_with_gpus.py→[_create_fqn_mappings((self,state_dict: dict[str,torch.Tensor]))→{convert_to_titan_fqns,permute_indices_testing::setUp,permute_indices_testing::setUp,permute_indices_testing::setUp,permute_indices_testing::setUp,permute_indices_testing::setUp},_verify_state_dict((state_dict: dict[str,torch.Tensor],path: str,rank: int))→{permute_indices_testing::setUp,math::rope},extract_layer_number((s))→{runtime_estimator::init},convert_to_titan_fqns((fqn: str))→{extract_layer_number},_load_metadata((self))→{math::rope},_get_load_assignments((self,state_dict: dict[str,Any]))→{convert_to_hf_shape},_create_verified_state_dict((pg: dist.ProcessGroup,mesh: DeviceMesh))→{simple_fsdp::_distribute_dtensor},convert_to_hf_shape((fqn: str,titan_fqns: list[str],dtensor: DTensor)),convert_to_titan_tensors((fqn: str,full_tensor: torch.Tensor)),__init__((self,process_group: dist.ProcessGroup,path: str,token: Optional[str] = None,loader_every_n_ranks: int = 8,))[CTOR,DUNDER],convert((self,state_dict: dict[str,torch.Tensor])),_load_round((self,assignment: _Assignment)),_reshard_send((self,assignment: _Assignment,loaded_state_dict: dict[str,torch.Tensor],)),_reshard_receive((self,assignment: _Assignment,state_dict: dict[str,torch.Tensor])),_reshard((self,result: dict[str,torch.Tensor],state_dict: dict[str,torch.Tensor],))] convert_meta_to_dcp_with_gpus.py→[_create_fqn_mappings((self,state_dict: dict[str,torch.Tensor]))→{permute_indices_testing::setUp,permute_indices_testing::setUp,permute_indices_testing::setUp,permute_indices_testing::setUp,permute_indices_testing::setUp,permute_indices_testing::setUp},_create_verified_state_dict((pg: dist.ProcessGroup,mesh: DeviceMesh))→{simple_fsdp::_distribute_dtensor},convert_to_titan_fqns((fqn: str)),get_shard_dim((fqn: str)),split_fused_qkv((shards: list[torch.Tensor])),__init__((self,process_group: dist.ProcessGroup,path: str,loader_every_n_ranks: int = 8,))[CTOR,DUNDER],convert((self,state_dict: dict[str,torch.Tensor])),_get_file_path((self,loader_id: int)),_load_metadata((self)),_get_load_assignments((self,state_dict: dict[str,torch.Tensor])),_load_round((self,assignment: _Assignment)),_reshard_send((self,assignment: _Assignment,loaded_state_dict: dict[str,torch.Tensor],)),_reshard_receive((self,assignment: _Assignment,state_dict: dict[str,torch.Tensor])),_reshard((self,results: list[dict[str,torch.Tensor]],state_dict: dict[str,torch.Tensor],)),_verify_state_dict((state_dict: dict[str,torch.Tensor],path: str,rank: int))] convert_to_hf.py→[] custom_args.py→[] data_types.py→[] dataset_constants_types.py→[] dataset_types.py→[] debug.py→[benchmark_performance(void)→{pytorch_reference,pytorch_reference,create_aligned_test_data,runtime_estimator::init,runtime_estimator::init,runtime_estimator::init},run_all_tests(void)→{benchmark_performance,debug::test_large,debug::test_medium,debug::test_small},pytorch_reference((inputs: torch.Tensor,expert_weights: torch.Tensor,expert_indices: torch.Tensor,group_size_m: int = 128,))→{integration_tests::main},create_aligned_test_data((batch_size: int,seq_len: int,hidden_dim: int,output_dim: int,num_experts: int,group_size_m: int = 128,device: str = "cuda",dtype: torch.dtype = torch.float16,)),verify_results((output_triton: torch.Tensor,output_reference: torch.Tensor,rtol: float = 1e-2,atol: float = 1e-2,))] decoupled_weight_decay.py→[] dispatch.py→[__init__((self,group_name: str,align: int,in_len,out_len,token_shape,num_ranks,num_local_experts,dtype,device: torch.device,))[CTOR,DUNDER],forward((self,inp: torch.Tensor,out: torch.Tensor,in_splits: torch.Tensor,out_splits_offsets: torch.Tensor,))] dispatcher.py→[dispatch_strategy_functional((cfg: BaseConfig,))→{fedadam::federated_adam,fedyogi::federated_yogi,fedavg_eff::federated_averaging_efficient,fedmom::federated_averaging_with_momentum,fednesterov::federated_averaging_with_nesterov_momentum},__init__((self,strategy_name: str))[CTOR,DUNDER],dispatch_strategy_state_keys((cfg: BaseConfig))] download.py→[print_usage(void)] download_autoencoder.py→[hf_download((repo_id: str,file_path: str,local_dir: str,hf_token: Optional[str] = None))] download_hf_assets.py→[download_hf_assets((repo_id: str,local_dir: str,asset_types: str | list[str],download_all: bool = False,hf_token: Optional[str] = None,additional_patterns: Optional[list] = None,))] download_mosaic_model.py→[load_checkpoint_into_model_from_path((model: ComposerModel,checkpoint_path: str,model_name: str,*,wte_only: bool = False,))→{get_n_experts_from_model_name,get_n_experts_from_model_name,utils::create_tensors_dict,utils::merge_experts_layers_tensors,model_parameters_setters::set_model_tensors_dict,s3_helper::list_objects},get_randomly_initialized_model((model_name: str,tokenizer_name: str,safe_tensors_dir: str,))→{lr_scheduler::step,lr_scheduler::step,lr_scheduler::step},download_and_load_pretrained_model((args: argparse.Namespace,))→{load_checkpoint_into_model_from_path,load_checkpoint_into_model_from_path,get_randomly_initialized_model},save_safetensors_model((model: HuggingFaceModel | ComposerModel,safe_tensors_dir: str,))→{optimizer_states_setters::_cast,optimizer_states_setters::_cast},get_n_experts_from_model_name((model_name: str))→{utils::get_n_experts_from_model_config},overwrite_model_config_file((safe_tensors_dir: str,))] download_python_edu.py→[main((output_dir: str,num_proc: int))[ENTRY],download_contents((blob_id: str))→{metrics::log}] dsgemm_kernels.py→[groupwise_activation_quant((x: torch.Tensor,block_size: int = 128,switching_size=2048,))→{integration_tests::main,integration_tests::main}] dsgemm_unit_testing.py→[create_m_indices_fast((m_sizes: torch.Tensor)),per_token_cast_to_fp8((x: torch.Tensor)),per_block_cast_to_fp8((x: torch.Tensor)),compute_reference_with_scaling((lhs: torch.Tensor,lhs_scales: torch.Tensor,rhs: torch.Tensor,rhs_scales: torch.Tensor,m_indices: torch.Tensor,num_groups: int,))[HOT]] dsgemm_utils.py→[make_grouped_layout((num_groups: int,x: torch.Tensor,y: torch.Tensor))→{get_col_major_tma_aligned_tensor,dsgemm_unit_testing::per_token_cast_to_fp8,dsgemm_unit_testing::per_block_cast_to_fp8,dsgemm_unit_testing::per_token_cast_to_fp8},construct_grouped((num_groups: int,x: torch.Tensor,y: torch.Tensor,is_masked: bool))→{get_col_major_tma_aligned_tensor,dsgemm_unit_testing::per_token_cast_to_fp8,dsgemm_unit_testing::per_block_cast_to_fp8,dsgemm_unit_testing::per_token_cast_to_fp8},prepare_fp8_input((x))→{get_col_major_tma_aligned_tensor,dsgemm_unit_testing::per_token_cast_to_fp8},per_block_cast_to_fp8((x: torch.Tensor))→{ceil_div,ceil_div},prepare_fp8_weight((w))→{dsgemm_unit_testing::per_block_cast_to_fp8},get_tma_aligned_size((x: int,element_size: int))→{ceil_div},get_col_major_tma_aligned_tensor((x: torch.Tensor))→{get_tma_aligned_size},compare_fp8_tensors((a: torch.Tensor,b: torch.Tensor)),create_indices_from_offsets_nosync((m_offsets: torch.Tensor)),create_m_indices_from_offsets((m_offsets: torch.Tensor)),create_m_indices_from_sizes((m_sizes: torch.Tensor)),get_m_indices((num_groups: int,m: int)),set_num_sms((num_sms: int)),get_num_sms(void),ceil_div((x: int,y: int)),get_m_alignment_for_contiguous_layout(void),per_token_cast_to_fp8((x: torch.Tensor))] engine.py→[close((self))] estimation.py→[estimate_memory((job_config: JobConfig))→{numerical_tests_example::loss_fn,lr_scheduler::build_lr_schedulers,optimizer::build_optimizers,model_converter::build_model_converters,__init__::get_train_spec,runtime_estimator::init}] eval_fit_round.py→[eval_fit_round((cfg: BaseConfig,server_state: ServerState,server_load_balancer: ServerLoadBalancer,comm_stack: CommStack,server_optimizer: ServerOptimizer,))→{handle_client_state_accumulator,get_aggregation_mask,get_fit_config_fn,create_eval_messages,aggregation::weighted_average,aggregation::weighted_average},get_aggregation_mask((server_state: ServerState,))→{permute_indices_testing::setUp,permute_indices_testing::setUp,masks_utils::generate_full_mask},handle_client_state_accumulator((server_state: ServerState,metrics: list[tuple[int,dict[str,Scalar]]],))→{integration_tests::main,optimizer_states_setters::_cast},create_eval_messages((cfg: BaseConfig,server_state: ServerState,server_load_balancer: ServerLoadBalancer,))→{configs::get_photon_evaluate_config_fn,utils::construct_message_for_client},get_fit_config_fn((cfg: BaseConfig,server_state: ServerState,))→{configs::get_photon_fit_config_fn}] eval_mosaic.py→[] example_train.py→[batch_generator((self,data_iterable: Iterable[tuple[dict[str,torch.Tensor],torch.Tensor]]))→{lr_scheduler::step},forward_backward_step((self,input_dict: dict[str,torch.Tensor],labels: torch.Tensor))→{attention::init_attention_mask},train_step((self,data_iterator: Iterable[tuple[dict[str,torch.Tensor],torch.Tensor]])),state_dict((self)),load_state_dict((self,state_dict: dict[str,Any])),close((self))] expert_parallel.py→[_partition_fn((self,name,module,device_mesh))→{simple_fsdp::_distribute_dtensor,simple_fsdp::_distribute_dtensor,simple_fsdp::_distribute_dtensor},_partition_fn_2d((self,name,mod,ep_tp_mesh))→{simple_fsdp::_distribute_dtensor,simple_fsdp::_distribute_dtensor,simple_fsdp::_distribute_dtensor},set_token_group_alignment_size_m((alignment_size: ValidTokenGroupAlignmentSize,)),_apply((self,module: nn.Module,device_mesh: DeviceMesh)),__init__((self))[CTOR,DUNDER],_token_dispatch((self,mod,inputs,device_mesh)),_token_combine((self,mod,routed_output,device_mesh)),_apply((self,module: nn.Module,device_mesh: DeviceMesh)),__init__((self,tp_mesh: DeviceMesh,ep_mesh: DeviceMesh,))[CTOR,DUNDER],_token_dispatch((self,mod,inputs,device_mesh)),_token_combine((self,mod,routed_output,device_mesh)),_apply((self,module: nn.Module,device_mesh: DeviceMesh)),expert_parallel((func: Callable)),__init__((self))[CTOR,DUNDER],_prepare_inputput_fn((self,mod,inputs,device_mesh)),_prepare_output_fn((self,mod,outputs,device_mesh)),_apply((self,module: nn.Module,device_mesh: DeviceMesh))] expert_similarity_monitor.py→[] experts_assignment.py→[experts_assignment((cfg: BaseConfig,client_id: str | int,server_round: int,))→{fixed_assignment,random_assignment,lr_scheduler::step,lr_scheduler::step,permute_indices_testing::setUp,permute_indices_testing::setUp},fixed_assignment((expert_ids_to_assign: set[int],client_id: str | int))→{runtime_estimator::init,runtime_estimator::init},random_assignment((expert_ids_to_assign: set[int],clients_per_round: int,client_id: str | int,experts_overlapping_factor: int,rng: secrets.SystemRandom,))→{runtime_estimator::init},__init__((self,message: str))[CTOR,DUNDER]] experts_selection_callback.py→[] fast_debug_ao.py→[] fedadam.py→[federated_adam((# noqa: PLR0913,PLR0917 server_state: ServerState,fedavg_result: dict[tuple[str,ModelStateNames],np.ndarray],beta_1: float = 0.9,beta_2: float = 0.999,eta: float = 1e-1,tau: float = 1e-9,))→{utils::l2_norm,utils::l2_norm,utils::l2_norm,utils::l2_norm,utils::l2_norm}] fedavg_eff.py→[federated_averaging_efficient((server_state: ServerState,fedavg_result: dict[tuple[str,ModelStateNames],np.ndarray],aggregation_num_workers_process_ndarrays: int | None = None,server_learning_rate: float = 1.0,*,ndarrays_parallelization: bool = True,))] fedmom.py→[federated_averaging_with_momentum((server_state: ServerState,fedavg_result: dict[tuple[str,ModelStateNames],np.ndarray],server_learning_rate: float = 1.0,server_momentum: float = 0.9,))→{utils::l2_norm,utils::l2_norm,utils::l2_norm,utils::l2_norm}] fednesterov.py→[federated_averaging_with_nesterov_momentum((server_state: ServerState,fedavg_result: dict[tuple[str,ModelStateNames],np.ndarray],server_learning_rate: float = 1.0,server_momentum: float = 0.9,))→{utils::l2_norm,utils::l2_norm,utils::l2_norm,utils::l2_norm}] fedyogi.py→[federated_yogi((# noqa: PLR0913,PLR0917 server_state: ServerState,fedavg_result: dict[tuple[str,ModelStateNames],np.ndarray],beta_1: float = 0.9,beta_2: float = 0.999,eta: float = 1e-1,tau: float = 1e-9,))→{utils::l2_norm,utils::l2_norm,utils::l2_norm,utils::l2_norm,utils::l2_norm}] file_utils.py→[] finemath.py→[] fit_eval_callback.py→[compose_metrics((self))→{optimizer_states_setters::_cast,optimizer_states_setters::_cast,comms_ray_batch_variants::load,optimizer_states_setters::_cast,integration_tests::main,optimizer_states_setters::_cast},__init__((self,time_unit: str = "hours"))[CTOR,DUNDER],record_send_message((self,message: Message)),record_recv_message((self,message: Message)),start_record_submit_aggregation((self)),end_record_submit_aggregation((self)),start_record_waiting_for_aggregation((self)),end_record_waiting_for_aggregation((self)),start_record_server_optimization((self)),end_record_server_optimization((self))] flux_dataset.py→[_get_data_iter((self))→{lr_scheduler::step,lr_scheduler::step,lr_scheduler::step},_cc12m_wds_data_processor((sample: dict[str,Any],t5_tokenizer: FluxTokenizer,clip_tokenizer: FluxTokenizer,output_size: int = 256,))→{_process_cc12m_image},_coco_data_processor((sample: dict[str,Any],t5_tokenizer: FluxTokenizer,clip_tokenizer: FluxTokenizer,output_size: int = 256,))→{_process_cc12m_image},__init__((self,dataset_name: str,dataset_path: Optional[str],t5_tokenizer: BaseTokenizer,clip_tokenizer: BaseTokenizer,job_config: Optional[JobConfig] = None,dp_rank: int = 0,dp_world_size: int = 1,infinite: bool = False,))[CTOR,DUNDER]→{hf_datasets::_validate_dataset},build_flux_dataloader((dp_world_size: int,dp_rank: int,job_config: JobConfig,# This parameter is not used,keep it for compatibility tokenizer: FluxTokenizer | None,infinite: bool = True,))[HOT]→{tokenizer::build_flux_tokenizer},build_flux_validation_dataloader((dp_world_size: int,dp_rank: int,job_config: JobConfig,# This parameter is not used,keep it for compatibility tokenizer: BaseTokenizer | None,generate_timestamps: bool = True,infinite: bool = False,))[HOT]→{tokenizer::build_flux_tokenizer},_process_cc12m_image((img: PIL.Image.Image,output_size: int = 256,)),_validate_dataset((dataset_name: str,dataset_path: Optional[str] = None)),__iter__((self))[DUNDER],load_state_dict((self,state_dict)),state_dict((self)),__init__((self,dataset_name: str,dataset_path: Optional[str],t5_tokenizer: BaseTokenizer,clip_tokenizer: BaseTokenizer,job_config: Optional[JobConfig] = None,dp_rank: int = 0,dp_world_size: int = 1,generate_timesteps: bool = True,infinite: bool = False,))[CTOR,DUNDER],__iter__((self))[DUNDER]] flwr_utils.py→[deflate((self))] gather.py→[build_gather_base_recorddict((gather_msg: str,aggregation_mask: tuple[tuple[bool,...],list[str],list[str]],batch_id: int = 0,))[HOT]→{math::rope,lr_scheduler::step,runtime_estimator::init,generic::build_success_recorddict},build_gather_res_recorddict((aggregation_mask: tuple[tuple[bool,...],list[str],list[str]],num_examples: int,batch_id: int = 0,))[HOT]→{build_gather_base_recorddict},gather_res_recorddict_to_num_samples((gather_res_recorddict: RecordDict,))→{optimizer_states_setters::_cast}] gather_utils.py→[finalize_gather_message((node_id: int,server_state: ServerState,comm_stack: CommStack,aggregation_mask: tuple[tuple[bool,...],list[str],list[str]],))→{lr_scheduler::step,lr_scheduler::step,gather::build_gather_base_recorddict,lr_scheduler::step,lr_scheduler::step,gather::build_gather_base_recorddict}] generate.py→[create_model((dist_config: DistConfig))→{checkpoint::load_weights_from_hf},decode((tokenizer,x))→{colorize_chat},colorize_chat((text,user_color=None,assistant_color=None,output_color=None)),create_dist_config((mesh: DeviceMesh)),time_generation((func))] generate_amd_eval_scripts.py→[main(void)[ENTRY]→{create_eval_script_for_run,top_level_runs_from_objects,list_checkpoints_bucket,permute_indices_testing::setUp,permute_indices_testing::setUp,permute_indices_testing::setUp},list_checkpoints_bucket((prefix: str = "s3://checkpoints/"))→{s3_helper::list_objects},top_level_runs_from_objects((objects: Iterable[str]))→{permute_indices_testing::setUp},create_eval_script_for_run((run: str,out_dir: Path))] generate_eval_scripts_from_saved_list.py→[main(void)[ENTRY]→{write_script_for_run,find_runs},find_runs((objects_file: Path))→{permute_indices_testing::setUp},write_script_for_run((out_dir: Path,run: str))] generate_expert_eval_scripts.py→[main(void)[ENTRY]→{write_script,find_run_uuids},find_run_uuids((scan_dir: Path))→{permute_indices_testing::setUp},write_script((out_dir: Path,run: str))] generic.py→[build_success_with_metrics_recorddict((msg_str: str,metrics: dict[str,Scalar],num_examples: int,))[HOT]→{optimizer_states_setters::_cast,runtime_estimator::init},get_metrics_from_recorddict((recorddict: RecordDict,msg_str: str,))→{optimizer_states_setters::_cast,runtime_estimator::init},build_success_recorddict((msg_str: str,))[HOT]→{runtime_estimator::init},build_metrics_recorddict((msg_str: str,metrics: dict[str,float],loss: float,num_examples: int,))[HOT]→{runtime_estimator::init},get_client_id_from_train_eval_ins_recorddict((recorddict: RecordDict,msg_str: str,))→{runtime_estimator::init},build_failure_recorddict((msg_str: str,))[HOT]] group_gemms.py→[execute((self,contig_tokens,m_sizes,m_offsets,module))→{cg_forward::cg_grouped_gemm_forward,cg_forward::cg_grouped_gemm_forward,cg_forward::cg_grouped_gemm_forward},execute((self,contig_tokens,m_sizes,m_offsets,module))→{mg_grouped_gemm::grouped_gemm_forward,mg_grouped_gemm::grouped_gemm_forward,mg_grouped_gemm::grouped_gemm_forward},__init__((self,custom_activation))[CTOR,DUNDER],arrange_expert_weights((self,all_weights,submod_name,module)),execute((self,contig_tokens,m_sizes,m_offsets,module)),arrange_expert_weights((self,all_weights,submod_name,module)),execute((self,contig_tokens,m_sizes,m_offsets,module)),arrange_expert_weights((self,all_weights,submod_name,module)),arrange_expert_weights((self,all_weights,submod_name,module)),execute((self,contig_tokens,m_sizes,m_offsets,module)),arrange_expert_weights((self,all_weights,submod_name,module)),arrange_expert_weights((self,all_weights,submod_name,module)),execute((self,contig_tokens,m_sizes,m_offsets,module)),__init__((self,custom_activation,use_triton_quant=True))[CTOR,DUNDER],arrange_expert_weights((self,all_weights,submod_name,module)),execute((self,contig_tokens,m_sizes,m_offsets,module))] helper.py→[restore_check_from_safetensors((checkpoint_files: list[str],*,preserve_optimizer_states: bool,**kwargs: Any,# noqa: ANN401,ARG001))→{sharding::load_chunked_model,sharding::load_chunked_model},restore_check_from_numpy((checkpoint_files: list[str],*,preserve_optimizer_states: bool,**kwargs: Any,# noqa: ANN401))→{metrics::log,lr_scheduler::step},validate_torch_check((checkpoints_to_download: list[str],desired_steps: int | None,))→{runtime_estimator::init,runtime_estimator::init},restore_check_from_torch((checkpoint_files: list[str],*,n_experts: int | None = None,preserve_optimizer_states: bool = True,**kwargs: Any,# noqa: ANN401,ARG001))→{utils::transform_torch_checkpoint_to_photon_model_states},determine_checkpoints_and_type((centralised_checkpoint_dir: str,))→{s3_helper::list_objects},retrieve_model_states_and_types((checkpoint_files: list[str],n_experts: int | None,*,is_remote: bool,preserve_optimizer_states: bool,))] hf_datasets.py→[_get_data_iter((self))→{lr_scheduler::step,lr_scheduler::step,lr_scheduler::step},__init__((self,dataset_name: str,dataset_path: str | None,tokenizer: BaseTokenizer,seq_len: int = 2048,dp_rank: int = 0,dp_world_size: int = 1,infinite: bool = False,))[CTOR,DUNDER]→{_validate_dataset},_load_c4_dataset((dataset_path: str,split: str)),_process_c4_text((sample: dict[str,Any])),_validate_dataset((dataset_name: str,dataset_path: str | None = None)),__iter__((self))[DUNDER],load_state_dict((self,state_dict)),state_dict((self)),build_hf_dataloader((dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer,job_config: JobConfig,infinite: bool = True,))[HOT],build_hf_validation_dataloader((dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer,job_config: JobConfig,infinite: bool = False,))[HOT]] hf_embedder.py→[__init__((self,version: str,random_init=False,**hf_kwargs))[CTOR,DUNDER],forward((self,batch_tokens: Tensor))] hf_tokenizer.py→[remove_notset_root_handlers(void),__init__((self,tokenizer))[CTOR,DUNDER],encode((self,text,bos=False,eos=False,**kwargs)),__getattr__((self,name))[DUNDER],get_hf_tokenizer((model_id: str))] hydra_resolver.py→[] incoming_message.py→[queue_gather_results_incoming((message: Message,aggregation_queue: Queue[tuple[np.ndarray,int,str,ModelStateNames]],server_state: ServerState,comm_stack: CommStack,))→{optimizer_states_setters::_cast,optimizer_states_setters::_cast,gather::gather_res_recorddict_to_num_samples,comms::load_recorddict_parameters_from_remote,metrics::log,utils::check_message_success},create_gather_messages((message: Message,server_state: ServerState,comm_stack: CommStack,aggregation_mask: tuple[tuple[bool,...],list[str],list[str]],outgoing_messages: deque,))→{generic::get_metrics_from_recorddict,gather_utils::finalize_gather_message,metrics::log,utils::check_message_success},handle_evaluate_message((message: Message,))→{generic::get_metrics_from_recorddict,metrics::log,utils::check_message_success}] indices.py→[generate_permute_indices((tokens_per_expert_group: torch.Tensor,experts_per_rank: int,num_ranks: int,max_len: int,alignment: int,use_cpu: bool = False,))→{fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},simple_test(void)→{generate_permute_indices,generate_permute_indices},fill_indices_wrapper((tokens_per_expert_group: torch.Tensor,start_index_values: torch.Tensor,write_offsets: torch.Tensor,experts_per_rank: int,num_ranks: int,max_len: int,block_size: int = 128,max_blocks: int = 1024,# cap on total number of blocks to launch))→{integration_tests::main},fill_indices_cpu((tokens_per_expert_group: torch.Tensor,start_index_values: torch.Tensor,write_offsets: torch.Tensor,experts_per_rank: int,num_ranks: int,max_len: int,))→{integration_tests::main}] infer.py→[] init_utils.py→[restore_run_from_centralised_checkpoint((centralised_checkpoint_path: str,cfg: BaseConfig,*,preserve_optimizer_states: bool,))→{utils::get_n_experts_from_model,s3_helper::list_objects,s3_checkpoint_loading::get_checkpoint_files_for_path,helper::retrieve_model_states_and_types,trainer_utils::get_trainer_object,lr_scheduler::step},resume_from_round((cfg: BaseConfig,remote_up_down: RemoteUploaderDownloader | None,photon_save_path: str,))→{initialize_federated_learning,s3_checkpoint_loading::get_server_checkpoint,metrics::log,metrics::log,metrics::log,s3_checkpoint_loading::copy_old_checkpoints_to_new_run},resolve_pretrained_model_path((cfg: BaseConfig,initial_model_states: NDArrays,layer_names_and_types: tuple[tuple[str,ModelStateNames],...],))[HOT]→{_get_n_experts_from_base_config,s3_helper::list_objects,s3_checkpoint_loading::get_checkpoint_files_for_path,helper::retrieve_model_states_and_types,metrics::log},initialize_federated_learning((cfg: BaseConfig,remote_up_down: RemoteUploaderDownloader | None,photon_save_path: str,))→{resolve_pretrained_model_path,dispatcher::dispatch_strategy_state_keys,param_scheduler_dispatcher::dispatch_model_state_scheduler,param_scheduler_dispatcher::dispatch_model_state_scheduler,utils::get_random_model_states_from_config},_get_n_experts_from_base_config((cfg: BaseConfig,))→{utils::get_n_experts_from_model_config},initialize_server_state((# noqa: C901 cfg: BaseConfig,photon_save_path: str,))] io.py→[download_file_from_s3((remote_up_down: RemoteUploaderDownloader,remote_file_name: str,local_file_name: str,))→{lr_scheduler::step},upload_file_to_s3((remote_up_down: RemoteUploaderDownloader,remote_file_name: str,local_file_name: str,)),create_remote_up_down((# noqa: PLR0913 bucket_name: str,prefix: str,run_uuid: str | None,num_attempts: int,client_config: dict[str,Any],*,num_concurrent_uploads: int = 1,upload_staging_folder: str | None = None,use_procs: bool = True,))] job_config.py→[] layers.py→[forward((self,ids: Tensor))→{math::rope},forward((self,x: Tensor,pe: Tensor))→{math::attention},__init__((self,hidden_size: int,num_heads: int,mlp_ratio: float,qkv_bias: bool = False))[CTOR,DUNDER]→{runtime_estimator::init},forward((self,img: Tensor,txt: Tensor,vec: Tensor,pe: Tensor))→{math::attention},__init__((self,hidden_size: int,num_heads: int,mlp_ratio: float = 4.0,qk_scale: float | None = None,))[CTOR,DUNDER]→{runtime_estimator::init},forward((self,x: Tensor,vec: Tensor,pe: Tensor))→{math::attention},__init__((self,dim: int,theta: int,axes_dim: list[int]))[CTOR,DUNDER],timestep_embedding((t: Tensor,dim,max_period=10000,time_factor: float = 1000.0)),__init__((self,in_dim: int,hidden_dim: int))[CTOR,DUNDER],init_weights((self,init_std: float = 0.02)),forward((self,x: Tensor)),__init__((self,dim: int))[CTOR,DUNDER],init_weights((self)),forward((self,q: Tensor,k: Tensor,v: Tensor)),__init__((self,dim: int,num_heads: int = 8,qkv_bias: bool = False))[CTOR,DUNDER],init_weights((self)),__init__((self,dim: int,double: bool))[CTOR,DUNDER],init_weights((self)),forward((self,vec: Tensor)),init_weights((self)),init_weights((self)),__init__((self,hidden_size: int,patch_size: int,out_channels: int))[CTOR,DUNDER],init_weights((self)),forward((self,x: Tensor,vec: Tensor))] llm_client_functions.py→[llm_fit((# noqa: PLR0914,PLR0915 external_objects: tuple[ Trainer | None,FitConfig | EvaluateConfig | None,TrainConfig | None,],payload: NDArrays,config: ConfigRecord,llm_config: DictConfig,))→{metrics::log,utils::post_process_client_result,metrics::log,lr_scheduler::step,composer_trainer_setters::set_trainer_states_for_fit,trainer_utils::correct_time_before_fit},llm_eval((external_trainer: Trainer | None,payload: NDArrays,config: ConfigRecord,llm_config: DictConfig,))→{utils::streaming_shms_clean_up,trainer_utils::trainer_clean_up,composer_trainer_setters::set_trainer_states_for_eval,trainer_utils::get_trainer_object}] llm_config_functions.py→[get_train_config((# noqa: PLR0913 cfg: DictConfig,cid: int | str | None,log_name: str | None = None,*,force_cpu: bool = False,no_data_loading: bool = False,split_eval: bool = False,))→{set_client_tensorboard_logger,set_client_wandb_logger,set_dataset_default_params,client_set_data_config,set_n_workers_dataloaders,adapt_train_batch_size_to_num_devices},set_client_checkpoints_path((cfg: DictConfig,cid: int | str,n_steps: int,))→{set_client_save_folder,metrics::log,lr_scheduler::step,runtime_estimator::init,runtime_estimator::init,s3_helper::list_objects},set_n_workers_dataloaders((train_cfg: TrainConfig,device: Device | DeviceGPU | DeviceCPU | None,cap: int = 32,))→{integration_tests::main,integration_tests::main,math::rope,utils::get_n_cuda_devices,runtime_estimator::init,utils::get_n_cpu_cores},get_stream_freq_dict_for_client((client_streams: dict[str,dict[str,Any]],s3_comm_config: S3CommConfig | None,run_uuid: str | None,cid: int | str | None,*,allow_failures: bool = False,))→{metrics::log,runtime_estimator::init,metrics::log,utils::merge_freq_dicts,runtime_estimator::init,io::download_file_from_s3},set_stream((cid: int | str | None,loader: dict[str,Any] | None,))→{get_actual_stream,concatenate_streams,preprocess_stream_paths,runtime_estimator::init},adapt_train_batch_size_to_num_devices((cfg: DictConfig))→{metrics::log,runtime_estimator::init,math::rope,lr_scheduler::step},get_split_streams((loader: dict[str,Any] | None,))→{get_actual_stream,preprocess_stream_paths,runtime_estimator::init},client_set_data_config((cid: int | str | None,train_cfg: TrainConfig,*,split_eval: bool = False,))→{get_split_streams,set_stream,set_stream},set_dataset_default_params((train_cfg: TrainConfig))→{integration_tests::main,integration_tests::main,integration_tests::main},concatenate_streams((clients_streams: list[dict[str,Any]]))→{math::rope},set_client_save_folder((cfg: DictConfig,cid: int | str))→{lr_scheduler::step},set_initial_config_from_fit_config((fit_config: FitConfig,llm_config: DictConfig,))→{set_client_checkpoints_path},check_skip_init((ext_client_config: FitConfig | EvaluateConfig | None,ext_train_config: TrainConfig | None,client_config: FitConfig | EvaluateConfig,train_config: TrainConfig,)),set_icl_tasks_root_dir((icl_tasks_listconfig: list[dict[str,Any]],root_dir: str,)),preprocess_stream_paths((dataset_config: DictConfig)),get_actual_stream((root_local: str,root_remote: str,split: str,current_client_stream: dict[str,Any],)),set_client_wandb_logger((train_cfg: TrainConfig,log_name: str)),set_client_tensorboard_logger((train_cfg: TrainConfig,log_name: str))] load_balancer.py→[check_nodes_health((self))→{metrics::log,permute_indices_testing::setUp,permute_indices_testing::setUp,metrics::log},assign_clients_to_nodes((self,sampled_clients: list[int]))→{metrics::log,integration_tests::main,permute_indices_testing::setUp,metrics::log},balance_nodes_assignments((self))→{integration_tests::main,integration_tests::main},set_all_nodes_states((self,*,state: bool))→{metrics::log},__init__((self,grid: Grid,total_number_of_clients: int,rng: random.Random,*,is_production: bool,))[CTOR,DUNDER],get_available_nodes((self)),get_registered_nodes((self)),get_node_id_for_client((self,client_id: int))] logging.py→[init_logger(void),warn_once((logger: logging.Logger,msg: str))] loss.py→[mse_loss((pred: torch.Tensor,labels: torch.Tensor)),build_mse_loss((job_config: JobConfig))[HOT]] manager.py→[_dict_to_dataclass((self,cls,data: dict[str,Any]))→{train::close,permute_indices_testing::setUp,permute_indices_testing::setUp},_maybe_load_toml((self,args: list[str]))→{math::rope},__init__((self,config_cls: Type[JobConfig] = JobConfig))[CTOR,DUNDER],parse_args((self,args: list[str] = sys.argv[1:])),_maybe_add_custom_args((self,args: list[str],toml_values: dict[str,Any] | None)),_validate_config((self))] masks_utils.py→[mask_to_batches((full_mask: tuple[tuple[bool,...],list[str],list[str]],layer_names_and_types: tuple[tuple[str,ModelStateNames],...],n_batches: int,))→{utils::compress_with_strict,utils::compress_with_strict,integration_tests::main,integration_tests::main},generate_mask((layer_names_and_types: tuple[tuple[str,ModelStateNames],...],scheduler: Callable[ [str | int,int],list[ModelStateNames] | dict[ModelStateNames,list[str]],],current_cid: int | str,server_round: int,))→{utils::compress_with_strict,runtime_estimator::init,metrics::log,math::rope},combine_masks((mask_a: tuple[tuple[bool,...],list[str],list[str]],mask_b: tuple[tuple[bool,...],list[str],list[str]],layer_names_and_types: tuple[tuple[str,ModelStateNames],...],))→{utils::compress_with_strict},reconcile_model_state_with_scheduler((model_states: NDArrays,layer_names_and_types: tuple[tuple[str,ModelStateNames],...],cfg: BaseConfig,))→{permute_indices_testing::setUp},create_layers_names_and_types_from_names((layer_names: list[str],)),generate_full_mask((layer_names_and_types: tuple[tuple[str,ModelStateNames],...],)),generate_empty_mask((layer_names_and_types: tuple[tuple[str,ModelStateNames],...],))] math.py→[attention((q: Tensor,k: Tensor,v: Tensor,pe: Tensor))→{apply_rope},rope((pos: Tensor,dim: int,theta: int)),apply_rope((xq: Tensor,xk: Tensor,freqs_cis: Tensor))] math_utils.py→[strip_string((string,skip_unit=False))→{_fix_a_slash_b,_fix_fracs,_fix_sqrt,convert_word_number,lr_scheduler::step},_fix_a_slash_b((string))→{lr_scheduler::step,lr_scheduler::step,runtime_estimator::init,runtime_estimator::init},convert_word_number((text: str))→{lr_scheduler::step},extract_answer((pred_str,data_name,use_last_number=True))→{strip_string},parse_math_answer((text: str,data_name))→{extract_answer},_fix_fracs((string)),_fix_sqrt((string))] mc4.py→[] messages_utils.py→[extract_fit_eval_ins((msg_content: RecordDict,state: PhotonRecordDict,msg_str: str,))→{runtime_estimator::init,runtime_estimator::init,runtime_estimator::init,lr_scheduler::step,math::rope},get_and_set_parameters_and_transmission_mask((state: PhotonRecordDict,node_manager_app: NodeManagerApp,))→{extract_train_eval_state,utils::compress_with_strict,utils::set_parameters_in_place,utils::compress_with_strict},transform_record_dict_to_photon((record_dict: RecordDict | PhotonRecordDict,))→{metrics::log,optimizer_states_setters::_cast},extract_gather_parameters_state((state: PhotonRecordDict,))→{optimizer_states_setters::_cast,lr_scheduler::step},insert_gather_parameters_state((state: PhotonRecordDict,gather_parameters_state: GatherParametersState,))→{math::rope,math::rope},extract_set_parameters_state((state: PhotonRecordDict,node_manager_app: NodeManagerApp,))→{masks_utils::generate_empty_mask,lr_scheduler::step},insert_set_parameters_state((state: PhotonRecordDict,set_parameters_state: SetParametersState,))→{math::rope,math::rope},extract_train_eval_state((state: PhotonRecordDict,))→{lr_scheduler::step}] mg_grouped_gemm.py→[grouped_gemm_backward((grad_output: torch.Tensor,x: torch.Tensor,w: torch.Tensor,m_sizes: torch.Tensor,use_tma: bool = True,tma_size: int = 128,))→{grouped_gemm_dw_tma,grouped_gemm_dx_tma},grouped_gemm_forward((x: torch.Tensor,w: torch.Tensor,m_sizes: torch.Tensor,tma_size: int = 128,using_fp8: bool = False,)),grouped_gemm_dx_tma((grad_output: torch.Tensor,w: torch.Tensor,m_sizes: torch.Tensor,num_sms: int = 132,tma_size: int = 128,)),grouped_gemm_dw_tma((x: torch.Tensor,grad_output: torch.Tensor,m_sizes: torch.Tensor,num_sms: int = 132,tma_size: int = 128,)),mg_grouped_gemm((x: torch.Tensor,w: torch.Tensor,m_sizes: torch.Tensor,use_tma: bool = True,tma_size: int = 128,using_fp8: bool = False,))] mm_collator.py→[padded_collate((batch: List[Dict[str,List[int]]],padding_idx: int = 0,ignore_idx: int = -100,))] mm_collator_nld.py→[] mm_dataset.py→[_process_obelics_sample((sample: dict[str,Any],image_token: str = "<|image|>"))→{integration_tests::main,utils::load_image},_get_data_iter((self))→{lr_scheduler::step,lr_scheduler::step},__init__((self,dataset_name: str,dataset_path: Optional[str],tokenizer: BaseTokenizer,image_token: str = "<|image|>",tile_size: int = 448,max_num_tiles: int = 4,seq_len: int = 2048,dp_rank: int = 0,dp_world_size: int = 1,infinite: bool = False,))[CTOR,DUNDER]→{_validate_mm_dataset},__iter__((self))[DUNDER]→{permute_indices_testing::setUp},_load_obelics_dataset((dataset_path: str)),_validate_mm_dataset((dataset_name: str,dataset_path: str = None)),load_state_dict((self,state_dict)),state_dict((self)),build_mm_dataloader((dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer,job_config: JobConfig,infinite: bool = True,))[HOT]] mm_datasets.py→[_process_mm_sample((texts: list[str] | str,images: list[bytes] | bytes,tokenizer: BaseTokenizer,patch_size: int,max_patch_per_image: int,spatial_merge_size: int,special_tokens: SpecialTokens,))→{text::process_text_with_images,image::calculate_image_tokens,image::process_image},_get_data_iter((self))→{lr_scheduler::step,lr_scheduler::step,lr_scheduler::step},_process_obelics_sample((sample: dict[str,Any],tokenizer: HuggingFaceTokenizer,patch_size: int,spatial_merge_size: int,max_patch_per_image: int,special_tokens: SpecialTokens,))→{_process_mm_sample},_process_cc12_wd_sample((sample: dict[str,Any],tokenizer: BaseTokenizer,patch_size: int,spatial_merge_size: int,max_patch_per_image: int,special_tokens: SpecialTokens,))→{_process_mm_sample},__init__((self,dataset_name: str,dataset_path: str | None,tokenizer: BaseTokenizer,batch_size: int,seq_len: int,patch_size: int,spatial_merge_size: int,max_patches_per_image: int,max_images_per_batch: int,packing_buffer_size: int,special_tokens: SpecialTokens,dp_rank: int = 0,dp_world_size: int = 1,infinite: bool = False,))[CTOR,DUNDER]→{mm_dataset::_validate_mm_dataset},_validate_mm_dataset((dataset_name: str,dataset_path: str | None = None)),__iter__((self))[DUNDER],load_state_dict((self,state_dict)),state_dict((self)),build_mm_dataloader((dp_world_size: int,dp_rank: int,tokenizer: HuggingFaceTokenizer,job_config: JobConfig,infinite: bool = True,))[HOT]] model.py→[_set_cos_sin_cache((self,seq_len,device,dtype))→{yarn_get_mscale,yarn_get_mscale,yarn_linear_ramp_mask,yarn_find_correction_range,comms_ray_batch_variants::load},__init__((self,layer_id: int,model_args: TransformerModelArgs,))[CTOR,DUNDER]→{math::rope,runtime_estimator::init,runtime_estimator::init,runtime_estimator::init,math::attention},yarn_find_correction_range((low_rot,high_rot,dim,base=10000,max_position_embeddings=2048))→{yarn_find_correction_dim,yarn_find_correction_dim,integration_tests::main,integration_tests::main},apply_rotary_emb((xq: torch.Tensor,xk: torch.Tensor,rope_cache: torch.Tensor))→{rotate_half,rotate_half,model::reshape_for_broadcast},forward((self,x: torch.Tensor,freqs_cis: torch.Tensor,))→{model::repeat_kv,model::repeat_kv,model::apply_rotary_emb},forward((self,x: torch.Tensor,rope_cache: torch.Tensor,))→{model::repeat_kv,model::repeat_kv,model::apply_rotary_emb},forward((self,x: torch.Tensor,freqs_cis: torch.Tensor,))→{model::repeat_kv,model::repeat_kv,model::apply_rotary_emb},forward((self,x: torch.Tensor,freqs_cis: torch.Tensor,))→{model::repeat_kv,model::repeat_kv,model::apply_rotary_emb},__init__((self,dim: int,hidden_dim: int,multiple_of: int,ffn_dim_multiplier: float | None,))[CTOR,DUNDER]→{runtime_estimator::init,runtime_estimator::init},__init__((self,layer_id: int,model_args: Qwen3ModelArgs))[CTOR,DUNDER]→{math::rope,math::attention},__init__((self,dim: int,hidden_dim: int,multiple_of: int,ffn_dim_multiplier: Optional[float],activation: nn.Module = nn.SiLUvoid,))[CTOR,DUNDER]→{runtime_estimator::init,runtime_estimator::init},apply_rotary_pos_emb((q,k,cos,sin,position_ids,unsqueeze_dim=1))→{rotate_half,rotate_half},__init__((self,config))[CTOR,DUNDER]→{get_group,lr_scheduler::step},forward((self,hidden_states: torch.Tensor,attention_mask: Optional[torch.Tensor] = None,position_ids: Optional[torch.LongTensor] = None,))→{apply_rotary_pos_emb,attn_mask_utils::_prepare_4d_causal_attention_mask},__init__((self,dim: int,hidden_dim: int,multiple_of: int,ffn_dim_multiplier: Optional[float],))[CTOR,DUNDER]→{runtime_estimator::init,runtime_estimator::init},__init__((self,config: ModelArgs,layer_idx: int))[CTOR,DUNDER]→{math::rope,math::attention},forward((self,x: torch.Tensor,encoder_input: torch.Tensor,mask: Optional[torch.Tensor] = None,))→{model::repeat_kv,model::repeat_kv},apply_rotary_emb((xq: torch.Tensor,xk: torch.Tensor,freqs_cis: torch.Tensor,))→{model::reshape_for_broadcast},forward((self,tokens: torch.Tensor,pixel_values: torch.Tensor,grid_thw: torch.Tensor,special_tokens: SpecialTokens,input_batch: torch.Tensor | None = None,))→{_scatter_img_tokens},forward((self,img: Tensor,img_ids: Tensor,txt: Tensor,txt_ids: Tensor,timesteps: Tensor,y: Tensor,))→{layers::timestep_embedding},__init__((self,model_args: Qwen3ModelArgs))[CTOR,DUNDER]→{attention::build_attention},__init__((self,model_args: TransformerModelArgs,use_rope: bool = True,fixed_block_size: int | None = None,))[CTOR,DUNDER]→{attention::build_attention},apply_rotary_emb((xq: torch.Tensor,xk: torch.Tensor,freqs_cis: torch.Tensor,))→{model::reshape_for_broadcast},__init__((self,model_args: Qwen3ModelArgs))[CTOR,DUNDER]→{lr_scheduler::step},forward((self,x: torch.Tensor,aspect_ratio: torch.Tensor))→{runtime_estimator::init},__init__((self,model_args: TransformerModelArgs))[CTOR,DUNDER]→{lr_scheduler::step},_precompute_rope_cache((self))[HOT]→{precompute_rope_cache},_precompute_freqs_cis((self))[HOT]→{model::precompute_freqs_cis},forward((self,x: torch.Tensor,aspect_ratio: torch.Tensor))→{runtime_estimator::init},__init__((self,model_args: ModelArgs,attn_scale: Optional[nn.Module] = None,mlp_scale: Optional[nn.Module] = None,))[CTOR,DUNDER]→{math::attention},moe_on_device((self,x,topk_ids,topk_weight))→{indices::generate_permute_indices},__init__((self,config: ModelArgs,layer_idx: Optional[int] = None))[CTOR,DUNDER]→{yarn_get_mscale},__init__((self,model_args: ModelArgs))[CTOR,DUNDER]→{runtime_estimator::init},__init__((self,config: ModelArgs))[CTOR,DUNDER]→{lr_scheduler::step},_precompute_freqs_cis((self,model_args))[HOT]→{model::precompute_freqs_cis},__init__((self,model_args: TransformerModelArgs))[CTOR,DUNDER],init_weights((self,*args,**kwargs)),_scatter_img_tokens((h_BSD,tokens_BS,i_NLD,i_mask_NL,img_id)),precompute_freqs_cis((dim: int,end: int,theta: float = 10000.0))[HOT],precompute_rope_cache((dim: int,max_seq_len: int,base: float = 1_000_000.0))[HOT],__init__((self,model_args: FluxModelArgs))[CTOR,DUNDER],__init__((self,in_dim: int,out_dim: int))[CTOR,DUNDER],rotate_half((x: torch.Tensor)),reshape_for_broadcast((freqs_cis: torch.Tensor,x: torch.Tensor)),forward((self,x_NLD: torch.Tensor)),__init__((self,model_args: BaseModelArgs))[CTOR,DUNDER],reshape_for_broadcast((rope_cache: torch.Tensor,x: torch.Tensor)),init_weights((self)),__init__((self,model_args: Llama3Siglip2ModelArgs))[CTOR,DUNDER],__init__((self,*args: Any,**kwargs: Any))[CTOR,DUNDER],get_group((dim_name: Optional[str] = None)),init_weights((self,buffer_device=None)),forward((self,x: torch.Tensor)),__init__((self,hidden_size,eps=1e-6))[CTOR,DUNDER],forward((self,hidden_states)),init_weights((self,buffer_device=None)),precompute_freqs_cis((dim: int,end: int,theta: float = 10000.0))[HOT],__init__((self,dim,max_position_embeddings=2048,base=10000,device=None))[CTOR,DUNDER],repeat_kv((x: torch.Tensor,n_rep: int)),repeat_kv((x: torch.Tensor,n_rep: int)),_set_cos_sin_cache((self,seq_len,device,dtype)),reshape_for_broadcast((freqs_cis: torch.Tensor,x: torch.Tensor)),forward((self,x,seq_len=None)),__init__((self,dim,max_position_embeddings=2048,base=10000,device=None,scaling_factor=1.0,))[CTOR,DUNDER],_set_cos_sin_cache((self,seq_len,device,dtype)),__init__((self,dim,max_position_embeddings=2048,base=10000,device=None,scaling_factor=1.0,))[CTOR,DUNDER],init_weights((self,init_std: float)),init_weights((self,init_std: float)),repeat_kv((x: torch.Tensor,num_rep: int)),_set_cos_sin_cache((self,seq_len,device,dtype)),__init__((self,model_args: ModelArgs))[CTOR,DUNDER],yarn_find_correction_dim((num_rotations,dim,base=10000,max_position_embeddings=2048)),yarn_get_mscale((scale=1,mscale=1)),yarn_linear_ramp_mask((min,max,dim)),init_weights((self,init_std: float)),__init__((self,dim,max_position_embeddings=2048,base=10000,device=None,scaling_factor=1.0,original_max_position_embeddings=4096,beta_fast=32,beta_slow=1,mscale=1,mscale_all_dim=0,))[CTOR,DUNDER],__init__((self,dim: int,hidden_dim: int,))[CTOR,DUNDER],forward((self,x)),forward((self,x)),init_weights((self,init_std: float)),init_weights((self,init_std: float)),rotate_half((x)),forward((self,x: torch.Tensor,rope_cache: torch.Tensor,)),forward((self,x)),init_weights((self,init_std: float)),__init__((self))[CTOR,DUNDER],forward((self,x: torch.Tensor)),init_weights((self,buffer_device: torch.device)),forward((self,x: torch.Tensor,freqs_cis: torch.Tensor,)),__init__((self,config,hidden_size=None,intermediate_size=None))[CTOR,DUNDER],__init__((self,max_num_tiles: int,emb_dim: int,))[CTOR,DUNDER],init_weights((self,buffer_device: torch.device)),forward((self,x)),__init__((self,config))[CTOR,DUNDER],init_weights((self,buffer_device: torch.device | None = None,)),reset_parameters((self)),forward((self,hidden_states)),__init__((self,emb_dim: int,tile_size: int,patch_size: int))[CTOR,DUNDER],init_weights((self,buffer_device: torch.device | None = None,)),forward((self,x: torch.Tensor,*args: Tuple[Any])),forward((self,tokens: torch.Tensor,input_batch: torch.Tensor | None = None,)),__init__((self,max_num_tiles: int,emb_dim: int,tile_size: int,patch_size: int))[CTOR,DUNDER],forward((self,tokens: torch.Tensor,input_batch: torch.Tensor | None = None,)),__init__((self,in_channels: int,out_channels: int,kernel_size: int,stride: int,bias: bool = False,))[CTOR,DUNDER],forward((self,x: torch.Tensor)),combine_experts((self,submod_name: str)),forward((self,x: torch.Tensor,mask: Optional[torch.Tensor] = None,)),setup_symm_mem((self,dtype: torch.dtype,device: torch.device)),__init__((self,emb_dim: int))[CTOR,DUNDER],forward((self,x: torch.Tensor)),get_send_buf((self)),get_gather_buf((self)),forward((self,hidden_states)),moe_forward((self,x,topk_ids,topk_weight)),sort_tokens((self,x,topk_ids,topk_weights)),__init__((self,model_args: ModelArgs,))[CTOR,DUNDER],_run_group_gemm((self,contig_tokens,m_sizes,m_offsets)),forward((self,images: torch.Tensor,aspect_ratio: Optional[torch.Tensor] = None)),__init__((self,model_args: ModelArgs,))[CTOR,DUNDER],forward((self,x: torch.Tensor,hidden_states: Optional[List[torch.Tensor]] = None,)),_init_rope((self)),forward((self,images: torch.Tensor,aspect_ratio: Optional[torch.Tensor] = None)),forward((self,x)),init_weights((self,init_std: float)),__init__((self,model_args: ModelArgs))[CTOR,DUNDER],init_weights((self,init_std: float)),forward((self,hidden_states: torch.Tensor,attention_mask: Optional[torch.Tensor] = None,position_ids: Optional[torch.LongTensor] = None,)),__init__((self,model_args: ModelArgs))[CTOR,DUNDER],init_weights((self,init_std: float)),__init__((self,model_args: ModelArgs,))[CTOR,DUNDER],forward((self,x: torch.Tensor,freqs_cis: torch.Tensor,**kwargs: Dict,)),__init__((self,model_args: ModelArgs,))[CTOR,DUNDER],_skip_mask((self,mask: Optional[torch.Tensor])),_init_weights((self,module)),forward((self,x: torch.Tensor,*,encoder_input: Optional[torch.Tensor] = None,encoder_mask: Optional[torch.Tensor] = None,**kwargs: Dict,)),forward((self,tokens: torch.Tensor,attention_mask: Optional[torch.Tensor] = None,position_ids: Optional[torch.LongTensor] = None,)),__init__((self,config))[CTOR,DUNDER],forward((self,tokens: torch.Tensor,attention_mask: Optional[torch.Tensor] = None,position_ids: Optional[torch.LongTensor] = None,)),__init__((self,layer: nn.Module,fusion_layer: nn.Module,fusion_first: bool = True))[CTOR,DUNDER],forward((self,x: torch.Tensor,**kwargs: Dict)),__init__((self,vocab_size: int,fusion_vocab_size: int,embed_dim: int))[CTOR,DUNDER],prepare_inputs_for_generation((self,input_ids,past_key_values=None,attention_mask=None,**kwargs,)),forward((self,input: torch.Tensor)),__init__((self,model_args: ModelArgs))[CTOR,DUNDER],setup_symm_mem((self,dtype: torch.dtype,device: torch.device)),forward((self,tokens: torch.Tensor,*,encoder_input: Optional[torch.Tensor] = None,encoder_mask: Optional[torch.Tensor] = None,))] model_args.py→[] model_config.py→[] model_converter.py→[__init__((self,job_config: JobConfig,parallel_dims: ParallelDims))[CTOR,DUNDER],convert((self,model: nn.Module)),post_optimizer_hook((self,model: Union[nn.Module,List[nn.Module]])),register_model_converter((converter_cls: type[ModelConverter],name: str)),__init__((self,job_config: JobConfig,parallel_dims: ParallelDims))[CTOR,DUNDER],convert((self,model: nn.Module)),post_optimizer_hook((self,model: Union[nn.Module,List[nn.Module]])),build_model_converters((job_config: JobConfig,parallel_dims: ParallelDims))[HOT]] model_parameters_getters.py→[get_wte_parameters_from_model((model: torch.nn.Module))→{get_tensors_dict},get_tensors_dict((model: torch.nn.Module,*,only_requires_grad: bool = False,sort_dict: bool = True,no_detach_and_clone: bool = False,))→{math::rope}] model_parameters_setters.py→[set_model_tensors_dict((model: torch.nn.Module,tensors_dict: OrderedDict[str,torch.Tensor],))→{_set_param_base,set_params_fsdp,metrics::log,module_utils::is_leaf_fsdp_module,module_utils::collect_fsdp_modules},set_params_fsdp((local_module_name: str,local_module: FullyShardedDataParallel,tensors_dict: OrderedDict[str,torch.Tensor],))→{set_params_with_state_dict,_set_param_base,metrics::log},set_params_with_state_dict((modules_buffer: dict[str,tuple[torch.nn.Module,str,torch.Tensor]],))→{_set_param_base},_set_param_base((remote_param: torch.Tensor,local_param: torch.nn.Parameter | torch.Tensor,))→{utils::match_shapes}] module_utils.py→[collect_fsdp_modules((module_name: str,module: torch.nn.Module,maps: tuple[dict[str,FullyShardedDataParallel],dict[str,torch.nn.Module]],))→{collect_fsdp_modules},is_leaf_fsdp_module((module_name: str,maps: tuple[dict[str,FullyShardedDataParallel],dict[str,torch.nn.Module]],))→{is_leaf_fsdp_module}] moe_kernels.py→[generate_permute_indices((tokens_per_expert_group: torch.Tensor,experts_per_rank: int,num_ranks: int,max_len: int,alignment: int,use_cpu: bool = False,))→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},simple_test(void)→{indices::generate_permute_indices,indices::generate_permute_indices},fill_indices_wrapper((tokens_per_expert_group: torch.Tensor,start_index_values: torch.Tensor,write_offsets: torch.Tensor,experts_per_rank: int,num_ranks: int,max_len: int,block_size: int = 128,max_blocks: int = 1024,# cap on total number of blocks to launch))→{integration_tests::main},fill_indices_cpu((tokens_per_expert_group: torch.Tensor,start_index_values: torch.Tensor,write_offsets: torch.Tensor,experts_per_rank: int,num_ranks: int,max_len: int,))→{integration_tests::main}] node_manager_app.py→[__init__((self,client_fn: ClientFnExt | None = None,# Only for backward compatibility mods: list[Mod] | None = None,))[CTOR,DUNDER]→{masks_utils::generate_full_mask,masks_utils::generate_full_mask,utils::get_random_model_states_from_config,utils::remove_shm_from_resource_tracker,lr_scheduler::step,optimizer_states_setters::_cast},create_and_start_workers((self))→{metrics::log,worker::start_worker,metrics::log,worker::create_new_worker,metrics::log,metrics::log},eval((self,eval_ins: TrainEvalInstructions,))→{metrics::log,comms_ray_batch_variants::load,utils::get_config_shm,utils::get_num_samples_shm,utils::get_eval_loss_shm,utils::set_dict_configrecord_shm},fit((self,fit_ins: TrainEvalInstructions,))→{comms_ray_batch_variants::load,worker::get_training_results_from_worker,utils::set_dict_configrecord_shm,utils::get_dict_configrecord_shm,port_utils::get_free_tcp_port},check_workers_health((self))→{worker::start_worker,worker::create_new_worker,utils::close_all_shms,metrics::log},create_parameters_shm((self))→{utils::is_shm_existing,utils::get_parameters_shm},close_workers((self))→{metrics::log},create_remote_up_down((self))] numerical_tests_example.py→[loss_fn((logits1,logits2))] optimizer_states_getters.py→[get_optimizer_states_base((model: torch.nn.Module,optimizer: torch.optim.Optimizer,fsdp_state_dict_type: str | None = None,*,fsdp_enabled: bool = False,))→{get_optimizer_states_simplified,runtime_estimator::init,optimizer_states_setters::_cast,math::rope},get_optimizer_states_simplified((model: torch.nn.Module,optimizer: torch.optim.Optimizer,fsdp_state_dict_type: str | None = None,*,fsdp_enabled: bool = False,))→{math::rope}] optimizer_states_setters.py→[set_optimizers_states((model: torch.nn.Module,optimizer: Optimizer,step: int,opt_state_dict: dict[str,dict[str,torch.Tensor]],))→{set_optimizer_state_base,set_optim_with_partial_state_dict,set_optimizer_state_fsdp,module_utils::is_leaf_fsdp_module,module_utils::collect_fsdp_modules},set_optim_with_partial_state_dict((model: torch.nn.Module,optimizer: Optimizer,momenta_buffer: dict[str,dict[str,torch.Tensor]],step_value: int,))→{_cast,_cast,lr_scheduler::step},_cast((param: torch.Tensor,value: torch.Tensor | dict | Iterable,param_id: int,param_groups: list[dict[Any,Any]],key: Hashable = None,))→{_cast,_cast,math::rope},set_optimizer_state_base((optimizer: torch.optim.Optimizer,local_param: torch.nn.Parameter | torch.Tensor,local_param_name: str,step: int,momentum_type_dict: dict[str,dict[str,torch.Tensor]],))→{utils::match_shapes,metrics::log},set_optimizer_state_fsdp((optim: torch.optim.Optimizer,local_module: FullyShardedDataParallel,local_module_name: str,momentum_dict: dict[str,dict[str,torch.Tensor]],step: int,))→{set_optimizer_state_base_fsdp},set_optimizer_state_base_fsdp((optim: torch.optim.Optimizer,local_names: tuple[str,str,str],buffers: tuple[ dict[str,torch.nn.Module],dict[str,torch.nn.Parameter | torch.Tensor],dict[str,dict[str,torch.Tensor]],],local_objects: tuple[FullyShardedDataParallel,torch.nn.Parameter | torch.Tensor],remote_objects: tuple[dict[str,dict[str,torch.Tensor]],int],))→{utils::match_shapes},_recurse_set_optimizer_state_dict((optimizer: Optimizer,state: defaultdict[torch.Tensor,dict[Any,Any]],)),_create_partial_id_map((remote_groups: list[dict[str,Any]],groups: list[dict[str,Any]],))] outgoing_message.py→[dispatch_train_and_eval_message_outgoing((message: Message,server_load_balancer: ServerLoadBalancer,message_type: str,))→{lr_scheduler::step,metrics::log,generic::get_client_id_from_train_eval_ins_recorddict},route_dispatch_outgoing((message: Message,server_load_balancer: ServerLoadBalancer,))→{dispatch_gather_message_outgoing,dispatch_train_and_eval_message_outgoing},dispatch_gather_message_outgoing((message: Message,server_load_balancer: ServerLoadBalancer,))→{lr_scheduler::step,metrics::log},handle_outgoing_message((outgoing_messages: deque,server_load_balancer: ServerLoadBalancer,incoming_message_ids: deque,fit_eval_callback: FitEvalCallback,))→{route_dispatch_outgoing}] parallel_dims.py→[] parallelize.py→[parallelize_vlm((model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,))→{parallelize::apply_ddp,parallelize::apply_fsdp,parallelize::apply_compile,parallelize::apply_compile,activation_checkpoint::apply_ac,activation_checkpoint::apply_ac},parallelize_qwen3((model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,))→{apply_moe_ep_tp,parallelize::apply_ddp,parallelize::apply_fsdp,parallelize::apply_compile,activation_checkpoint::apply_ac,parallelize::apply_non_moe_tp},parallelize_llama((model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,))→{apply_moe_ep_tp,parallelize::apply_ddp,parallelize::apply_fsdp,parallelize::apply_compile,activation_checkpoint::apply_ac,tensor_parallel::maybe_enable_async_tp},parallelize_llama((model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,))→{simple_fsdp::data_parallel,activation_checkpoint::apply_ac,tensor_parallel::maybe_enable_async_tp,parallelize::apply_tp},parallelize_flux((model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,))→{parallelize::apply_fsdp,activation_checkpoint::apply_ac},apply_moe_ep_tp((model: nn.Module,tp_mesh: DeviceMesh | None,ep_mesh: DeviceMesh | None,ep_tp_mesh: DeviceMesh | None,etp_enabled: bool,))→{expert_parallel::expert_parallel,expert_parallel::expert_parallel},apply_fsdp((model: nn.Module,dp_mesh: DeviceMesh,param_dtype: torch.dtype,reduce_dtype: torch.dtype,cpu_offload: bool = False,)),apply_ac((model: nn.Module,ac_config)),apply_fsdp((model: nn.Module,dp_mesh: DeviceMesh,param_dtype: torch.dtype,reduce_dtype: torch.dtype,pp_enabled: bool,cpu_offload: bool = False,reshard_after_forward_policy: str = "default",)),parallelize_encoders((t5_model: nn.Module,clip_model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,)),apply_non_moe_tp((model: nn.Module,tp_mesh: DeviceMesh,loss_parallel: bool,enable_float8_tensorwise_tp: bool,enable_async_tp: bool,)),apply_non_moe_tp((model: nn.Module,tp_mesh: DeviceMesh,loss_parallel: bool,enable_float8_tensorwise_tp: bool,)),apply_fsdp((model: nn.Module,dp_mesh: DeviceMesh,param_dtype: torch.dtype,reduce_dtype: torch.dtype,pp_enabled: bool,cpu_offload: bool = False,reshard_after_forward_policy: str = "default",ep_degree: int = 1,dp_mod_ep_mesh: DeviceMesh | None = None,gradient_divide_factor: int | None = None,)),apply_compile((model: nn.Module))] parallelize_deepseek.py→[parallelize_deepseek((# model: nn.Module,world_mesh: DeviceMesh,device: torch.device,model_args,rank: int,# parallel_dims: ParallelDims,# job_config: JobConfig,))→{runtime_estimator::init},get_group((dim_name: Optional[str] = None))] param_scheduler_dispatcher.py→[dispatch_model_state_scheduler((cfg: BaseConfig,))→{get_layer_filter_scheduler,get_freq_param_scheduler},get_freq_param_scheduler((kwargs: DictConfig | dict[str,int],)),get_layer_filter_scheduler((kwargs: DictConfig | dict[str,dict[str,int]],))] patched_mpt.py→[__init__((# noqa: PLR0915 self,config: MPTMuPConfig,cache_config: CacheConfig | None = None,quant_config: QuantizationConfig | None = None,prefix: str = "",))[CTOR,DUNDER]→{_get_alibi_slopes,math::attention,runtime_estimator::init,integration_tests::main},_get_alibi_slopes((total_num_heads: int,alibi_bias_max: int,)),__init__((self,mup_config: dict | None = None,*,use_peri_norm: bool = False,use_embedding_norm: bool = False,**kwargs: Any,# noqa: ANN401))[CTOR,DUNDER],forward((self,position_ids: torch.Tensor,hidden_states: torch.Tensor,)),__init__((self,config: MPTMuPConfig,quant_config: QuantizationConfig | None = None,))[CTOR,DUNDER],forward((self,x: torch.Tensor)),__init__((self,config: MPTMuPConfig,quant_config: QuantizationConfig | None = None,))[CTOR,DUNDER],forward((self,x: torch.Tensor)),_forward_simplified((self,x: torch.Tensor)),__init__((self,config: MPTMuPConfig,cache_config: CacheConfig | None = None,quant_config: QuantizationConfig | None = None,prefix: str = "",))[CTOR,DUNDER],forward((self,position_ids: torch.Tensor,hidden_states: torch.Tensor,)),__init__((self,*,vllm_config: VllmConfig,prefix: str = ""))[CTOR,DUNDER],get_input_embeddings((self,input_ids: torch.Tensor)),forward((# type: ignore[reportIncompatibleMethodOverride,override] self,input_ids: torch.Tensor,positions: torch.Tensor,intermediate_tensors: IntermediateTensors | None = None,inputs_embeds: torch.Tensor | None = None,)),compute_logits((self,hidden_states: torch.Tensor,sampling_metadata: SamplingMetadata,))[HOT],load_weights((self,weights: Iterable[tuple[str,torch.Tensor]]))] permute_indices_testing.py→[fill_indices_cpu((tokens_per_expert_group: torch.Tensor,start_index_values: torch.Tensor,write_offsets: torch.Tensor,experts_per_rank: int,num_ranks: int,max_len: int,))→{integration_tests::main},fill_indices_cpu((tokens_per_expert_group: torch.Tensor,start_index_values: torch.Tensor,write_offsets: torch.Tensor,experts_per_rank: int,num_ranks: int,max_len: int,))→{integration_tests::main},setUp((self)),setUp((self)),create_test_data((self,experts_per_rank: int,num_ranks: int,token_range: Tuple[int,int] = (1,16),alignment: int = 32,)),create_test_data((self,experts_per_rank: int,num_ranks: int,token_range: Tuple[int,int] = (1,16),alignment: int = 32,))] pickle_compat.py→[find_class((self,module: str,name: str)),custom_pickle_load((file_obj: Any))] pipeline_parallel.py→[build_pipeline_schedule((job_config: JobConfig,stages: list[PipelineStage],loss_fn: Callable))[HOT]→{loss::rescale_accumulated_loss},stage_ids_this_rank((pp_rank: int,pp_size: int,num_stages: int,style: str = "loop")),generate_llm_fqn_per_model_part((num_stages: int,num_layers: int,input_weight: int = 1,output_weight: int = 1,)),pipeline_module_split((whole_model: nn.Module,pp_mesh: DeviceMesh,pp_schedule: str,device: torch.device,module_names_per_stage: list[list[str]],))] plot_metrics.py→[main((parsed_args: Namespace))[ENTRY]→{metrics::log,metrics::log,metrics::log,runtime_estimator::init,math::rope,metrics::log},parse_args(void)] plotting_utils.py→[create_heatmap((similarity_matrix: torch.Tensor,expert_ids: list[int],step: int,opts: HeatmapOptions | None = None,))→{fig_to_wandb_image,integration_tests::main,integration_tests::main,lr_scheduler::step,lr_scheduler::step},create_tokens_kl_histograms((block_full_selection_history: torch.Tensor,expert_ids: list[int],step: int,max_entries: int = 10_000,))→{_plot_cos_sim_distance_histo,_get_list_of_probability_distribution_references},create_expert_heatmap((block_histograms: dict[int,torch.Tensor],expert_ids: list[int],step: int,))→{fig_to_wandb_image},create_thresholded_expert_scores_histogram((block_token_wise_threshold_histogram: torch.Tensor,step: int,))→{fig_to_wandb_image},create_expert_scores_histogram((block_scores_histograms: dict[int,tuple[torch.Tensor,torch.Tensor]],expert_ids: list[int],step: int,))→{fig_to_wandb_image},create_expert_tokens_scores_histogram((block_full_selection_history: torch.Tensor,expert_ids: list[int],step: int,))→{fig_to_wandb_image},_plot_l1_distance_histo((# pyright: ignore[reportUnusedFunction] input_tokenwise_distribution: torch.Tensor,target_experts_distribution: torch.Tensor,step: int,ref_name: str,))→{fig_to_wandb_image},_plot_kl_distance_histo((# pyright: ignore[reportUnusedFunction] input_tokenwise_distribution: torch.Tensor,target_experts_distribution: torch.Tensor,step: int,ref_name: str,))→{fig_to_wandb_image},_plot_cos_sim_distance_histo((input_tokenwise_distribution: torch.Tensor,target_experts_distribution: torch.Tensor,step: int,ref_name: str,))→{fig_to_wandb_image},fig_to_wandb_image((fig: Figure)),_get_list_of_probability_distribution_references((experts_ids: list[int],))] port_utils.py→[get_free_tcp_port((excluded_ports: list[int] | None = None))] profiling.py→[] qa_evaluation.py→[main(void)[ENTRY]→{get_preprocess_validation_examples_fn,get_preprocess_training_examples_fn,metrics::log,metrics::log,optimizer_states_setters::_cast,lr_scheduler::step},update((self,outputs: Mapping | torch.Tensor,labels: torch.Tensor,# noqa: ARG002))→{math::rope},__init__((self,tokenizer: PreTrainedTokenizerBase,*,dist_sync_on_step: bool = False,))[CTOR,DUNDER]→{comms_ray_batch_variants::load},update((self,outputs: Mapping | torch.Tensor,labels: torch.Tensor,# noqa: ARG002))→{math::rope},get_preprocess_training_examples_fn((tokenizer: PreTrainedTokenizerBase,max_length: int,stride: int,train_example_ids_map: dict[str,int],)),get_preprocess_validation_examples_fn((tokenizer: PreTrainedTokenizerBase,max_length: int,stride: int,validation_example_ids_map: dict[str,int],)),__init__((self,tokenizer: PreTrainedTokenizerBase))[CTOR,DUNDER],compute((self))[HOT],compute((self))[HOT]] qhadopt.py→[_multi_tensor_qhadopt((# noqa: PLR0915,PLR0912,PLR0917,PLR0913,C901 params: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | None,has_complex: bool,beta1: float,beta2: float,v1: float,lr: float | Tensor,clip_lambda: Callable[[Number | Tensor | Any],float] | None,weight_decay: float,decouple: bool,eps: float,maximize: bool,capturable: bool,differentiable: bool,))→{optimizer_states_setters::_cast,optimizer_states_setters::_cast,optimizer_states_setters::_cast,optimizer_states_setters::_cast,optimizer_states_setters::_cast},_default_clip_lambda((step: Number | Tensor))→{math::rope,runtime_estimator::init,runtime_estimator::init},_single_tensor_qhadopt((# noqa: PLR0917,PLR0913 params: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | None,decouple: bool,clip_lambda: Callable[[Number | Tensor | Any],float] | None,beta1: float,beta2: float,v1: float,lr: float | Tensor,weight_decay: float,eps: float,maximize: bool,capturable: bool,differentiable: bool,has_complex: bool,# noqa: ARG001)),_fused_qhadopt((# noqa: PLR0917,PLR0913 params: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | None,has_complex: bool,# Needed for consistency. beta1: float,beta2: float,v1: float,lr: float | Tensor,clip_lambda: Callable[[int],float] | None,weight_decay: float,decouple: bool,eps: float,maximize: bool,capturable: bool,# Needed for consistency. differentiable: bool,))] reference_utils.py→[compute_reference_backward((x,w,m_sizes,grad_output))[HOT]→{compute_reference_forward},compute_reference_forward((x,w,m_sizes))[HOT],analyze_tensor_differences((actual,expected,name))] regen_generated_to_helper.py→[main(void)[ENTRY]→{rewrite_file},rewrite_file((p: Path))] retokenize_dataset.py→[main((args: Namespace))[ENTRY]→{metrics::log,samples_generators::generate_samples_retokenized_streaming_text_dataset,utils::build_dataloader,runtime_estimator::init,lr_scheduler::step,metrics::log},parse_args(void)→{metrics::log,permute_indices_testing::setUp}] router_similarity_callback.py→[] runtime_estimator.py→[__init__((self,time_unit: str = "hours"))[CTOR,DUNDER],init((self,server_state: ServerState)),round_start((self,server_state: ServerState)),round_end((self,server_state: ServerState)),broadcast_start((self,server_state: ServerState)),broadcast_end((self,server_state: ServerState)),eval_fit_round_start((self,server_state: ServerState)),eval_fit_round_end((self,server_state: ServerState)),checkpoint_start((self,server_state: ServerState)),checkpoint_end((self,server_state: ServerState))] s3_checkpoint_loading.py→[upload_server_state((server_state: ServerState,remote_up_down: RemoteUploaderDownloader | None,local_checkpoint_path: str,))→{metrics::log,lr_scheduler::step,io::upload_file_to_s3,metrics::log,server_state::check_server_state_lightweight_dict,server_state::create_server_state_lightweight_dict},_prepare_model_parameters_paths((local_checkpoint_path: str,server_checkpoint_path: str,remote_up_down: RemoteUploaderDownloader | None,timeout: float = 0.5,))→{_check_existence_paths_for_parameters,_check_existence_paths_for_parameters,io::download_file_from_s3,metrics::log,lr_scheduler::step,lr_scheduler::step},get_model_parameters_checkpoint((cfg: BaseConfig,remote_up_down: RemoteUploaderDownloader | None,server_path: str,local_checkpoint_path: Path,timeout: float = 0.5,))→{_prepare_model_parameters_paths,metrics::log,sharding::load_chunked_model,metrics::log,sharding::load_model_states_from_file,lr_scheduler::step},get_server_state_checkpoint((cfg: BaseConfig,remote_up_down: RemoteUploaderDownloader | None,local_checkpoint_path: Path,))→{server_state::read_server_state_lightweight_dict,server_state::check_server_state_lightweight_dict,metrics::log,pickle_compat::custom_pickle_load,lr_scheduler::step,io::download_file_from_s3},get_server_checkpoint((cfg: BaseConfig,remote_up_down: RemoteUploaderDownloader | None,photon_save_path: str,timeout: float = 0.5,))→{get_server_momentum_checkpoint,get_server_momentum_checkpoint,get_server_state_checkpoint,get_model_parameters_checkpoint,dispatcher::dispatch_strategy_state_keys,param_scheduler_dispatcher::dispatch_model_state_scheduler},copy_old_checkpoints_to_new_run((*,remote_up_down: RemoteUploaderDownloader | None,photon_save_path: str,bucket_uri: str,restore: RestoreRunData,))→{_perform_copy_operations,_expand_and_collect_paths,_validate_client_count,_collect_latest_client_checkpoints,_gather_server_roots,_validate_local_run_folders},_perform_copy_operations((*,all_paths: list[str],is_remote: bool,restore_run_uuid: str,run_uuid: str,remote_up_down: RemoteUploaderDownloader | None,))→{metrics::log,metrics::log,s3_helper::key_exists,optimizer_states_setters::_cast,optimizer_states_setters::_cast,metrics::log},cleanup_checkpoints((server_state: ServerState,run_uuid: str | None,end_idx: int | None = -1,*,local_cleanup: bool = False,remote_cleanup: bool = False,))→{s3_helper::delete_rounds,s3_helper::delete_clients_checkpoints,s3_helper::delete_rounds,s3_helper::delete_clients_checkpoints},interpret_resume_round((resume_round: int | None,run_uuid_path: str,state_keys: tuple[str,...],*,raise_error: bool = True,))→{metrics::log,s3_helper::obtain_sorted_runs,metrics::log},upload_server_checkpoint_model_states((server_state: ServerState))→{s3_helper::generic_model_state_upload_to_s3,s3_helper::generic_model_state_upload_to_s3,s3_helper::generic_model_state_upload_to_s3},_collect_latest_client_checkpoints((*,old_run_folder: str,restore_run_step: int,))→{runtime_estimator::init,runtime_estimator::init,s3_helper::list_objects},get_checkpoint_files_for_path((checkpoint_path: str))→{metrics::log,s3_single_file::get_bucket_and_key_from_s3_uri,s3_helper::list_objects},_check_existence_paths_for_parameters((base_path_to_check: str,check_fn: Callable[[str],bool],))→{metrics::log,metrics::log},_validate_backend((remote_up_down: RemoteUploaderDownloader | None))→{metrics::log,metrics::log},_validate_local_run_folders((*,old_run_folder: str,new_run_folder: str))→{s3_helper::list_objects,s3_helper::list_objects},_gather_server_roots((*,old_run_folder: str,restore_run_round: int | None,))→{metrics::log,s3_helper::list_objects},upload_server_checkpoint_server_state((server_state: ServerState))→{upload_server_state},delete_past_communication_states((bucket_name: str,prefix: str))→{s3_helper::delete_object},get_server_momentum_checkpoint((# noqa: C901 cfg: BaseConfig,remote_up_down: RemoteUploaderDownloader | None,server_path: str,local_checkpoint_path: Path,momentum_type: str,)),_compute_remote_run_folders((*,bucket_uri: str,run_uuid: str,restore_run_uuid: str,))[HOT],_compute_local_run_folders((*,photon_save_path: str,run_uuid: str,restore_run_uuid: str,))[HOT],_validate_remote_run_folders((*,old_run_folder: str,new_run_folder: str)),_validate_client_count((client_roots: list[str],n_total_clients: int | None,old_run_folder: str,)),_expand_and_collect_paths((*,server_roots: list[str],client_roots: list[str],))] s3_helper.py→[generic_vector_upload_to_s3((# noqa: PLR0917,PLR0913 vector: NDArrays,layer_names: list[str],current_round: int,remote_up_down: RemoteUploaderDownloader | None,local_checkpoint_path: str,model_type: str,*,parameter_upload: bool = True,is_second_momentum: bool = False,max_shard_size: str = "5GB",))→{lr_scheduler::step,io::upload_file_to_s3,sharding::_walk_load_directory,metrics::log,metrics::log,sharding::chunk_and_save_file},get_file_from_path((input_file_path: str,run_uuid: str,s3_comm_config: S3CommConfig,output_parent_path: str,relative_header: str = "",))→{metrics::log,lr_scheduler::step,io::download_file_from_s3,io::create_remote_up_down,metrics::log},delete_clients_checkpoints((run_uuid_path: str,end_idx: int | None = -1))→{delete_object,delete_object,list_objects,list_objects,runtime_estimator::init},extract_s3_comm_config_from_configrecord((s3_comm_config: ConfigRecord,))→{lr_scheduler::step,lr_scheduler::step,lr_scheduler::step},obtain_sorted_runs((run_uuid_path: str,state_keys: tuple[str,...],*,catch_all: bool = True,))→{list_objects,permute_indices_testing::setUp,runtime_estimator::init},delete_rounds((run_uuid_path: str,state_keys: tuple[str,...],end_idx: int | None = -1,*,catch_all: bool = True,))→{delete_object,list_objects,obtain_sorted_runs},list_objects((run_uuid_path: str,))→{lr_scheduler::step},delete_object((object_path: str,))→{delete_remote_object},get_num_batches_from_checkpoint_name((checkpoint_name: str))→{runtime_estimator::init},key_exists((client: BaseClient,bucket: str,key: str)),delete_remote_object((object_name: str)),generic_model_state_upload_to_s3((# noqa: PLR0913 model_states: NDArrays,layer_names_and_types: tuple[tuple[str,ModelStateNames],...],current_round: int,remote_up_down: RemoteUploaderDownloader | None,local_checkpoint_path: str,*,parameter_upload: bool = True,is_second_momentum: bool = False,max_shard_size: str = "5GB",))] s3_model_loading.py→[load_pretrained_model_from_path((pretrained_model_path: str,run_uuid: str,s3_comm_config: S3CommConfig,trainer: Trainer,))→{utils::create_tensors_dict,composer_trainer_setters::set_trainer_tensors_dict,utils::split_experts_layers_tensors,utils::get_n_experts_from_model,model_parameters_getters::get_tensors_dict,sharding::load_model_states_from_file}] s3_single_file.py→[download_single_file((remote_path: str,local_tmp_dir: str))→{get_bucket_and_key_from_s3_uri,metrics::log,lr_scheduler::step},get_bucket_and_key_from_s3_uri((s3_uri: str))] samples_generators.py→[generate_samples_retokenized_streaming_text_dataset((loader: DataLoader,tokenizer_couple: TokenizersCouple,max_length: int,truncate_num_samples: int | None = None,*,no_wrap: bool,))→{lr_scheduler::step,lr_scheduler::step,optimizer_states_setters::_cast},stream_and_untokenize((loader: DataLoader,tokenizer: PreTrainedTokenizerBase,truncate_num_batches: int | None = None,))→{math::rope},generate_samples_from_dataloader((loader: DataLoader,truncate_num_samples: int | None = None,))] sampling.py→[denoise((device: torch.device,dtype: torch.dtype,model: FluxModel,img_width: int,img_height: int,denoising_steps: int,clip_encodings: torch.Tensor,t5_encodings: torch.Tensor,enable_classifier_free_guidance: bool = False,empty_t5_encodings: torch.Tensor | None = None,empty_clip_encodings: torch.Tensor | None = None,classifier_free_guidance_scale: float | None = None,))→{get_schedule,utils::unpack_latents,utils::pack_latents,utils::create_position_encoding_for_latents,utils::generate_noise_latent},generate_image((device: torch.device,dtype: torch.dtype,job_config: JobConfig,model: FluxModel,prompt: str | list[str],autoencoder: AutoEncoder,t5_tokenizer: BaseTokenizer,clip_tokenizer: BaseTokenizer,t5_encoder: FluxEmbedder,clip_encoder: FluxEmbedder,))→{denoise,utils::preprocess_data,utils::preprocess_data},get_schedule((num_steps: int,image_seq_len: int,base_shift: float = 0.5,max_shift: float = 1.15,shift: bool = True,))→{time_shift,get_lin_function},time_shift((mu: float,sigma: float,t: Tensor)),get_lin_function((x1: float = 256,y1: float = 0.5,x2: float = 4096,y2: float = 1.15)),save_image((name: str,output_dir: str,x: torch.Tensor,add_sampling_metadata: bool,prompt: str,))] sc_evaluation.py→[main(void)[ENTRY]→{get_tokenization_fn,metrics::log,metrics::log,optimizer_states_setters::_cast,lr_scheduler::step,s3_model_loading::load_pretrained_model_from_path},get_tokenization_fn((tokenizer: PreTrainedTokenizerBase,max_length: int,))] server_app.py→[federated_training_loop((# noqa: C901,PLR0915 cfg: BaseConfig,server_state: ServerState,runtime_estimator: RuntimeEstimator,grid: Grid,))→{metrics::log,s3_checkpoint_loading::cleanup_checkpoints,metrics::log,metrics::log,s3_checkpoint_loading::upload_server_checkpoint_model_states,s3_checkpoint_loading::upload_server_checkpoint_server_state}] server_state.py→[read_server_state_lightweight_dict((server_state_dict: dict[str,Any],n_total_clients: int,n_local_steps: int,))→{metrics::log,integration_tests::main,runtime_estimator::init,metrics::log,runtime_estimator::init},check_server_state_lightweight_dict((server_state_dict: dict[str,Any],*,raise_if_not_lightweight: bool = False,))→{metrics::log,metrics::log},create_server_state_lightweight_dict((server_state: ServerState,))→{lr_scheduler::step}] server_state_tool.py→[main((argv: list[str] | None = None))[ENTRY]→{build_arg_parser,reset_clients,_handle_edit,set_client_steps,_handle_edit,_coerce_client_id},_backup_if_exists((url: str))→{_parse_s3_url,_parse_s3_url,_parse_s3_url,_parse_s3_url,_s3_put_object_bytes,_s3_get_object_bytes},save_state((state: dict[str,Any],*,path: str | Path,inplace: bool,backup: bool,))→{_parse_s3_url,_s3_put_object_bytes,_backup_if_exists,optimizer_states_setters::_cast,lr_scheduler::step,lr_scheduler::step},build_arg_parser(void)[HOT]→{_add_common_write_args,_add_common_write_args,_add_common_write_args,_add_common_write_args,_add_common_write_args,_add_common_write_args},load_state((path: str | Path))→{_parse_s3_url,_s3_get_object_bytes,_ensure_s3_exists,optimizer_states_setters::_cast,lr_scheduler::step},export_json((state: dict[str,Any],out_path: str | Path,*,include_history: bool = False,))→{_parse_s3_url,_s3_put_object_bytes,optimizer_states_setters::_cast,lr_scheduler::step,math::rope},_handle_upgrade((args: argparse.Namespace))→{save_state,load_state,photon_log,upgrade_state},_handle_show((args: argparse.Namespace))→{load_state,export_json,photon_log,summarize_state},_handle_export_json((args: argparse.Namespace))→{load_state,photon_log,export_json,lr_scheduler::step},_handle_edit((args: argparse.Namespace,editor: Callable[[dict[str,Any]],None],))→{save_state,load_state,photon_log,lr_scheduler::step},set_client_steps((state: dict[str,Any],client_id: str | int,steps: int))→{put_client_states_dict,ensure_client_states_dict,runtime_estimator::init},reset_clients((state: dict[str,Any],steps: int))→{put_client_states_dict,ensure_client_states_dict,runtime_estimator::init},_handle_validate((args: argparse.Namespace))→{load_state,photon_log,server_state::check_server_state_lightweight_dict},summarize_state((state: dict[str,Any]))→{ensure_client_states_dict,math::rope},bump_server_round((state: dict[str,Any],delta: int))→{runtime_estimator::init,runtime_estimator::init},put_client_states_dict((state: dict[str,Any],client_states: dict[str | int,dict[str,Any]],))→{lr_scheduler::step},set_server_round((state: dict[str,Any],new_round: int))→{runtime_estimator::init},set_time_offset((state: dict[str,Any],new_offset: float))→{comms_ray_batch_variants::load},set_server_steps((state: dict[str,Any],new_steps: int))→{runtime_estimator::init},_s3_client(void)→{optimizer_states_setters::_cast},_ensure_s3_exists((bucket: str,key: str))→{_s3_client},_s3_get_object_bytes((bucket: str,key: str))→{_s3_client},_s3_put_object_bytes((bucket: str,key: str,data: bytes))→{_s3_client},_coerce_client_id((cid: str))→{runtime_estimator::init},photon_log((level: int,msg: str,*args: object)),ensure_client_states_dict((state: dict[str,Any])),_add_common_write_args((p: argparse.ArgumentParser)),upgrade_state((state: dict[str,Any])),_parse_s3_url((url: str))] sharding.py→[load_chunked_model((load_directory: Path))→{load_model_states_from_file,_walk_load_directory,metrics::log},chunk_and_save_file((layer_names: list[str],model_arrays: NDArrays,# type: ignore[PGH003] save_directory: Path,max_shard_size: str = DEFAULT_MAX_SHARD_SIZE,))→{dump_model_parameters_to_file},load_model_states_from_file((file_path: Path)),dump_model_parameters_to_file((file_path: Path,model_parameters: NDArrays | dict[str,torch.Tensor],)),_walk_load_directory((directory: Path))] shared_utils.py→[determine_layers_for_statistics((delta_mode: DeltaModes,aggregation_layer_names: list[str],transmission_layer_names: list[str],))→{permute_indices_testing::setUp,permute_indices_testing::setUp,metrics::log,metrics::log,metrics::log,metrics::log},calculate_l2_norms_per_state_and_mode((aggregation_state: tuple[NDArrays,list[str]],transmission_state: tuple[NDArrays,list[str]],delta_modes: list[DeltaModes],train_metrics: dict[str,Scalar],metric_keys: tuple[str,str,str],))→{extract_l2_norm_for_model_state,report_l2_norms,determine_layers_for_statistics,permute_indices_testing::setUp,utils::sum_of_squares_per_layer,utils::sum_of_squares_per_layer},report_l2_norms((# noqa: PLR0913,PLR0917 layer_names: list[str],layer_deltas: dict[str,float],transmission_state_sum_of_squares: dict[str,float],aggregation_state_sum_of_squares: dict[str,float],train_metrics: dict[str,Scalar],metric_keys: tuple[str,str,str],))→{comms_ray_batch_variants::load,comms_ray_batch_variants::load,comms_ray_batch_variants::load},get_ndarrays_and_names_from_payload((payload: NDArrays,layer_names: list[str],layer_types: list[ModelStateNames],model_state_type: ModelStateNames,))→{utils::compress_with_strict,utils::compress_with_strict},extract_l2_norm_for_model_state((transmission_state: tuple[NDArrays,list[str]],aggregation_state: tuple[NDArrays,list[str]],layers_for_calculation: set[str],train_metrics: dict[str,Scalar],pseudo_gradient: str,))→{metrics::log},filter_expert_parameters((mask_transmission: tuple[bool,...],layer_names_transmission: list[str],layer_types_transmission: list[ModelStateNames],experts_ids: list[int],*,change_names: bool = True,))→{process_sigma_moe_expert},process_sigma_moe_expert((layer_name: str,expert_id_mapping: dict[int,int],))→{runtime_estimator::init},remap_expert_parameters_to_original_indices((layer_names: list[str],experts_ids: list[int],))→{remap_sigma_moe_expert_to_original},remap_sigma_moe_expert_to_original((layer_name: str,new_to_original_mapping: dict[int,int],))→{runtime_estimator::init},modify_aggregation_mask_with_frozen_layers((client_config: FitConfig,)),pass_expert_ids_to_callbacks((trainer: Trainer,experts_ids: list[int],))] siglip2.py→[resize_positional_embeddings((pos_embs_HWD: torch.Tensor,spatial_shapes_N2: torch.Tensor,max_length: int,))→{runtime_estimator::init},forward((self,pixels_NLD: torch.Tensor,grid_hw: torch.Tensor))→{resize_positional_embeddings},__init__((self,args: Siglip2ModelArgs))[CTOR,DUNDER]→{attention::build_attention},__init__((self,args: Siglip2ModelArgs))[CTOR,DUNDER]→{math::attention},__init__((self,args: Siglip2ModelArgs))[CTOR,DUNDER]→{lr_scheduler::step},forward((self,pixel_values_NLD: torch.FloatTensor,pixel_masks_NL: torch.BoolTensor,grid_hw: torch.LongTensor,))→{attention::init_attention_mask},__init__((self,args: Siglip2ModelArgs))[CTOR,DUNDER],init_weights((self)),forward((self,x: torch.Tensor)),init_weights((self)),__init__((self,args: Siglip2ModelArgs))[CTOR,DUNDER],forward((self,x: torch.Tensor)),init_weights((self)),forward((self,x: torch.Tensor)),init_weights((self)),init_weights((self))] sigma_moe_expert_utils.py→[recombine_ffn_experts((split_params_dict: dict[str,torch.Tensor],n_experts: int,))→{runtime_estimator::init},verify_split_recombine_consistency((original_params: dict[str,torch.Tensor],split_params: dict[str,torch.Tensor],n_experts: int,atol: float = 1e-6,))→{recombine_ffn_experts},split_ffn_experts((sigma_moe_params_dict: dict[str,torch.Tensor],n_experts: int,)),get_expert_parameters((split_params_dict: dict[str,torch.Tensor],expert_idx: int,))] simpleMoE.py→[measure_performance((model: nn.Module,batch_size: int,seq_len: int,vocab_size: int,num_batches: int,device: str,))→{generate_sample_data,generate_sample_data,generate_sample_data},forward_mg_gemm((self,x: torch.Tensor))→{mg_grouped_gemm::grouped_gemm_forward,mg_grouped_gemm::grouped_gemm_forward},train_epoch((model: nn.Module,optimizer: torch.optim.Optimizer,batch_size: int,seq_len: int,vocab_size: int,num_batches: int,device: str,load_balance_coef: float = 0.01,))→{compute_load_balancing_loss,generate_sample_data},compare_methods((args))→{measure_performance,measure_performance},train_model((args))→{evaluate,train_epoch},evaluate((model: nn.Module,batch_size: int,seq_len: int,vocab_size: int,num_batches: int,device: str,))→{generate_sample_data},__init__((self,input_dim: int,num_experts: int,top_k: int = 2))[CTOR,DUNDER],forward((self,x: torch.Tensor)),__init__((self,input_dim: int,hidden_dim: int,output_dim: int))[CTOR,DUNDER],forward((self,x: torch.Tensor)),__init__((self,input_dim: int,hidden_dim: int,output_dim: int,num_experts: int,top_k: int = 2,use_mg_gemm: bool = False,))[CTOR,DUNDER],forward_manual_loop((self,x: torch.Tensor)),forward((self,x: torch.Tensor)),__init__((self,vocab_size: int,embed_dim: int,hidden_dim: int,num_experts: int,top_k: int = 2,use_mg_gemm: bool = False,))[CTOR,DUNDER],forward((self,x: torch.Tensor)),compute_load_balancing_loss((router_logits: torch.Tensor,num_experts: int))[HOT],generate_sample_data((batch_size: int,seq_len: int,vocab_size: int,device: str = "cuda"))] simple_fsdp.py→[_register_parametrization((module: nn.Module,param_names: List[str],parametrization: nn.Module))→{math::rope},data_parallel((model,device_mesh,mode="replicate",ac_mode: str = "none",mp_policy: Optional[MixedPrecisionPolicy] = None,))→{_register_parametrization},_distribute_dtensor((tensor: DTensor,device_mesh: DeviceMesh,placements: Sequence[Placement],)),fsdp_policy(void),__init__((self,device_mesh,param_sharding,mode,regional_ac,mp_policy,))[CTOR,DUNDER],replicate_compute((self,x))[HOT],forward((self,x))] smollm_corpus.py→[] state_dict_adapter.py→[__init__((self,model_args: BaseModelArgs,hf_assets_path: str | None,))[CTOR,DUNDER]→{runtime_estimator::init,math::rope},__init__((self,model_args: FluxModelArgs,hf_assets_path: str | None))[CTOR,DUNDER]→{math::rope},to_hf((self,state_dict: dict[str,Any]))→{runtime_estimator::init},__init__((self,model_args: TransformerModelArgs,hf_assets_path: str | None))[CTOR,DUNDER],__init__((self,model_args: Qwen3ModelArgs,hf_assets_path: str | None))[CTOR,DUNDER],to_hf((self,state_dict: dict[str,Any])),to_hf((self,state_dict: dict[str,Any])),from_hf((self,hf_state_dict: dict[str,Any])),from_hf((self,hf_state_dict: dict[str,Any])),_swap_scale_shift((self,weight)),from_hf((self,hf_state_dict: dict[str,Any]))] stream_partitioner.py→[partition_streams((input_file: Path,output_file: Path,num_clients: int))] switch_head_expert_utils.py→[recombine_switch_head_experts((split_params_dict: dict[str,torch.Tensor],n_experts: int,n_heads: int,))→{runtime_estimator::init,runtime_estimator::init},verify_split_recombine_consistency((original_params: dict[str,torch.Tensor],split_params: dict[str,torch.Tensor],n_experts: int,n_heads: int,atol: float = 1e-6,))→{recombine_switch_head_experts},split_switch_head_experts((switch_head_params_dict: dict[str,torch.Tensor],n_experts: int,n_heads: int,)),get_expert_head_parameters((split_params_dict: dict[str,torch.Tensor],expert_idx: int,head_idx: int,))] tasks.py→[mmlu_pro_mc_prompt((line,task_name: str = None))→{comms_ray_batch_variants::load},mmlu_cloze_prompt((line,task_name: str = None))→{runtime_estimator::init},prompt_hellaswag((line,task_name: str = None)),prompt_commonsense_qa((line,task_name: str = None)),bbh_prompt((line,task_name: str = None)),prompt_math((line,task_name: str = None)),__init__((self,name,prompt_function=None,hf_repo="lighteval/mmlu",hf_subset=None,# metric=[Metrics.loglikelihood_acc_single_token],metric=[Metrics.loglikelihood_acc,Metrics.loglikelihood_acc_norm_nospace],hf_avail_splits=None,evaluation_splits=["test"],few_shots_split="dev",few_shots_select=None,suite=["custom"],generation_size=-1,stop_sequence=None,output_regex=None,frozen=False,))[CTOR,DUNDER]] tensor_parallel.py→[maybe_enable_async_tp((job_config: JobConfig,tp_mesh: DeviceMesh))] test_create_m_indices.py→[] test_generate.py→[apply_tp_minus_sp((model: nn.Module,tp_mesh: DeviceMesh))] the_pile.py→[] tiktoken.py→[encode((self,s: str,*,bos: bool,eos: bool,allowed_special: Optional[Union[Literal["all"],AbstractSet[str]]] = None,disallowed_special: Optional[Union[Literal["all"],Collection[str]]] = None,))→{permute_indices_testing::setUp,math::rope},decode((self,t: Sequence[int]))→{optimizer_states_setters::_cast},encode_multimodal((self,sample: Mapping[str,Any]))→{permute_indices_testing::setUp},__init__((self,model_path: str))[CTOR,DUNDER],build_tiktoken_tokenizer((job_config: JobConfig))[HOT]] tma_autotuning.py→[__init__((self,tma_size: int = 128))[CTOR,DUNDER],init_tma_descriptor((self,name: str)),fill_1d_tma_descriptor((self,name: str,ptr: int,dim: int,block_dim: int,element_size: int)),fill_2d_tma_descriptor((self,name: str,ptr: int,dim1: int,dim0: int,block_dim1: int,block_dim0: int,element_size: int,)),get_tma_descriptor_kernel_param((self,name: str)),early_config_prune((configs,named_args,dtsize=None,dtype=None,**kwargs))] tma_cuda_autotune.py→[early_config_prune((configs,args,**kwargs))→{comms_ray_batch_variants::load,integration_tests::main},__init__((self,tma_size: int = 128))[CTOR,DUNDER],init_tma_descriptor((self,name: str)),fill_1d_tma_descriptor((self,name: str,ptr: int,dim: int,block_dim: int,element_size: int)),fill_2d_tma_descriptor((self,name: str,ptr: int,dim1: int,dim0: int,block_dim1: int,block_dim0: int,element_size: int,)),get_tma_descriptor_kernel_param((self,name: str))] tokenizer.py→[__init__((self,model_path: str = "t5-small",max_length: int = 77,**hf_kwargs))[CTOR,DUNDER],_pad_and_chunk_tokens((self,tokens: List[int],max_length: int,pad_token: int)),get_vocab_size((self)),encode((self,text: str | list[str])),decode((self,t: List[int])),__init__((self,model_path: str = "t5-small",max_length: int = 77,**hf_kwargs))[CTOR,DUNDER],get_vocab_size((self)),encode((self,s: str | list[str],)),decode((self,t: List[int])),build_flux_tokenizer((job_config: JobConfig))[HOT]] train.py→[forward_backward_step((self,input_dict: dict[str,torch.Tensor],labels: torch.Tensor))→{utils::unpack_latents,utils::pack_latents,utils::create_position_encoding_for_latents,utils::preprocess_data},__init__((self,job_config: JobConfig))[CTOR,DUNDER]→{parallelize::parallelize_encoders,autoencoder::load_ae},batch_generator((self,data_iterable: Iterable[tuple[dict[str,torch.Tensor],torch.Tensor]]))→{lr_scheduler::step},forward_backward_step((self,input_dict: dict[str,torch.Tensor],labels: torch.Tensor))→{attention::init_attention_mask},train_step((self,data_iterator: Iterable[tuple[dict[str,torch.Tensor],torch.Tensor]])),should_continue_training((self)),state_dict((self)),load_state_dict((self,state_dict: dict[str,Any])),close((self))] train_ds_dev.py→[run_full_model((mesh: DeviceMesh,))→{numerical_tests_example::loss_fn}] train_ds_real.py→[run_full_model((config: JobConfig,))→{next_batch,numerical_tests_example::loss_fn,lr_scheduler::step,comms_ray_batch_variants::load,lr_scheduler::build_lr_schedulers,optimizer::build_optimizers},cross_entropy_loss((pred: torch.Tensor,labels: torch.Tensor)),next_batch((data_iterator: Iterable,metrics_processor))] train_spec.py→[get_train_spec((name: str))→{_transform_train_spec,_transform_train_spec},_transform_train_spec((original_spec: TrainSpec)),register_train_spec((train_spec: ForgeTrainSpec)),register_train_spec((train_spec: TrainSpec)),get_train_spec((name: str))] train_tokenizer_from_tokenized.py→[main((args: Namespace))[ENTRY]→{create_streaming_text_dataloader,elaborate_stream_yamls,metrics::log,metrics::log,samples_generators::stream_and_untokenize,metrics::log},parse_args(void)→{metrics::log,permute_indices_testing::setUp,permute_indices_testing::setUp},elaborate_stream_yamls((streams_config_file: str,dataset_config_file: str,split: str,max_length: int,split_temp_dir: TemporaryDirectory,))→{lr_scheduler::step,metrics::log},create_streaming_text_dataloader((streams: Sequence[Stream],decode_tokenizer: PreTrainedTokenizerBase,max_length: int,num_workers: int,batch_size: int = 512,))→{utils::build_dataloader}] trainer_utils.py→[get_trainer_mutables_from_config((# noqa: PLR0914,C901,PLR0913,PLR0917,PLR0915,PLR0912 trainer: Trainer,train_cfg: TrainConfig,client_config: FitConfig | EvaluateConfig,icl_tasks_config_dict: dict[str,Any] | None,device: DeviceGPU | DeviceCPU | None,logged_cfg: dict[str,Any],*,no_data_loading: bool = False,))→{add_unigram_metrics,utils::split_experts_layers_tensors,utils::get_n_experts_from_model,model_parameters_getters::get_tensors_dict,llm_config_functions::set_icl_tasks_root_dir,metrics::log},set_mutables_trainer_callbacks_and_loggers((# noqa: C901,PLR0912,PLR0913,PLR0914,PLR0915,PLR0917 trainer: Trainer,callbacks: Callback | Sequence[Callback] | None,loggers: LoggerDestination | Sequence[LoggerDestination] | None,train_cfg: TrainConfig,profiler: Profiler | None = None,console_stream: str | TextIO = "stderr",save_latest_filename: str | None = None,save_filename: str = "ep{epoch}-ba{batch}-rank{rank}.pt",*,log_traces: bool = False,))→{lr_scheduler::step,lr_scheduler::step,metrics::log,lr_scheduler::step,metrics::log,lr_scheduler::step},get_trainer_object((# noqa: PLR0914,C901,PLR0913,PLR0915,PLR0912 cfg: DictConfig,client_config: FitConfig | EvaluateConfig | CentralizedConfig,log_name: str | None = None,*,force_cpu: bool = False,no_data_loading: bool = False,split_eval: bool = False,))→{add_unigram_metrics,build_optimizer::build_optimizer,permute_indices_testing::setUp,permute_indices_testing::setUp,permute_indices_testing::setUp,permute_indices_testing::setUp},add_unigram_metrics((# TODO(<Lorenzo>): Since the client_config contains the CID # we can remove it from all functions which have access to client_config client_config: FitConfig | EvaluateConfig | CentralizedConfig,train_cfg: TrainConfig,streams: dict[str,dict[str,Any]],))→{unigram_normalized_metrics::create_wrapped_subclass,metrics::log,utils::get_unigram_probabilities_tensor,llm_config_functions::get_stream_freq_dict_for_client,optimizer_states_setters::_cast},set_mutables_trainer_eval_dataloader((trainer: Trainer,eval_dataloader: Iterable | DataSpec | Evaluator | Sequence[Evaluator] | None,train_cfg: TrainConfig,))→{math::rope,lr_scheduler::step,lr_scheduler::step},set_mutables_trainer((trainer: Trainer,trainer_mutable_attributes: TrainerMutableAttributes,client_config: FitConfig | EvaluateConfig | CentralizedConfig,))→{set_mutables_trainer_eval_dataloader,set_mutables_trainer_train_dataloader,set_mutables_trainer_callbacks_and_loggers},trainer_clean_up((trainer: Trainer,))→{metrics::log},set_mutables_trainer_train_dataloader((# noqa: PLR0913 trainer: Trainer,train_dataloader: DataSpec | None,client_config: FitConfig | EvaluateConfig | CentralizedConfig,train_dataloader_label: str = "train",train_subset_num_batches: int = -1,*,spin_dataloaders: bool = True,))→{lr_scheduler::step},correct_time_before_fit((trainer: Trainer,client_state_struct: ClientState,server_steps_cumulative: int | None,fit_config: FitConfig,)),load_trainer_checkpoint((trainer: Trainer,train_cfg: TrainConfig,))] transform.py→[__call__((self,image: torch.Tensor))[DUNDER]→{utils::tile_crop,utils::resize_with_pad,utils::get_canvas_best_fit},__init__((self,*,image_mean: Optional[List[float]] = None,image_std: Optional[List[float]] = None,possible_resolutions: Optional[List[Tuple[int,int]]] = None,tile_size: int = 224,max_num_tiles: Optional[int] = 4,dtype: torch.dtype = torch.bfloat16,resample: str = "bilinear",resize_to_max_canvas: bool = False,))[CTOR,DUNDER]→{utils::find_supported_resolutions}] triton_barrier.py→[] triton_on_device_all_to_all_v.py→[_on_device_all_to_all_v((output: torch.Tensor,output_splits: torch.Tensor,input: torch.Tensor,input_splits: torch.Tensor,group: dist.ProcessGroup = dist.group.WORLD,BLOCKS_PER_REMOTE_RANK=8,UNROLL_FACTOR: int = 8,BLOCK_SIZE: int = 16384,))] triton_utils.py→[] unigram_normalized_metrics.py→[update((self,output: Mapping | Tensor,target: Tensor))→{math::rope},__init__((self,unigram_probabilities: Tensor,ignore_index: int = -100,*,dist_sync_on_step: bool = False,))[CTOR,DUNDER],update((self,output: Mapping | Tensor,target: Tensor)),compute((self))[HOT],compute((self))[HOT],__init__((self,unigram_probabilities: Tensor,ignore_index: int = -100,*,dist_sync_on_step: bool = False,))[CTOR,DUNDER],compute((self))[HOT],compute((self))[HOT],create_wrapped_subclass((base_class: type,**kwargs: Any))] unit_test_backwards.py→[_run_grouped_gemm_backward_test((self,shape: Tuple[int,int,int,int],device: torch.device,dtype: torch.dtype = torch.bfloat16,atol: float = 1e-5,rtol: float = 1.6e-2,))→{reference_utils::analyze_tensor_differences,reference_utils::analyze_tensor_differences,reference_utils::compute_reference_backward,mg_grouped_gemm::grouped_gemm_backward,reference_utils::analyze_tensor_differences,reference_utils::compute_reference_forward},setUp((self))] unit_test_cg.py→[benchmark_forward((self,M_total,K,N,num_experts,group_size_m,num_runs=10))→{integration_tests::main,cg_forward::cg_grouped_gemm_forward,cg_forward::cg_grouped_gemm_forward},benchmark_backward((self,M_total,K,N,num_experts,group_size_m,num_runs=5))→{integration_tests::main,cg_backward::cg_grouped_gemm,cg_backward::cg_grouped_gemm},verify_forward((self,M_total,K,N,num_experts,group_size_m,print_stats=False))→{integration_tests::main,cg_forward::cg_grouped_gemm_forward},verify_backward((self,M_total,K,N,num_experts,group_size_m,print_stats=False))→{integration_tests::main,cg_backward::cg_grouped_gemm},run_tests((run_benchmarks=False))] unit_test_forwards.py→[_run_grouped_gemm_test((self,shape: Tuple[int,int,int,int],device: torch.device,dtype: torch.dtype = torch.bfloat16,atol: float = 1e-5,rtol: float = 1.6e-2,))→{mg_grouped_gemm::grouped_gemm_forward},setUp((self))] utils.py→[post_process_client_result((# noqa: PLR0913,PLR0917 train_metrics: dict[str,Scalar],client_state_struct: ClientState,llm_config: DictConfig,trainer: Trainer,payload: NDArrays,fit_config: FitConfig,))→{runtime_estimator::init,metrics::log,shared_utils::filter_expert_parameters,lr_scheduler::step,composer_trainer_getters::update_client_state,composer_trainer_getters::calculate_trainer_post_training_l2_norms},resize_with_pad((image: torch.Tensor,target_size: Tuple[int,int],resample: torchvision.transforms.InterpolationMode,max_size: Optional[int] = None,))→{_pad_image_top_left,_get_max_res_without_distortion,integration_tests::main,integration_tests::main,integration_tests::main,integration_tests::main},get_random_model_states_from_config((cfg: BaseConfig,))→{split_experts_layers_tensors,get_n_experts_from_model,get_random_model_from_config,masks_utils::create_layers_names_and_types_from_names,metrics::log,model_parameters_getters::get_tensors_dict},randomize_layers((# noqa: PLR0913 parameters: NDArrays,dummy_config: DictConfig,names: list[str],random_layers: list[str],cid: int = 0,*,server_round: int = 1,truly_random_init: bool,))→{get_random_model_states_from_config,metrics::log,metrics::log,optimizer_states_setters::_cast,metrics::log},get_n_experts_from_model((model: torch.nn.Module,))→{runtime_estimator::init,optimizer_states_setters::_cast,optimizer_states_setters::_cast,optimizer_states_setters::_cast},check_tokenizer_config((tokenizer: PreTrainedTokenizerBase,bos_text: str = "",eos_text: str = "",))→{lr_scheduler::step,lr_scheduler::step,lr_scheduler::step,lr_scheduler::step},get_random_model_from_config((llm_cfg: DictConfig,))→{math::rope,math::rope,math::rope,configuration_deptoe::validate_config},transform_torch_checkpoint_to_photon_model_states((model_states_dict: dict[str,Any],n_experts: int | None = None,*,preserve_optimizer_states: bool = True,))→{split_experts_opt_dict_tensors,split_experts_layers_tensors,lr_scheduler::step},split_experts_opt_dict_tensors((opt_state_dict: dict[str,dict[str,torch.Tensor]],n_experts: int,))→{invert_opt_state_dict_keys,split_experts_layers_tensors,invert_opt_state_dict_keys},merge_experts_opt_dict_tensors((opt_state_dict: dict[str,dict[str,torch.Tensor]],n_experts: int,))→{invert_opt_state_dict_keys,merge_experts_layers_tensors,invert_opt_state_dict_keys},collect_expert_selections((# noqa: PLR0913 transformer: MPTMuPModel,n_bins: int | None = None,experts_per_token_threshold: float = 0.5,*,full_selection_history: bool = False,scores_histograms: bool = True,topk_heatmaps: bool = True,))→{compute_histograms,collect_distributed_selection_history,metrics::log},collect_routers_weights_statistics((transformer: MPTMuPModel,))→{collect_router_weights_statistics,get_expert_sel_weights,metrics::log},construct_message_for_client((ids: tuple[int,int],gen_ins_function: Callable[[int,int | str],dict[str,ConfigRecordValues]],message_constants: tuple[str,str],server_state: ServerState,))→{lr_scheduler::step,lr_scheduler::step},set_determinism((world_mesh: DeviceMesh | None,device: torch.device,seed: int | None = None,deterministic: bool = False,distinct_seed_mesh_dim: str = "pp",))→{lr_scheduler::step,lr_scheduler::step},_get_max_res_without_distortion((image_size: Tuple[int,int],target_size: Tuple[int,int],))→{integration_tests::main,integration_tests::main},compute_histograms((sel_history: torch.Tensor,n_bins: int | None = None,*,scores_histograms: bool = True,topk_heatmaps: bool = True,log_warning: bool = False,))[HOT]→{get_n_bins_from_fd_rule,metrics::log},build_dataloader((dataset: Dataset,batch_size: int,num_workers: int | None,))[HOT]→{get_n_cpu_cores,integration_tests::main},_get_factors((n: int))→{runtime_estimator::init,permute_indices_testing::setUp},close_all_shms((process_uuid: str))→{metrics::log,lr_scheduler::step},print_all_keys((d: dict,indent: int = 0))→{print_all_keys,lr_scheduler::step},wandb_init((wandb_enabled: bool,# noqa: FBT001 *args: dict,**kwargs: Any,# noqa: ANN401))→{math::rope,math::rope},l2_norm((arrays: NDArrays))→{sum_of_squares,comms_ray_batch_variants::load},get_n_cuda_devices(void)→{get_device,lr_scheduler::step},get_unigram_probabilities_tensor((stream_freq_dict: dict[int,int],))→{integration_tests::main,comms_ray_batch_variants::load},get_cuda_compute_capability(void)[HOT]→{get_device,lr_scheduler::step},_float_to_tensor((value: float))→{get_device},wait_for_nodes_to_connect((grid: Grid,n_nodes: int,timeout: float = 3))→{metrics::log},all_reduce_sum_across_ranks((value: float))→{_float_to_tensor},check_message_success((message: Message,))→{metrics::log},invert_opt_state_dict_keys((opt_state_dict: dict[str,dict[str,torch.Tensor]],))→{permute_indices_testing::setUp},dist_max((x: torch.Tensor,mesh: DeviceMesh,extra_pg: dist.ProcessGroup | None = None,))→{_dist_reduce},all_reduce_avg_across_ranks((value: float))→{_float_to_tensor},dist_sum((x: torch.Tensor,mesh: DeviceMesh,extra_pg: dist.ProcessGroup | None = None,))→{_dist_reduce},interpret_unrecognized_message((message_id: str,message: Message,incoming_message_ids: deque[str],*,log_info: bool = False,))→{metrics::log},dist_mean((x: torch.Tensor,mesh: DeviceMesh,extra_pg: dist.ProcessGroup | None = None,))→{_dist_reduce},all_reduce_max_across_ranks((value: float))→{_float_to_tensor},all_reduce_min_across_ranks((value: float))→{_float_to_tensor},get_n_bins_from_fd_rule((sel_history: torch.Tensor,eps: float = 1e-9,))→{runtime_estimator::init},build_hf_dataset((# noqa: PLR0913,PLR0917 path: str,split: str,mode: ConcatMode,temp_dir: TemporaryDirectory,max_length: int | None = None,bos_text: str = "",eos_text: str = "",tokenizer: PreTrainedTokenizerBase | None = None,name: str | None = None,*,no_wrap: bool = False,))[HOT]→{check_tokenizer_config},finalize_curvature_metrics((acc: dict[str,Any]))→{comms_ray_batch_variants::load},split_experts_layers_tensors((tensors_dict: dict[str,torch.nn.Parameter] | dict[str,torch.Tensor],n_experts: int,))→{sigma_moe_expert_utils::split_ffn_experts},merge_experts_layers_tensors((tensors_dict: dict[str,torch.nn.Parameter] | dict[str,torch.Tensor],n_experts: int,))→{sigma_moe_expert_utils::recombine_ffn_experts},get_num_samples_shm((name: str,*,create: bool = False,))→{runtime_estimator::init},verify_experts_layers_split_merge_consistency((original_tensors: dict[str,torch.nn.Parameter] | dict[str,torch.Tensor],split_tensors: dict[str,torch.nn.Parameter] | dict[str,torch.Tensor],n_experts: int,*,atol: float = 1e-6,))→{merge_experts_layers_tensors},get_num_samples_from_streams((streams: Sequence[Stream],*,allow_unsafe_types: bool = True,))→{integration_tests::main},get_eval_loss_shm((name: str,*,create: bool = False,))→{comms_ray_batch_variants::load},verify_experts_opt_dict_split_merge_consistency((original_opt_state_dict: dict[str,dict[str,torch.Tensor]],split_opt_state_dict: dict[str,dict[str,torch.Tensor]],n_experts: int,*,atol: float = 1e-6,))→{merge_experts_opt_dict_tensors},find_supported_resolutions((max_num_tiles: int,tile_size: int))→{_get_factors},get_n_experts_from_model_config((model_config: dict[str,Any] | None,))→{runtime_estimator::init},get_tokenizer_config_from_streams((streams: Sequence[Stream],))→{metrics::log},get_expert_sel_weights((block: torch.nn.Module,))→{lr_scheduler::step},set_trainable_params_dict((model: torch.nn.Module,parameters_dict: OrderedDict[str,torch.Tensor],))→{math::rope},set_frozen_layers_tensors_dict((tensors_dict: OrderedDict[str,torch.Tensor],frozen_layers: list[str] | None = None,))→{metrics::log},get_device(void)→{optimizer_states_setters::_cast},preprocess_data((# arguments from the recipe device: torch.device,dtype: torch.dtype,*,# arguments from the config autoencoder: Optional[AutoEncoder],clip_encoder: FluxEmbedder,t5_encoder: FluxEmbedder,batch: dict[str,Tensor],)),has_cuda_capability((major: int,minor: int)),tile_crop((image: torch.Tensor,tile_size: int)),compress_with_strict((# noqa: UP047 data: Iterable[T],selectors: Iterable[Any],*,strict: bool = True,)),get_report_curvature(void),_dist_reduce((x: torch.Tensor,reduceOp: str,mesh: DeviceMesh,extra_pg: dist.ProcessGroup | None,)),__init__((self,message: Message))[CTOR,DUNDER],get_device_info(void),__init__((self,gc_freq: int = 1000,debug: bool = False))[CTOR,DUNDER],get_client_state_struct((fit_config: FitConfig)),run((self,step_count: int)),get_ndarrays_size_and_bounds((ndarrays: NDArrays,)),generate_noise_latent((bsz: int,height: int,width: int,device: str | torch.device,dtype: torch.dtype,seed: int | None = None,)),get_peak_flops((device_name: str)),__init__((self,remote_name: str,remote_shape: torch.Size,local_shape: torch.Size,))[CTOR,DUNDER],create_position_encoding_for_latents((bsz: int,latent_height: int,latent_width: int,position_dim: int = 3)),__enter__((self))[DUNDER],__exit__((self,exc_type: type[BaseException] | None,exc_value: BaseException | None,traceback: types.TracebackType | None,))[DUNDER],accumulate_curvature_metrics((optimizer_metrics: dict[str,torch.Tensor],metric_name_prefixes: tuple[str,...] = ("curvature/param_diff_norm","curvature/grad_diff_norm","curvature/long_bb","curvature/short_bb","curvature/l2_norm/second_to_first_derivative_estimate_ratio","curvature/l2_norm/second_derivative_estimate","curvature/local_lipschitz",),)),pack_latents((x: Tensor)),collect_distributed_selection_history((ffn: SigmaMoE,)),_pad_image_top_left((image: torch.Tensor,target_size: Tuple[int,int],)),unpack_latents((x: Tensor,latent_height: int,latent_width: int)),check_if_feature_in_pytorch((feature_name: str,pull_request: str,min_nightly_version: Optional[str] = None,)),create_context_parallel_ctx((cp_mesh: DeviceMesh,cp_buffers: list[torch.Tensor],cp_seq_dims: list[int],cp_no_restore_buffers: set[torch.Tensor],cp_rotate_method: str,)),streaming_shms_clean_up(void),get_train_context((enable_loss_parallel: bool,enable_compiled_autograd: bool)),_round_up((x: int,y: int)),maybe_enable_amp((parallel_dims: ParallelDims,mixed_precision_param: str,device_type: torch.device)),set_dict_configrecord_shm((config: ConfigRecord,shm: SharedMemory,)),init_distributed((comm_config: CommConfig,enable_cpu_backend: bool = False,base_folder: str = "")),get_canvas_best_fit((image: torch.Tensor,possible_resolutions: torch.Tensor,resize_to_max_canvas: bool)),set_parameters_in_place((old_parameters: NDArrays,new_parameters: NDArrays,parameter_pos: Iterable[int],)),set_num_samples_shm((shm: SharedMemory,value: int,)),set_pg_timeouts((timeout,world_mesh)),clear_expert_selections((transformer: MPTMuPModel,)),set_eval_loss_shm((shm: SharedMemory,value: float,)),get_sequence_length_from_dataset((streaming_text_dataset: StreamingTextDataset,)),load_image((image_loc: Union[Path,str])),remove_shm_from_resource_tracker(void),get_dict_configrecord_shm((name: str,config: ConfigRecord | None = None,*,create: bool = False,)),collect_router_weights_statistics((router_weights: torch.Tensor,)),get_config_shm((config: Config,name: str,*,create: bool = False,)),match_shapes((remote_param: torch.Tensor,local_param: torch.nn.Parameter | torch.Tensor,)),is_shm_existing((name: str)),set_config_shm((config: Config,shm: SharedMemory,)),get_parameters_shm((parameters_metadata: ModelParametersMetadata,name: str,*,create: bool = False,)),clean_shm_by_name((shm_name: str,)),create_tensors_dict((parameters_names: list[str],parameters: NDArrays,)),sum_of_squares((arrays: NDArrays)),sum_of_squares_per_layer((model_state: tuple[NDArrays,list[str]],)),get_n_cpu_cores(void),merge_freq_dicts((a: dict[int,int],b: dict[int,int],)),is_literal_for_ast((s: str))] validate.py→[__init__((self,job_config: JobConfig,dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer,parallel_dims: ParallelDims,loss_fn: LossFunction,validation_context: Generator[None,None,None],maybe_enable_amp: Generator[None,None,None],metrics_processor: MetricsProcessor | None = None,pp_schedule: _PipelineSchedule | None = None,pp_has_first_stage: bool | None = None,pp_has_last_stage: bool | None = None,))[CTOR,DUNDER]→{tokenizer::build_flux_tokenizer,flux_dataset::build_flux_validation_dataloader},flux_init((self,device: torch.device,_dtype: torch.dtype,autoencoder: AutoEncoder,t5_encoder: FluxEmbedder,clip_encoder: FluxEmbedder,)),build_flux_validator((job_config: JobConfig,dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer,parallel_dims: ParallelDims,loss_fn: LossFunction,validation_context: Generator[None,None,None],maybe_enable_amp: Generator[None,None,None],metrics_processor: MetricsProcessor | None = None,pp_schedule: _PipelineSchedule | None = None,pp_has_first_stage: bool | None = None,pp_has_last_stage: bool | None = None,))[HOT]] wandb_history.py→[__init__((self,*,use_wandb: bool = True))[CTOR,DUNDER],add_loss_distributed((self,server_round: int,loss: float)),add_loss_centralized((self,server_round: int,loss: float)),add_metrics_distributed_fit((self,server_round: int,metrics: dict[str,Scalar],)),add_metrics_distributed((self,server_round: int,metrics: dict[str,Scalar],)),add_metrics_centralized((self,server_round: int,metrics: dict[str,Scalar],))] worker.py→[fit_action((self,config: ConfigRecord,))→{utils::set_config_shm,utils::get_config_shm,utils::compress_with_strict,utils::set_parameters_in_place,utils::set_num_samples_shm,llm_client_functions::llm_fit},evaluate_action((self,config: ConfigRecord,))→{utils::set_config_shm,utils::get_config_shm,utils::set_eval_loss_shm,utils::set_num_samples_shm,llm_client_functions::llm_eval,utils::get_parameters_shm},process_task((self,client_id: int,action: str = FIT))→{metrics::log,metrics::log,masks_utils::generate_empty_mask,metrics::log,lr_scheduler::step,lr_scheduler::step},link_shms((self))→{utils::get_eval_loss_shm,utils::set_num_samples_shm,utils::get_num_samples_shm},create_parameters_shm((self))→{metrics::log,utils::is_shm_existing,utils::get_parameters_shm},get_training_results_from_worker((worker: Worker | None,))→{utils::get_config_shm,utils::get_num_samples_shm,utils::get_parameters_shm},run((self))→{lr_scheduler::step,utils::remove_shm_from_resource_tracker},soft_shutdown((self))→{utils::close_all_shms},create_new_worker((# noqa: PLR0913,PLR0917 config: BaseConfig,task_queue: QueueType,result_queue: QueueType,node_manager_uuid: str,run_uuid: str,parameters_metadata: ModelParametersMetadata,worker_rank: int,))→{lr_scheduler::step},start_worker((worker: Worker))→{metrics::log},__init__((# noqa: PLR0913,PLR0917 self,config: BaseConfig,worker_uuid: str,task_queue: QueueType,result_queue: QueueType,node_manager_uuid: str,run_uuid: str,parameters_metadata: ModelParametersMetadata,worker_rank: int,))[CTOR,DUNDER],_dispatch_action((self,action: str,client_config: ConfigRecord,))] wsd.py→[] 
### UTILS
NODES:35 CALL_DEPTH:5

image.py→[_resize_image_by_patch_count((image: Image.Image,max_patch_per_image: int,patch_size: int,merge_size: int,min_patch_per_image: int = 1,))→{_smart_resize,_smart_resize,_smart_resize,runtime_estimator::init,runtime_estimator::init,runtime_estimator::init},_smart_resize((height: int,width: int,factor: int,# should be equal patch_size * merge_size max_patch_per_image: int,min_patch_per_image: int = 1,))→{integration_tests::main,integration_tests::main,integration_tests::main,integration_tests::main},process_image((image: str | bytes | Image.Image,patch_size: int = 16,merge_size: int = 1,max_patch_per_image: int = 256,min_patch_per_image: int = 1,))→{_resize_image_by_patch_count},calculate_image_tokens((image: Image.Image | torch.Tensor,patch_size: int,spatial_merge_size: int,)),convert_to_patches((pixel_values: torch.Tensor,patch_size: int,temporal_patch_size: int = 1,)),pad_patches((patches: torch.Tensor,grids: torch.Tensor,max_patches: int,)),pad_empty_images_to_target_batch_size((patches: torch.Tensor,grids: torch.Tensor,max_images: int,))] list_checkpoints.py→[main(void)[ENTRY]→{s3_helper::list_objects},main(void)[ENTRY]→{s3_helper::list_objects}] packing.py→[__init__((self,max_seq_length: int,buffer_size: int = 100,batch_size: int = 8,))[CTOR,DUNDER],_pack_buffered_samples((self)),add_sample((self,sample: dict[str,Any])),has_batch_ready((self)),get_next_batch((self))] text.py→[pad_text_batch((input_ids: torch.Tensor,labels: torch.Tensor,seq_len: int,padding_idx: int = 0,ignore_idx: int = -100,)),pad_input_ids_and_labels_to_target_batch_size((input_ids: torch.Tensor,labels: torch.Tensor,target_batch_size: int,padding_idx: int = 0,ignore_idx: int = -100,)),process_text_with_images((text: list[str],image_tokens: list[tuple[int,int,int]],# [(total,width,height),...] tokenizer,special_tokens,add_eos: bool = True,))] 

## DEPENDENCY_PATTERNS

### EDGE_PATTERNS
Contains: 789 edges
Call: 2010 edges

### CROSS_CLUSTER_FLOW
DATA_MODELS→TESTS: 2
DATA_MODELS→UI_COMPONENTS: 9
DATA_MODELS→UTILITY_LAYER: 139
TESTS→DATA_MODELS: 3
TESTS→UI_COMPONENTS: 21
TESTS→UTILITY_LAYER: 103
UI_COMPONENTS→TESTS: 6
UI_COMPONENTS→UTILITY_LAYER: 18
UTILITY_LAYER→DATA_MODELS: 44
UTILITY_LAYER→TESTS: 76
UTILITY_LAYER→UI_COMPONENTS: 354
UTILITY_LAYER→UTILS: 3
UTILS→TESTS: 5
UTILS→UTILITY_LAYER: 6


orch._utils.is_compiling` is deprecated. Use `torch.compiler.is_compiling` instead.
[rank0]:  if not torch._utils.is_compiling() and device_state_steps[0].is_cpu:  # type: ignore[attr-defined]
[rank1]:/nfs-share/aai30/projects/torchtitan/torchtitan/experiments/fl/optimizers/adopt.py:709: UserWarning: `torch._utils.is_compiling` is deprecated. Use `torch.compiler.is_compiling` instead.
[rank1]:  if not torch._utils.is_compiling() and device_state_steps[0].is_cpu:  # type: ignore[attr-defined]
[rank3]:/nfs-share/aai30/projects/torchtitan/torchtitan/experiments/fl/optimizers/adopt.py:709: UserWarning: `torch._utils.is_compiling` is deprecated. Use `torch.compiler.is_compiling` instead.
[rank3]:  if not torch._utils.is_compiling() and device_state_steps[0].is_cpu:  # type: ignore[attr-defined]
[rank0]:[titan] 2025-10-16 16:56:09,868 - root - INFO - step:  2  loss: 10.8729  grad_norm:  1.5049  memory:  6.95GiB(15.63%)  tps: 56,403  tflops: 4.70  mfu: 1.51%
[rank0]:[FlexAttention] block_causal block lengths per sample: [[595, 1, 1, 1, 444, 1, 1, 1, 639, 1, 1, 1, 360], [693, 1, 1, 1, 538, 1, 1, 1, 509, 1, 1, 1, 298], [187, 1, 1, 1, 661, 1, 1, 1, 1193], [605, 1, 1, 1, 1439]]
[rank1]:[titan] 2025-10-16 16:56:09,868 - root - INFO - step:  2  loss: 10.8729  grad_norm:  1.5049  memory:  6.95GiB(15.63%)  tps: 56,214  tflops: 4.69  mfu: 1.50%
[rank1]:[FlexAttention] block_causal block lengths per sample: [[395, 1, 1, 1, 665, 1, 1, 1, 334, 1, 1, 1, 644], [319, 1, 1, 1, 763, 1, 1, 1, 746, 1, 1, 1, 210], [29, 1, 1, 1, 273, 1, 1, 1, 1739], [2047]]
[rank2]:[titan] 2025-10-16 16:56:09,869 - root - INFO - step:  2  loss: 10.8729  grad_norm:  1.5049  memory:  6.95GiB(15.63%)  tps: 56,153  tflops: 4.68  mfu: 1.50%
[rank2]:[FlexAttention] block_causal block lengths per sample: [[322, 1, 1, 1, 679, 1, 1, 1, 517, 1, 1, 1, 520], [462, 1, 1, 1, 803, 1, 1, 1, 776], [1068, 1, 1, 1, 976], [1168, 1, 1, 1, 850, 1, 1, 1, 23]]
[rank3]:[titan] 2025-10-16 16:56:09,868 - root - INFO - step:  2  loss: 10.8729  grad_norm:  1.5049  memory:  6.95GiB(15.63%)  tps: 56,157  tflops: 4.68  mfu: 1.50%
[rank3]:[FlexAttention] block_causal block lengths per sample: [[1184, 1, 1, 1, 575, 1, 1, 1, 282], [1522, 1, 1, 1, 187, 1, 1, 1, 205, 1, 1, 1, 124], [14, 1, 1, 1, 449, 1, 1, 1, 458, 1, 1, 1, 422, 1, 1, 1, 168, 1, 1, 1, 215, 1, 1, 1, 54, 1, 1, 1, 116, 1, 1, 1, 127], [99, 1, 1, 1, 673, 1, 1, 1, 1117, 1, 1, 1, 149]]
[rank0]:[FlexAttention] block_causal block lengths per sample: [[1209, 1, 1, 1, 169, 1, 1, 1, 361, 1, 1, 1, 172, 1, 1, 1, 124], [652, 1, 1, 1, 590, 1, 1, 1, 673, 1, 1, 1, 123], [1117, 1, 1, 1, 298, 1, 1, 1, 626], [452, 1, 1, 1, 501, 1, 1, 1, 184, 1, 1, 1, 251, 1, 1, 1, 148, 1, 1, 1, 350, 1, 1, 1, 143]]
[rank1]:[FlexAttention] block_causal block lengths per sample: [[2047], [1128, 1, 1, 1, 543, 1, 1, 1, 370], [627, 1, 1, 1, 580, 1, 1, 1, 834], [7, 1, 1, 1, 362, 1, 1, 1, 168, 1, 1, 1, 181, 1, 1, 1, 111, 1, 1, 1, 112, 1, 1, 1, 318, 1, 1, 1, 398, 1, 1, 1, 366]]
[rank2]:[FlexAttention] block_causal block lengths per sample: [[1267, 1, 1, 1, 410, 1, 1, 1, 364], [393, 1, 1, 1, 620, 1, 1, 1, 965, 1, 1, 1, 60], [803, 1, 1, 1, 673, 1, 1, 1, 565], [468, 1, 1, 1, 143, 1, 1, 1, 247, 1, 1, 1, 1171, 1, 1, 1, 6]]
[rank3]:[FlexAttention] block_causal block lengths per sample: [[851, 1, 1, 1, 725, 1, 1, 1, 465], [298, 1, 1, 1, 1006, 1, 1, 1, 225, 1, 1, 1, 367, 1, 1, 1, 122, 1, 1, 1, 14], [500, 1, 1, 1, 828, 1, 1, 1, 713], [1085, 1, 1, 1, 257, 1, 1, 1, 699]]
[rank0]:[FlexAttention] block_causal block lengths per sample: [[152, 1, 1, 1, 543, 1, 1, 1, 660, 1, 1, 1, 347, 1, 1, 1, 333], [150, 1, 1, 1, 922, 1, 1, 1, 172, 1, 1, 1, 110, 1, 1, 1, 433, 1, 1, 1, 245], [8, 1, 1, 1, 1349, 1, 1, 1, 468, 1, 1, 1, 213], [459, 1, 1, 1, 197, 1, 1, 1, 947, 1, 1, 1, 435]]
[rank1]:[FlexAttention] block_causal block lengths per sample: [[1, 1, 1, 1, 244, 1, 1, 1, 714, 1, 1, 1, 555, 1, 1, 1, 521], [81, 1, 1, 1, 1019, 1, 1, 1, 236, 1, 1, 1, 702], [68, 1, 1, 1, 379, 1, 1, 1, 848, 1, 1, 1, 383, 1, 1, 1, 357], [105, 1, 1, 1, 794, 1, 1, 1, 894, 1, 1, 1, 245]]
[rank2]:[FlexAttention] block_causal block lengths per sample: [[2047], [837, 1, 1, 1, 701, 1, 1, 1, 251, 1, 1, 1, 88, 1, 1, 1, 152, 1, 1, 1, 3], [1247, 1, 1, 1, 184, 1, 1, 1, 433, 1, 1, 1, 174], [521, 1, 1, 1, 319, 1, 1, 1, 691, 1, 1, 1, 362, 1, 1, 1, 142]]
[rank3]:[FlexAttention] block_causal block lengths per sample: [[1374, 1, 1, 1, 159, 1, 1, 1, 508], [545, 1, 1, 1, 395, 1, 1, 1, 113, 1, 1, 1, 302, 1, 1, 1, 352, 1, 1, 1, 325], [775, 1, 1, 1, 485, 1, 1, 1, 554, 1, 1, 1, 224], [56, 1, 1, 1, 1458, 1, 1, 1, 297, 1, 1, 1, 128, 1, 1, 1, 96]]
[rank0]:[FlexAttention] block_causal block lengths per sample: [[138, 1, 1, 1, 1906], [696, 1, 1, 1, 1348], [828, 1, 1, 1, 1216], [385, 1, 1, 1, 638, 1, 1, 1, 849, 1, 1, 1, 166]]
[rank1]:[FlexAttention] block_causal block lengths per sample: [[358, 1, 1, 1, 302, 1, 1, 1, 337, 1, 1, 1, 104, 1, 1, 1, 190, 1, 1, 1, 38, 1, 1, 1, 106, 1, 1, 1, 72, 1, 1, 1, 329, 1, 1, 1, 126, 1, 1, 1, 55], [1446, 1, 1, 1, 231, 1, 1, 1, 364], [244, 1, 1, 1, 1096, 1, 1, 1, 514, 1, 1, 1, 184], [58, 1, 1, 1, 524, 1, 1, 1, 416, 1, 1, 1, 121, 1, 1, 1, 760, 1, 1, 1, 153]]
[rank2]:[FlexAttention] block_causal block lengths per sample: [[241, 1, 1, 1, 651, 1, 1, 1, 151, 1, 1, 1, 105, 1, 1, 1, 258, 1, 1, 1, 266, 1, 1, 1, 176, 1, 1, 1, 178], [1869, 1, 1, 1, 175], [65, 1, 1, 1, 1979], [330, 1, 1, 1, 148, 1, 1, 1, 213, 1, 1, 1, 642, 1, 1, 1, 296, 1, 1, 1, 329, 1, 1, 1, 71]]
[rank3]:[FlexAttention] block_causal block lengths per sample: [[275, 1, 1, 1, 370, 1, 1, 1, 74, 1, 1, 1, 308, 1, 1, 1, 131, 1, 1, 1, 66, 1, 1, 1, 805], [356, 1, 1, 1, 503, 1, 1, 1, 469, 1, 1, 1, 175, 1, 1, 1, 166, 1, 1, 1, 243, 1, 1, 1, 117], [815, 1, 1, 1, 300, 1, 1, 1, 674, 1, 1, 1, 81, 1, 1, 1, 165], [257, 1, 1, 1, 876, 1, 1, 1, 908]]
[rank0]:[titan] 2025-10-16 16:56:10,291 - root - INFO - step:  3  loss: 10.8738  grad_norm:  1.4677  memory:  6.95GiB(15.63%)  tps: 77,569  tflops: 6.47  mfu: 2.07%
[rank0]:[FlexAttention] block_causal block lengths per sample: [[1229, 1, 1, 1, 815], [201, 1, 1, 1, 1236, 1, 1, 1, 604], [80, 1, 1, 1, 1044, 1, 1, 1, 91, 1, 1, 1, 711, 1, 1, 1, 109], [439, 1, 1, 1, 1037, 1, 1, 1, 249, 1, 1, 1, 313]]
[rank1]:[titan] 2025-10-16 16:56:10,292 - root - INFO - step:  3  loss: 10.8738  grad_norm:  1.4677  memory:  6.95GiB(15.63%)  tps: 77,647  tflops: 6.47  mfu: 2.08%
[rank1]:[FlexAttention] block_causal block lengths per sample: [[294, 1, 1, 1, 1750], [2047], [253, 1, 1, 1, 580, 1, 1, 1, 201, 1, 1, 1, 1004], [445, 1, 1, 1, 167, 1, 1, 1, 341, 1, 1, 1, 219, 1, 1, 1, 375, 1, 1, 1, 213, 1, 1, 1, 113, 1, 1, 1, 153]]
[rank2]:[titan] 2025-10-16 16:56:10,292 - root - INFO - step:  3  loss: 10.8738  grad_norm:  1.4677  memory:  6.95GiB(15.63%)  tps: 77,708  tflops: 6.48  mfu: 2.08%
[rank2]:[FlexAttention] block_causal block lengths per sample: [[377, 1, 1, 1, 463, 1, 1, 1, 1201], [2047], [889, 1, 1, 1, 558, 1, 1, 1, 594], [786, 1, 1, 1, 215, 1, 1, 1, 45, 1, 1, 1, 992]]
[rank3]:[titan] 2025-10-16 16:56:10,292 - root - INFO - step:  3  loss: 10.8738  grad_norm:  1.4677  memory:  6.95GiB(15.63%)  tps: 77,635  tflops: 6.47  mfu: 2.07%
[rank3]:[FlexAttention] block_causal block lengths per sample: [[419, 1, 1, 1, 1357, 1, 1, 1, 265], [1671, 1, 1, 1, 373], [1176, 1, 1, 1, 325, 1, 1, 1, 540], [282, 1, 1, 1, 655, 1, 1, 1, 104, 1, 1, 1, 76, 1, 1, 1, 166, 1, 1, 1, 169, 1, 1, 1, 246, 1, 1, 1, 266, 1, 1, 1, 59]]
[rank0]:[FlexAttention] block_causal block lengths per sample: [[2047], [1, 1, 1, 1, 2043], [1236, 1, 1, 1, 808], [352, 1, 1, 1, 210, 1, 1, 1, 143, 1, 1, 1, 350, 1, 1, 1, 980]]
[rank1]:[FlexAttention] block_causal block lengths per sample: [[2047], [1009, 1, 1, 1, 442, 1, 1, 1, 234, 1, 1, 1, 353], [517, 1, 1, 1, 1527], [1755, 1, 1, 1, 289]]
[rank2]:[FlexAttention] block_causal block lengths per sample: [[1479, 1, 1, 1, 565], [478, 1, 1, 1, 801, 1, 1, 1, 762], [2047], [32, 1, 1, 1, 983, 1, 1, 1, 1026]]
[rank3]:[FlexAttention] block_causal block lengths per sample: [[2047], [288, 1, 1, 1, 651, 1, 1, 1, 1102], [1074, 1, 1, 1, 613, 1, 1, 1, 354], [499, 1, 1, 1, 721, 1, 1, 1, 446, 1, 1, 1, 372]]
[rank0]:[FlexAttention] block_causal block lengths per sample: [[592, 1, 1, 1, 243, 1, 1, 1, 1206], [713, 1, 1, 1, 172, 1, 1, 1, 1156], [85, 1, 1, 1, 417, 1, 1, 1, 1539], [2047]]
[rank1]:[FlexAttention] block_causal block lengths per sample: [[2047], [1060, 1, 1, 1, 406, 1, 1, 1, 103, 1, 1, 1, 237, 1, 1, 1, 138, 1, 1, 1, 88], [122, 1, 1, 1, 343, 1, 1, 1, 419, 1, 1, 1, 203, 1, 1, 1, 948], [40, 1, 1, 1, 358, 1, 1, 1, 1643]]
[rank2]:[FlexAttention] block_causal block lengths per sample: [[1060, 1, 1, 1, 436, 1, 1, 1, 545], [125, 1, 1, 1, 125, 1, 1, 1, 449, 1, 1, 1, 503, 1, 1, 1, 597, 1, 1, 1, 231, 1, 1], [1920, 1, 1, 1, 124], [497, 1, 1, 1, 331, 1, 1, 1, 122, 1, 1, 1, 259, 1, 1, 1, 826]]
[rank3]:[FlexAttention] block_causal block lengths per sample: [[128, 1, 1, 1, 186, 1, 1, 1, 614, 1, 1, 1, 174, 1, 1, 1, 933], [1149, 1, 1, 1, 895], [151, 1, 1, 1, 415, 1, 1, 1, 881, 1, 1, 1, 591], [429, 1, 1, 1, 420, 1, 1, 1, 279, 1, 1, 1, 579, 1, 1, 1, 322, 1, 1, 1, 3]]
[rank0]:[FlexAttention] block_causal block lengths per sample: [[623, 1, 1, 1, 987, 1, 1, 1, 431], [1013, 1, 1, 1, 1031], [460, 1, 1, 1, 512, 1, 1, 1, 1069], [519, 1, 1, 1, 174, 1, 1, 1, 407, 1, 1, 1, 938]]
[rank1]:[FlexAttention] block_causal block lengths per sample: [[17, 1, 1, 1, 231, 1, 1, 1, 463, 1, 1, 1, 781, 1, 1, 1, 225, 1, 1, 1, 315], [47, 1, 1, 1, 676, 1, 1, 1, 1304, 1, 1, 1, 11], [2047], [310, 1, 1, 1, 912, 1, 1, 1, 372, 1, 1, 1, 444]]
[rank2]:[FlexAttention] block_causal block lengths per sample: [[631, 1, 1, 1, 1413], [2047], [25, 1, 1, 1, 239, 1, 1, 1, 1295, 1, 1, 1, 278, 1, 1, 1, 198], [2047]]
[rank3]:[FlexAttention] block_causal block lengths per sample: [[493, 1, 1, 1, 221, 1, 1, 1, 1327], [2047], [338, 1, 1, 1, 221, 1, 1, 1, 234, 1, 1, 1, 413, 1, 1, 1, 256, 1, 1, 1, 293, 1, 1, 1, 274], [145, 1, 1, 1, 830, 1, 1, 1, 454, 1, 1, 1, 362, 1, 1, 1, 244]]
[rank0]:[titan] 2025-10-16 16:56:10,669 - root - INFO - step:  4  loss: 10.8608  grad_norm:  1.5229  memory:  6.95GiB(15.63%)  tps: 86,867  tflops: 7.24  mfu: 2.32%
[rank0]:[FlexAttention] block_causal block lengths per sample: [[506, 1, 1, 1, 1538], [1564, 1, 1, 1, 480], [2047], [60, 1, 1, 1, 825, 1, 1, 1, 411, 1, 1, 1, 172, 1, 1, 1, 567]]
[rank1]:[titan] 2025-10-16 16:56:10,670 - root - INFO - step:  4  loss: 10.8608  grad_norm:  1.5229  memory:  6.95GiB(15.63%)  tps: 86,929  tflops: 7.25  mfu: 2.32%
[rank1]:[FlexAttention] block_causal block lengths per sample: [[1255, 1, 1, 1, 447, 1, 1, 1, 148, 1, 1, 1, 150, 1, 1, 1, 35], [1193, 1, 1, 1, 160, 1, 1, 1, 350, 1, 1, 1, 241, 1, 1, 1, 91], [419, 1, 1, 1, 627, 1, 1, 1, 908, 1, 1, 1, 84], [2047]]
[rank2]:[titan] 2025-10-16 16:56:10,670 - root - INFO - step:  4  loss: 10.8608  grad_norm:  1.5229  memory:  6.95GiB(15.63%)  tps: 87,020  tflops: 7.26  mfu: 2.33%
[rank2]:[FlexAttention] block_causal block lengths per sample: [[284, 1, 1, 1, 534, 1, 1, 1, 564, 1, 1, 1, 603, 1, 1, 1, 50], [338, 1, 1, 1, 209, 1, 1, 1, 1494], [2047], [216, 1, 1, 1, 206, 1, 1, 1, 1619]]
[rank3]:[titan] 2025-10-16 16:56:10,670 - root - INFO - step:  4  loss: 10.8608  grad_norm:  1.5229  memory:  6.95GiB(15.63%)  tps: 86,918  tflops: 7.25  mfu: 2.32%
[rank3]:[FlexAttention] block_causal block lengths per sample: [[2047], [96, 1, 1, 1, 1725, 1, 1, 1, 220], [12, 1, 1, 1, 563, 1, 1, 1, 583, 1, 1, 1, 839, 1, 1, 1, 38], [1285, 1, 1, 1, 484, 1, 1, 1, 272]]
[rank0]:[FlexAttention] block_causal block lengths per sample: [[353, 1, 1, 1, 920, 1, 1, 1, 215, 1, 1, 1, 550], [662, 1, 1, 1, 1255, 1, 1, 1, 124], [2047], [1876, 1, 1, 1, 168]]
[rank1]:[FlexAttention] block_causal block lengths per sample: [[873, 1, 1, 1, 440, 1, 1, 1, 728], [2047], [88, 1, 1, 1, 835, 1, 1, 1, 637, 1, 1, 1, 478], [5, 1, 1, 1, 1487, 1, 1, 1, 549]]
[rank2]:[FlexAttention] block_causal block lengths per sample: [[1366, 1, 1, 1, 541, 1, 1, 1, 134], [2047], [871, 1, 1, 1, 896, 1, 1, 1, 274], [171, 1, 1, 1, 1873]]
[rank3]:[FlexAttention] block_causal block lengths per sample: [[2047], [72, 1, 1, 1, 24, 1, 1, 1, 268, 1, 1, 1, 431, 1, 1, 1, 572, 1, 1, 1, 665], [333, 1, 1, 1, 567, 1, 1, 1, 586, 1, 1, 1, 552], [211, 1, 1, 1, 388, 1, 1, 1, 154, 1, 1, 1, 1285]]
[rank0]:[FlexAttention] block_causal block lengths per sample: [[86, 1, 1, 1, 508, 1, 1, 1, 488, 1, 1, 1, 610, 1, 1, 1, 343], [832, 1, 1, 1, 533, 1, 1, 1, 676], [808, 1, 1, 1, 405, 1, 1, 1, 545, 1, 1, 1, 99, 1, 1, 1, 69, 1, 1, 1, 90, 1, 1, 1, 13], [571, 1, 1, 1, 429, 1, 1, 1, 452, 1, 1, 1, 321, 1, 1, 1, 37, 1, 1, 1, 112, 1, 1, 1, 107]]
[rank1]:[FlexAttention] block_causal block lengths per sample: [[334, 1, 1, 1, 1654, 1, 1, 1, 53], [1081, 1, 1, 1, 963], [1555, 1, 1, 1, 103, 1, 1, 1, 326, 1, 1, 1, 54], [2047]]
[rank2]:[FlexAttention] block_causal block lengths per sample: [[655, 1, 1, 1, 638, 1, 1, 1, 748], [720, 1, 1, 1, 613, 1, 1, 1, 268, 1, 1, 1, 131, 1, 1, 1, 303], [77, 1, 1, 1, 62, 1, 1, 1, 59, 1, 1, 1, 457, 1, 1, 1, 165, 1, 1, 1, 149, 1, 1, 1, 269, 1, 1, 1, 156, 1, 1, 1, 325, 1, 1, 1, 301], [275, 1, 1, 1, 631, 1, 1, 1, 421, 1, 1, 1, 383, 1, 1, 1, 259, 1, 1, 1, 63]]
[rank3]:[FlexAttention] block_causal block lengths per sample: [[281, 1, 1, 1, 121, 1, 1, 1, 40, 1, 1, 1, 47, 1, 1, 1, 172, 1, 1, 1, 979, 1, 1, 1, 212, 1, 1, 1, 106, 1, 1, 1, 65], [1773, 1, 1, 1, 271], [156, 1, 1, 1, 542, 1, 1, 1, 307, 1, 1, 1, 253, 1, 1, 1, 777], [43, 1, 1, 1, 106, 1, 1, 1, 1892]]
[rank0]:[FlexAttention] block_causal block lengths per sample: [[2047], [830, 1, 1, 1, 681, 1, 1, 1, 530], [2047], [431, 1, 1, 1, 902, 1, 1, 1, 693, 1, 1, 1, 12]]
[rank1]:[FlexAttention] block_causal block lengths per sample: [[267, 1, 1, 1, 547, 1, 1, 1, 158, 1, 1, 1, 391, 1, 1, 1, 109, 1, 1, 1, 560], [123, 1, 1, 1, 64, 1, 1, 1, 128, 1, 1, 1, 259, 1, 1, 1, 405, 1, 1, 1, 548, 1, 1, 1, 311, 1, 1, 1, 188], [102, 1, 1, 1, 1284, 1, 1, 1, 449, 1, 1, 1, 203], [684, 1, 1, 1, 972, 1, 1, 1, 385]]
[rank2]:[FlexAttention] block_causal block lengths per sample: [[2047], [124, 1, 1, 1, 78, 1, 1, 1, 1562, 1, 1, 1, 152, 1, 1, 1, 119], [2047], [2047]]
[rank3]:[FlexAttention] block_causal block lengths per sample: [[1313, 1, 1, 1, 48, 1, 1, 1, 44, 1, 1, 1, 633], [252, 1, 1, 1, 86, 1, 1, 1, 237, 1, 1, 1, 285, 1, 1, 1, 744, 1, 1, 1, 348, 1, 1, 1, 77], [644, 1, 1, 1, 390, 1, 1, 1, 782, 1, 1, 1, 222], [2047]]
[rank0]:[titan] 2025-10-16 16:56:11,077 - root - INFO - step:  5  loss: 10.8685  grad_norm:  1.5239  memory:  6.95GiB(15.63%)  tps: 80,511  tflops: 6.71  mfu: 2.15%
[rank0]:[FlexAttention] block_causal block lengths per sample: [[1162, 1, 1, 1, 435, 1, 1, 1, 281, 1, 1, 1, 160], [2047], [179, 1, 1, 1, 576, 1, 1, 1, 1286], [666, 1, 1, 1, 603, 1, 1, 1, 132, 1, 1, 1, 515, 1, 1, 1, 119]]
[rank1]:[titan] 2025-10-16 16:56:11,078 - root - INFO - step:  5  loss: 10.8685  grad_norm:  1.5239  memory:  6.95GiB(15.63%)  tps: 80,550  tflops: 6.72  mfu: 2.15%
[rank1]:[FlexAttention] block_causal block lengths per sample: [[2047], [808, 1, 1, 1, 779, 1, 1, 1, 173, 1, 1, 1, 278], [94, 1, 1, 1, 978, 1, 1, 1, 753, 1, 1, 1, 213], [2047]]
[rank2]:[titan] 2025-10-16 16:56:11,078 - root - INFO - step:  5  loss: 10.8685  grad_norm:  1.5239  memory:  6.95GiB(15.63%)  tps: 80,649  tflops: 6.73  mfu: 2.16%
[rank2]:[FlexAttention] block_causal block lengths per sample: [[1120, 1, 1, 1, 247, 1, 1, 1, 674], [485, 1, 1, 1, 311, 1, 1, 1, 419, 1, 1, 1, 298, 1, 1, 1, 522], [1757, 1, 1, 1, 120, 1, 1, 1, 164], [2047]]
[rank3]:[titan] 2025-10-16 16:56:11,078 - root - INFO - step:  5  loss: 10.8685  grad_norm:  1.5239  memory:  6.95GiB(15.63%)  tps: 80,564  tflops: 6.72  mfu: 2.15%
[rank3]:[FlexAttention] block_causal block lengths per sample: [[960, 1, 1, 1, 906, 1, 1, 1, 175], [777, 1, 1, 1, 219, 1, 1, 1, 349, 1, 1, 1, 256, 1, 1, 1, 166, 1, 1, 1, 215, 1, 1, 1, 47], [523, 1, 1, 1, 887, 1, 1, 1, 206, 1, 1, 1, 422], [1609, 1, 1, 1, 435]]
[rank0]:[FlexAttention] block_causal block lengths per sample: [[53, 1, 1, 1, 126, 1, 1, 1, 247, 1, 1, 1, 716, 1, 1, 1, 845, 1, 1, 1, 45], [540, 1, 1, 1, 478, 1, 1, 1, 708, 1, 1, 1, 312], [2047], [213, 1, 1, 1, 717, 1, 1, 1, 259, 1, 1, 1, 849]]
[rank1]:[FlexAttention] block_causal block lengths per sample: [[110, 1, 1, 1, 173, 1, 1, 1, 886, 1, 1, 1, 734, 1, 1, 1, 132], [1, 42, 1, 1, 1, 352, 1, 1, 1, 529, 1, 1, 1, 320, 1, 1, 1, 255, 1, 1, 1, 263, 1, 1, 1, 197, 1, 1, 1, 67], [370, 1, 1, 1, 1674], [438, 1, 1, 1, 1606]]
[rank2]:[FlexAttention] block_causal block lengths per sample: [[27, 1, 1, 1, 251, 1, 1, 1, 429, 1, 1, 1, 170, 1, 1, 1, 227, 1, 1, 1, 139, 1, 1, 1, 179, 1, 1, 1, 109, 1, 1, 1, 59, 1, 1, 1, 83, 1, 1, 1, 291, 1, 1, 1, 50], [2047], [230, 1, 1, 1, 1814], [761, 1, 1, 1, 1031, 1, 1, 1, 249]]
[rank3]:[FlexAttention] block_causal block lengths per sample: [[2047], [323, 1, 1, 1, 1173, 1, 1, 1, 113, 1, 1, 1, 429], [99, 1, 1, 1, 148, 1, 1, 1, 176, 1, 1, 1, 892, 1, 1, 1, 179, 1, 1, 1, 236, 1, 1, 1, 233, 1, 1, 1, 63], [47, 1, 1, 1, 693, 1, 1, 1, 603, 1, 1, 1, 529, 1, 1, 1, 163]]
[rank0]:[FlexAttention] block_causal block lengths per sample: [[198, 1, 1, 1, 299, 1, 1, 1, 285, 1, 1, 1, 397, 1, 1, 1, 443, 1, 1, 1, 410], [476, 1, 1, 1, 295, 1, 1, 1, 1087, 1, 1, 1, 180], [24, 1, 1, 1, 1057, 1, 1, 1, 219, 1, 1, 1, 738], [83, 1, 1, 1, 273, 1, 1, 1, 235, 1, 1, 1, 155, 1, 1, 1, 718, 1, 1, 1, 146, 1, 1, 1, 419]]
[rank1]:[FlexAttention] block_causal block lengths per sample: [[128, 1, 1, 1, 64, 1, 1, 1, 117, 1, 1, 1, 198, 1, 1, 1, 1162, 1, 1, 1, 163, 1, 1, 1, 197], [907, 1, 1, 1, 1137], [953, 1, 1, 1, 782, 1, 1, 1, 306], [456, 1, 1, 1, 1049, 1, 1, 1, 536]]
[rank2]:[FlexAttention] block_causal block lengths per sample: [[693, 1, 1, 1, 154, 1, 1, 1, 779, 1, 1, 1, 412], [287, 1, 1, 1, 377, 1, 1, 1, 401, 1, 1, 1, 323, 1, 1, 1, 647], [174, 1, 1, 1, 1870], [295, 1, 1, 1, 98, 1, 1, 1, 312, 1, 1, 1, 159, 1, 1, 1, 1171]]
[rank3]:[FlexAttention] block_causal block lengths per sample: [[2047], [

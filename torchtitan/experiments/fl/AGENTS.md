# EMBARGO: LLM-Optimized Codebase Dependency Graph

**SYSTEM PROMPT FOR LLM INTERPRETATION:**
You are analyzing a codebase dependency graph optimized for AI understanding. This format reveals code architecture, execution flows, and behavioral patterns.

## INTERPRETATION KEY

### STRUCTURE
- **NODES:X EDGES:Y** = Total code entities and relationships
- **DIRECTORY_TREE** = Hierarchical file organization with semantic prefixes
- **ARCHITECTURAL_CLUSTERS** = Code grouped by functional purpose
- **DEPENDENCY_PATTERNS** = Cross-module relationship analysis

### BEHAVIORAL NOTATION
- **filename.rs→[...]** = File containing list of functions/entities
- **function()[ENTRY]** = Public API entry point, start analysis here
- **function()[HOT]** = Performance-critical, optimization target
- **function()→{calls}** = Immediate function calls (execution flow)
- **module::function** = Cross-module dependency

### ANALYSIS GUIDANCE
1. **Entry Points**: Start with [ENTRY] functions to understand public APIs
2. **Execution Flow**: Follow →{calls} to trace code execution paths
3. **Hot Paths**: Focus [HOT] functions for performance analysis
4. **Architecture**: Use clusters to understand system organization
5. **Dependencies**: Cross-cluster flows show coupling patterns

### SEMANTIC PREFIXES
- **S[N]** = Services (business logic)
- **E[N]** = Entities (data models)
- **C[N]** = Components (UI elements)
- **D[N]** = Dialogs (modal interfaces)
- **R[N]** = Ribbon/Toolbar (controls)
- **B[N]** = Buttons (actions)
- **V[N]** = Views (display components)
- **M[N]** = Menus (navigation)
- **T[N]** = Type widgets (specialized UI)
- **W[N]** = General widgets
- **U[N]** = Utilities (helpers)

### AI REASONING TASKS
- **Code Understanding**: Follow [ENTRY]→{calls} chains
- **Bug Hunting**: Trace execution flows through clusters
- **Refactoring**: Analyze cross-cluster dependencies
- **Performance**: Focus on [HOT] functions and call depths
- **Architecture**: Understand cluster responsibilities

---

# CODE_GRAPH
NODES:625 EDGES:342

## DIRECTORY_TREE
ROOT: torchtitan/experiments/fl/
├─ configs/ → TST[1] U[3]
│  └─ tests/ → TST[1]
├─ dataloader/ → U[7]
├─ models/ → E[15]
│  ├─ llama3_mup/ → E[9]
│  │  ├─ infra/ → E[2]
│  │  ├─ model/ → E[4]
│  │  ├─ tests/ → E[1]
│  │  └─ train_configs/ → E[1]
│  ├─ mosaic_llama3/ → E[1]
│  ├─ mosaic_llama3_mup/ → E[1]
│  └─ tests/ → E[1]
├─ optimizers/ → U[8]
└─ tests/ → TST[1]

## ARCHITECTURAL_CLUSTERS

### DATA_MODELS
NODES:136 CALL_DEPTH:4

__init__.py→[_get_llama3_mup_spec(void),_update_vocab_sizes((base_spec: TrainSpec,mosaic_spec: TrainSpec)),_update_vocab_sizes((base_spec: TrainSpec,mosaic_spec: TrainSpec)),build_mup_optimizers((model_parts: list[nn.Module],optimizer_config: OptimizerConfig,parallel_dims: ParallelDims,ft_manager: FTManager | None = None,))[HOT],get_train_spec(void),get_train_spec(void),get_train_spec(void)] mosaic_adapter.py→[] mup_args.py→[] mup_model.py→[_precompute_freqs_cis((self))[HOT]→{_precompute_freqs_cis},get_optimizer_param_groups((self,optimizer_config: dict[str,Any]))→{desloc::tick},init_weights((self,init_std: float)),init_weights((self,init_std: float)),__init__((self,layer_id: int,model_args: TransformerModelArgs # noqa: ARG002))[CTOR,DUNDER],forward((self,x: torch.Tensor,freqs_cis: torch.Tensor,)),init_weights((self)),__init__((self,model_args: TransformerModelArgs))[CTOR,DUNDER],init_weights((self,buffer_device: torch.device | None = None)),_iter_trainable_params((self)),_bucketize_parameters((self,param_entries: list[tuple[str,Parameter]])),_resolve_bucket_name((self,name: str,embed_suffixes: list[str],hidden_ln_suffixes: list[str],no_decay_suffixes: list[str],decay_weight_suffixes: list[str],))[HOT],_validate_bucket_counts((self,total_params: int,buckets: dict[str,list[Parameter]])),_compute_lr_scaling((self))[HOT],_resolve_optimizer_eps((self,eps: float,*,width_lr_scaling: float,))[HOT],_build_param_groups((self,buckets: dict[str,list[Parameter]],*,base_lr: float,weight_decay: float,width_lr_scaling: float,depth_lr_scaling: float,))[HOT],build_mup_optimizer_overrides((self,*,lr: float,eps: float,weight_decay: float,))[HOT],forward((self,tokens: torch.Tensor,input_batch: torch.Tensor | None = None,# noqa: ARG002))] parallelize.py→[parallelize_llama_mup((model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,))] state_dict_adapter.py→[__init__((self,model_args: TransformerModelArgs,hf_assets_path: str | None,))[CTOR,DUNDER]] test_mosaic_adapter.py→[test_register_applies_builder_overrides((self))[HOT,TEST]→{__init__::get_train_spec},_dummy_builder((*_args: object,**_kwargs: object))[HOT],tearDown((self)),test_build_uses_mosaic_name_by_default((self))[HOT,TEST]] test_mup_model.py→[test_mosaic_builder_integrates_mup_overrides((self))[HOT,TEST]→{optimizer_builder::build_mosaic_optimizers},test_mosaic_builder_desloc_requires_ft((self))[HOT,TEST]→{optimizer_builder::build_mosaic_optimizers},setUp((self)),_get_expected_mup_eps((self,base_eps: float)),test_model_initialization((self))[TEST],test_forward_pass((self))[TEST],test_weight_initialization((self))[TEST],test_optimizer_overrides_build_param_groups((self))[HOT,TEST],test_optimizer_overrides_disabled_when_hidden_scaling_off((self))[TEST]] utils.py→[build_mosaic_spec((base_spec: TrainSpec,*,spec_name: str,overrides: MosaicSpecOverrides | None = None,))[HOT],ensure_mosaic_spec((base_spec_name: str,*,spec_name: str | None = None,overrides: MosaicSpecOverrides | None = None,))] 
### TESTS
NODES:36 CALL_DEPTH:1

test_config_manager.py→[teardown_module(void),test_parse_args_produces_typed_dataclasses(void)[TEST],test_cli_overrides_nested_metrics_field(void)[TEST],test_toml_invalid_metrics_payload_rejected((tmp_path: Path))[TEST],test_manual_init_coerces_nested_sections(void)[TEST],test_manual_init_invalid_section_type_raises(void)[TEST]] test_unigram_metrics.py→[__init__((self,*_args: object,**_kwargs: object))[CTOR,DUNDER],add_state((self,name: str,default: torch.Tensor,dist_reduce_fx: str | None = None,)),register_buffer((self,name: str,tensor: torch.Tensor)),__init__((self,*_args: object,**_kwargs: object))[CTOR,DUNDER],__init__((self,*_args: object,**_kwargs: object))[CTOR,DUNDER],get_peak_stats((self)),reset_peak_stats((self)),test_unigram_manager_aggregation_and_reset(void)[TEST],test_unigram_manager_teardown_removes_metric(void)[TEST],test_fl_metrics_processor_registers_expected_callbacks(void)[TEST]] 
### UTILITY_LAYER
NODES:453 CALL_DEPTH:10

__init__.py→[__init__((self))[CTOR,DUNDER],parse_args((self,args: list[str] = sys.argv[1:])),load_mosaic_job_config((args: list[str] | None = None))] _decoupled_decay.py→[_compute_decay_factor((lr: float | Tensor,initial_lr: float | Tensor | None))[HOT]] adopt.py→[_multi_tensor_adopt((# noqa: C901,PLR0912,PLR0913,PLR0915 params: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | None,has_complex: bool,beta1: float,beta2: float,lr: float | Tensor,clip_lambda: Callable[[Number | Tensor | Any],float] | None,weight_decay: float,decouple: bool,eps: float,maximize: bool,capturable: bool,differentiable: bool,))→{_decoupled_decay::_compute_decay_factor,_decoupled_decay::_compute_decay_factor},report_per_parameter_metrics((self,param: torch.Tensor,name: str,optimizer_metrics: dict[str,torch.Tensor],))→{_decoupled_decay::_compute_decay_factor},_single_tensor_adopt((# noqa: PLR0913 params: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | None,decouple: bool,clip_lambda: Callable[[Number | Tensor | Any],float] | None,beta1: float,beta2: float,lr: float | Tensor,weight_decay: float,eps: float,maximize: bool,capturable: bool,differentiable: bool,has_complex: bool,# noqa: ARG001))→{_decoupled_decay::_compute_decay_factor},_default_clip_lambda((step: Number | Tensor)),__init__((# noqa: C901,PLR0913 self,params: ParamsT,lr: float | Tensor = 1e-3,betas: tuple[float,float] = (0.9,0.9999),eps: float = 1e-6,clip_lambda: (Callable[[Number | Tensor | Any],float] | None) = _default_clip_lambda,weight_decay: float = 0.0,*,decouple: bool = False,foreach: bool | None = None,maximize: bool = False,capturable: bool = False,differentiable: bool = False,fused: bool | None = None,))[CTOR,DUNDER],__setstate__((self,state: dict))[DUNDER],_init_group((# noqa: PLR0913 self,group: dict,params_with_grad: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],state_steps: list[Tensor],)),_fused_adopt((# noqa: PLR0913 params: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | None,has_complex: bool,beta1: float,beta2: float,lr: float | Tensor,clip_lambda: Callable[[int],float] | None,weight_decay: float,decouple: bool,eps: float,maximize: bool,capturable: bool,differentiable: bool,))] aggmo_adamw.py→[report_per_parameter_metrics((# noqa: D102 self,param: torch.Tensor,name: str,optimizer_metrics: dict[str,torch.Tensor],))→{_decoupled_decay::_compute_decay_factor,aggmo_adopt::_sum_weights,aggmo_adopt::_build_moment_specs},_validate_vs_tuple((self,vs: Sequence[float]))→{aggmo_adopt::_sum_weights,aggmo_adopt::_build_moment_specs},_single_tensor_aggmo_qhadamw((# noqa: C901,PLR0913,PLR0912 params: list[Tensor],grads: list[Tensor],moment_buffers: list[list[Tensor]],exp_avg_sqs: list[Tensor],max_exp_avg_sqs: list[Tensor] | None,state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | Tensor | None,decouple: bool,amsgrad: bool,beta1s: Sequence[float],beta2: float,vs: Sequence[float],lr: float | Tensor,weight_decay: float,eps: float,maximize: bool,capturable: bool,differentiable: bool,has_complex: bool,# noqa: ARG001 grad_coeff: float,))→{_decoupled_decay::_compute_decay_factor,aggmo_adopt::_build_moment_specs},_validate_betas_tuple((self,betas: Sequence[float],vs: Sequence[float]))→{aggmo_adopt::_build_moment_specs},_setup_metric_functions((self,vs: Sequence[float]))→{aggmo_adopt::_build_moment_specs},_prepare_param_state((# noqa: C901 self,group: dict[str,Any],param: Tensor,moment_specs: Sequence[tuple[float,str]],))→
{aggmo_adopt::_is_moment_key},__init__((# noqa: PLR0913 self,params: ParamsT,lr: float | Tensor = 1e-3,betas: tuple[float,...] = (0.9,0.95),vs: tuple[float,...] = (0.7,),eps: float = 1e-8,weight_decay: float = 1e-5,*,amsgrad: bool = False,decouple: bool = True,foreach: bool | None = None,maximize: bool = False,capturable: bool = False,differentiable: bool = False,fused: bool | None = None,))[CTOR,DUNDER],_validate_param_groups((self))] aggmo_adopt.py→[report_per_parameter_metrics((# noqa: D102 self,param: torch.Tensor,name: str,optimizer_metrics: dict[str,torch.Tensor],))→{_sum_weights,_build_moment_specs,_decoupled_decay::_compute_decay_factor},_validate_vs_tuple((self,vs: Sequence[float]))→{_sum_weights,_build_moment_specs},_single_tensor_aggmo_qhadopt((# noqa: C901,PLR0913 params: list[Tensor],grads: list[Tensor],moment_buffers: list[list[Tensor]],exp_avg_sqs: list[Tensor],state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | Tensor | None,decouple: bool,clip_lambda: Callable[[Number | Tensor | Any],float] | None,beta1s: Sequence[float],beta2: float,vs: Sequence[float],lr: float | Tensor,weight_decay: float,eps: float,maximize: bool,capturable: bool,differentiable: bool,has_complex: bool,# noqa: ARG001 grad_coeff: float,))→{_build_moment_specs,_decoupled_decay::_compute_decay_factor},_validate_betas_tuple((self,betas: Sequence[float],vs: Sequence[float]))→{_build_moment_specs},_setup_metric_functions((self,vs: Sequence[float]))→{_build_moment_specs},_prepare_param_state((self,group: dict[str,Any],param: Tensor,moment_specs: Sequence[tuple[float,str]],))→{_is_moment_key},_build_moment_specs((vs: Sequence[float]))[HOT],_is_moment_key((key: str)),_sum_weights((moment_specs: Iterable[tuple[float,str]])),__init__((# noqa: PLR0913 self,params: ParamsT,lr: float | Tensor = 1e-3,betas: tuple[float,...] = (0.999,0.9999),vs: tuple[float,...] = (0.9,),eps: float = 1e-6,clip_lambda: (Callable[[Number | Tensor | Any],float] | None) = _default_clip_lambda,weight_decay: float = 0.0,*,decouple: bool = False,foreach: bool | None = None,maximize: bool = False,capturable: bool = False,differentiable: bool = False,fused: bool | None = None,))[CTOR,DUNDER],_validate_param_groups((self))] callbacks.py→[setup((self,context: CallbackSetupContext)),on_step_end((self,context: CallbackStepContext)),on_validation_end((self,context: CallbackValidationContext)),close((self))] components.py→[build_metrics_processor((job_config: JobConfig,parallel_dims: ParallelDims,model_args: BaseModelArgs | None = None,# noqa: ARG001 tag: str | None = None,))[HOT]→{metrics::get_or_create_unigram_manager}] config.py→[_coerce_nested_dataclass((value: Any,cls: type[T],*,factory: Callable[[Mapping[str,Any]],T] | None = None,))→{metrics::close,metrics::close},_as_dict((value: Mapping[str,Any] | None))→{desloc::tick}] dataloader.py→[_build_mosaic_dataloader((request: DataloaderBuildRequest,*,register_unigram_metric: Callable[[PureUnigramCrossEntropy],None] | None = None,))[HOT]→{_apply_split_overrides,unigram::setup_unigram_metric,metrics::get_or_create_unigram_manager,dataset_factory::build_dataset_for_rank,desloc::tick,streams::_extract_streams},build_mosaic_dataloader((*,dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer,job_config: MosaicJobConfig,register_unigram_metric: Callable[[PureUnigramCrossEntropy],None] | None = None,))[HOT]→{_build_mosaic_dataloader},build_mosaic_validation_dataloader((# noqa: PLR0913 *,dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer,job_config: MosaicJobConfig,infinite: bool = False,# noqa: ARG001 - kept for compatibility register_unigram_metric: Callable[[PureUnigramCrossEntropy],None] | None = None,))[HOT]→{_build_mosaic_dataloader},_apply_split_overrides((normalized: NormalizedMosaicConfig,*,job_config: MosaicJobConfig,split: str))] dataset_factory.py→
[build_dataset_for_rank((# noqa: PLR0913 normalized: NormalizedMosaicConfig,extraction: StreamExtractionResult,*,dp_rank: int,dp_world_size: int,tokenizer: BaseTokenizer,split: str,))[HOT]→{create_streaming_dataset,_prepare_dataset_kwargs,streams::_select_stream_subset},_select_dataset_config((dataset_cfg: Mapping[str,Any] | None,split: str))→{desloc::tick},_normalize_mosaic_dataloader_config((job_config: MosaicJobConfig,*,split: str,default_drop_last: bool,))→{_select_dataset_config},_prepare_dataset_kwargs((dataset_cfg: dict[str,Any],*,dataset_split_remote: str | None,)),create_streaming_dataset((*,assignment: StreamAssignment,tokenizer: BaseTokenizer,dataset_config: DatasetFactoryConfig,batch_size: int,split: str,))] decoupled_adamw.py→[report_per_parameter_metrics((self,param: torch.Tensor,name: str,optimizer_metrics: dict[str,torch.Tensor],))→{_decoupled_decay::_compute_decay_factor},__init__((# noqa: PLR0913 self,params: Iterable[torch.Tensor] | Iterable[dict],lr: float = 1e-3,betas: tuple[float,float] = (0.9,0.95),eps: float = 1e-8,weight_decay: float = 1e-5,*,amsgrad: bool = False,decouple: bool = True,foreach: bool | None = None,maximize: bool = False,capturable: bool = False,differentiable: bool = False,fused: bool | None = None,))[CTOR,DUNDER]] desloc.py→[_init_backup_storage((self))→{_extract_local_tensor},save_state((self))→{_extract_local_tensor},restore_state((self))→{_copy_into_tensor},prepare_sync((self))→{_extract_local_tensor},perform_sync((self))→{_copy_into_tensor},__init__((self,config: OptimizerFragmentConfig))[CTOR,DUNDER]→{tick},__init__((self,config: DesLocControllerConfig))[CTOR,DUNDER]→{train::main},_lazy_init_optimizer_fragments((self))→{metrics::setup},_extract_local_tensor((tensor: torch.Tensor)),_copy_into_tensor((param: torch.Tensor,value: torch.Tensor)),__init__((self,sync_every: int))[CTOR,DUNDER],tick((self)),reset((self)),prepare_sync((self)),perform_sync((self)),save_state((self)),restore_state((self)),__init__((self,config: ParameterFragmentConfig))[CTOR,DUNDER],register_state_dict_fn((self)),_init_backup_storage((self)),save_state((self)),restore_state((self)),prepare_sync((self)),perform_sync((self)),register_state_dict_fn((self)),close((self)),_resolve_optimizer_sync_intervals((self,state_keys: Iterable[str]))[HOT],_expand_single_interval((self,interval: int,keys: list[str])),_expand_list_intervals((self,intervals: list[int],keys: list[str])),_expand_dict_intervals((self,mapping: dict[str,int],keys: list[str])),_validate_positive_interval((self,value: int)),_step_post_hook((self,_optimizer: Optimizer,_args: tuple[Any,...],_kwargs: dict[str,Any],)),_sync((self,fragments: list[_BaseFragment])),_prepare_sync((self,fragments: list[_BaseFragment])),_perform_sync((self,fragments: list[_BaseFragment])),__init__((self,config: DesLocFTOptimizersConfig))[CTOR,DUNDER],close_desloc((self))] ft_override.py→[] lr_scheduler.py→[_linear_warmup_stable_decay((current_step: int,*,params: WarmupStableDecayParams)),build_fl_lr_schedulers((optimizers: OptimizersContainer,lr_scheduler_config: FLLRSchedulerConfig,training_steps: int,))[HOT]] metrics.py→[_add_metrics((# noqa: C901,PLR0912,PLR0915 self,suffix: str,value: torch.Tensor))→{compute_kurtosis,compute_skewness},on_step_end((# noqa: C901,PLR0912,PLR0915 self,context: CallbackStepContext,))→{train::main,train::main},__init__((self,*,interval: int = 25,ignore_module_types: Sequence[str] | None = None,gradient_accumulation_steps: int = 1,enabled_metrics: set[str] | None = None,))[CTOR,DUNDER]→{train::main},_prepare_local_metrics((self))→{train::main},on_step_end((self,context: CallbackStepContext))→{desloc::tick},__init__((self,params: HyperparameterSwitchParams))[CTOR,DUNDER]→{setup},log((self,step: int,global_avg_loss: float,global_max_loss: float,grad_norm: float,extra_metrics: dict[str,Any] | None = None,))→
{desloc::tick},__init__((self,unigram_probabilities: Tensor,ignore_index: int = -100,*,dist_sync_on_step: bool = False,))[CTOR,DUNDER],update((self,output: Mapping | Tensor,target: Tensor)),compute((self))[HOT],__init__((self,manager: UnigramMetricManager,metric: PureUnigramCrossEntropy))[CTOR,DUNDER],close((self)),__enter__((self))[DUNDER],__exit__((self,exc_type: type[BaseException] | None,exc: BaseException | None,traceback: TracebackType | None,))[DUNDER],__init__((self))[CTOR,DUNDER],register((self,metric: PureUnigramCrossEntropy,group_key: str | None = None)),unregister((self,metric: PureUnigramCrossEntropy)),collect((self,*,reset: bool = True)),reset((self)),update((self,labels: Tensor)),clear((self)),has_metrics((self)),get_or_create_unigram_manager((job_config: Any)),compute_skewness((value: torch.Tensor))[HOT],compute_kurtosis((value: torch.Tensor))[HOT],setup((self,context: CallbackSetupContext)),on_step_end((self,context: CallbackStepContext)),close((self)),_is_metric_enabled((self,metric_key: str)),should_log_step((self,step: int)),register((self,model: torch.nn.Module)),_register_forward_hook((self,module: torch.nn.Module)),_forward_pre_hook((self,module: torch.nn.Module,args: tuple[Any,...],kwargs: dict[str,Any],)),_forward_hook((self,module: torch.nn.Module,inputs: tuple[Any,...],output: Any,)),_recursively_add_metrics((self,suffix: str,values: Any)),finalize((self,step: int,logger: Any,mesh: DeviceMesh | None,)),_reduce_metrics((# noqa: C901,PLR0912 self,metrics: dict[str,float | list[float]],mesh: DeviceMesh | None)),_reset_metrics((self)),__init__((self,interval: int = 10,*,only_global: bool = True,log_optimizer_metrics: bool = True,))[CTOR,DUNDER],setup((self,context: CallbackSetupContext)),_reduce_metrics_across_ranks((self,optimizer_metrics: dict[str,torch.Tensor],mesh: DeviceMesh)),__init__((self,*,interval: int = 1,enabled: bool = True))[CTOR,DUNDER],on_step_end((self,context: CallbackStepContext)),__init__((self,*,interval: int = 0,enabled: bool = False))[CTOR,DUNDER],_should_log((self,context: CallbackStepContext)),_collect_metrics((self,optimizers: Sequence[torch.optim.Optimizer])),_iter_param_groups((self,optimizers: Sequence[torch.optim.Optimizer])),_iter_values((self,betas: Any)),_as_float((self,value: Any)),__init__((self,*,interval: int = 0,enabled: bool = False))[CTOR,DUNDER],on_step_end((self,context: CallbackStepContext)),on_step_end((self,context: CallbackStepContext)),_should_apply((self,step: int)),_apply_switches((self,optimizers: Sequence[Optimizer])),_log_switch_metrics((self,context: CallbackStepContext)),_update_group_values((self,param_groups: list[dict[str,Any]],key: str,values: tuple[float,...])),_reset_momenta((self,optimizer_state: dict[Any,dict[str,Any]])),_zero_state_value((self,value: Any)),__init__((# noqa: PLR0913 - initializer wires multiple optional dependencies self,job_config: Any,parallel_dims: Any,metrics_config: MetricsConfig | None = None,*,unigram_manager: UnigramMetricManager | None = None,callbacks: Sequence[Callback] | None = None,tag: str | None = None,))[CTOR,DUNDER],_build_callbacks_from_config((self,metrics_config: MetricsConfig))[HOT],_assign_known_callbacks((self,callbacks: Sequence[Callback])),_init_optimizer_monitor((self,optimizer_config: OptimizerMonitorConfig)),_init_activation_monitor((self,activation_config: ActivationMonitorConfig)),_init_lr_monitor((self,lr_config: LRMonitorConfig)),_init_betas_monitor((self,betas_config: BetasMonitorConfig)),_init_vs_monitor((self,vs_config: VSMonitorConfig)),_init_hyperparameter_switch((self,hyper_switch_config: HyperparameterSwitchConfig)),should_log((self,step: int)),_build_unigram_payload((self,mesh: DeviceMesh | None))[HOT],update_unigram_metrics((self,labels: Tensor)),_ensure_callbacks_setup((self)),_run_step_callbacks((self,step: int,mesh: DeviceMesh | None)),_run_validation_callbacks((self,loss: float,step: int)),log_validation((self,loss: float,step: int)),close((self))] optimizer_builder.py→
[build_mosaic_optimizers((model_parts: list[torch.nn.Module],optimizer_config: MosaicOptimizerConfig | dict[str,Any],parallel_dims: ParallelDims,ft_manager: FTManager | None = None,param_groups: list[dict[str,Any]] | None = None,))[HOT]→{_build_optimizer_container,_build_optimizer_kwargs,_resolve_optimizer_class,_apply_mup_overrides,_normalize_mosaic_optimizer_config},_build_optimizer_container((request: OptimizerContainerRequest,))[HOT]→{_build_desloc_container,_validate_optim_in_backward},_resolve_optimizer_class((name: str))[HOT],_normalize_mosaic_optimizer_config((optimizer_config: MosaicOptimizerConfig | dict[str,Any],)),_build_optimizer_kwargs((config: MosaicOptimizerConfig,extra_kwargs: dict[str,Any]))[HOT],_apply_mup_overrides((model_parts: list[torch.nn.Module],config: MosaicOptimizerConfig,param_groups: list[dict[str,Any]] | None,)),_build_desloc_container((request: DeslocContainerRequest))[HOT],_validate_optim_in_backward((request: OptimizerContainerRequest))] optimizers.py→[] parallel.py→[__init__((self,*args: Any,**kwargs: Any))[CTOR,DUNDER],__getitem__((self,idx: int))[DUNDER],state_dict((self,num_samples: int | None = None,*,from_beginning: bool = True)),load_state_dict((self,obj: dict[str,Any])),__init__((self,dataset: StatefulStreamingTextDataset,request: ParallelDataLoaderRequest))[CTOR,DUNDER],state_dict((self)),load_state_dict((self,state_dict: dict[str,Any])),close((self)),titan_collate_fn((batch: list[Any]))] qhadamw.py→[report_per_parameter_metrics((self,param: torch.Tensor,name: str,optimizer_metrics: dict[str,torch.Tensor],))→{_decoupled_decay::_compute_decay_factor},_single_tensor_qhadamw((# noqa: PLR0913 params: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],max_exp_avg_sqs: list[Tensor],state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | None,decouple: bool,amsgrad: bool,beta1: float,beta2: float,v1: float,lr: float | Tensor,weight_decay: float,eps: float,maximize: bool,capturable: bool,differentiable: bool,has_complex: bool,# noqa: ARG001))→{_decoupled_decay::_compute_decay_factor},_multi_tensor_qhadamw((# noqa: C901,PLR0913,PLR0912,PLR0915 params: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],max_exp_avg_sqs: list[Tensor],state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | None,has_complex: bool,amsgrad: bool,beta1: float,beta2: float,v1: float,lr: float | Tensor,weight_decay: float,decouple: bool,eps: float,maximize: bool,capturable: bool,differentiable: bool,))→{_decoupled_decay::_compute_decay_factor},__init__((# noqa: PLR0913,PLR0912,C901 self,params: ParamsT,lr: float | Tensor = 1e-3,betas: tuple[float,float] = (0.9,0.95),vs: tuple[float,...] = (0.7,),eps: float = 1e-8,weight_decay: float = 1e-5,*,amsgrad: bool = False,decouple: bool = True,foreach: bool | None = None,maximize: bool = False,capturable: bool = False,differentiable: bool = False,fused: bool | None = None,))[CTOR,DUNDER],__setstate__((self,state: dict))[DUNDER],_init_group((# noqa: PLR0913 self,group: dict,params_with_grad: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],max_exp_avg_sqs: list[Tensor],state_steps: list[Tensor],)),_fused_qhadamw((*args: Any,**kwargs: Any,))] qhadopt.py→[_multi_tensor_qhadopt((# noqa: C901,PLR0912,PLR0913,PLR0915 params: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | None,has_complex: bool,beta1: float,beta2: float,v1: float,lr: float | Tensor,clip_lambda: Callable[[Number | Tensor | Any],float] | None,weight_decay: float,decouple: bool,eps: float,maximize: bool,capturable: bool,differentiable: bool,))→
{_decoupled_decay::_compute_decay_factor,_decoupled_decay::_compute_decay_factor},report_per_parameter_metrics((self,param: torch.Tensor,name: str,optimizer_metrics: dict[str,torch.Tensor],))→{_decoupled_decay::_compute_decay_factor},_single_tensor_qhadopt((# noqa: PLR0913 params: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | None,decouple: bool,clip_lambda: Callable[[Number | Tensor | Any],float] | None,beta1: float,beta2: float,v1: float,lr: float | Tensor,weight_decay: float,eps: float,maximize: bool,capturable: bool,differentiable: bool,has_complex: bool,# noqa: ARG001))→{_decoupled_decay::_compute_decay_factor},_default_clip_lambda((step: Number | Tensor)),__init__((# noqa: C901,PLR0913,PLR0912 self,params: ParamsT,lr: float | Tensor = 1e-3,betas: tuple[float,float] = (0.999,0.9999),vs: tuple[float,...] = (0.9,),eps: float = 1e-6,clip_lambda: (Callable[[Number | Tensor | Any],float] | None) = _default_clip_lambda,weight_decay: float = 0.0,*,decouple: bool = False,foreach: bool | None = None,maximize: bool = False,capturable: bool = False,differentiable: bool = False,fused: bool | None = None,))[CTOR,DUNDER],__setstate__((self,state: dict))[DUNDER],_init_group((# noqa: PLR0913 self,group: dict,params_with_grad: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],state_steps: list[Tensor],)),_fused_qhadopt((# noqa: PLR0913 params: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | None,has_complex: bool,beta1: float,beta2: float,v1: float,lr: float | Tensor,clip_lambda: Callable[[int],float] | None,weight_decay: float,decouple: bool,eps: float,maximize: bool,capturable: bool,differentiable: bool,))] s3_checkpoint.py→[__init__((self,checkpointer: CheckpointManager,config: S3CheckpointingConfig,job_config: MosaicJobConfig,*,enable_uploads: bool = True,))[CTOR,DUNDER]→{create_remote_up_down,metrics::setup,metrics::setup,metrics::setup},_download_step((self,step: int,remote_path: str))→{download_file_from_s3,download_file_from_s3,train::main},_upload_step((self,step: int,directory: Path))→{upload_file_to_s3},_write_latest_marker((self,step: int))→{upload_file_to_s3},_read_remote_latest_step((self,remote_root: str | None = None))→{download_file_from_s3},setup_s3_checkpointing((checkpointer: CheckpointManager,job_config: MosaicJobConfig,*,install: bool = True,))→{get_s3_checkpoint_wrapper_factory},download_file_from_s3((remote_up_down: RemoteUploaderDownloader,remote_file_name: str,local_file_name: Path | str,)),upload_file_to_s3((remote_up_down: RemoteUploaderDownloader,remote_file_name: str,local_file_name: Path,)),create_remote_up_down((# noqa: PLR0913 bucket_name: str,prefix: str,num_attempts: int,client_config: dict[str,Any],*,num_concurrent_uploads: int = 1,upload_staging_folder: str | None = None,use_procs: bool = True,)),__getattr__((self,name: str))[DUNDER],attach_to_trainer((self,trainer: Any)),install_onto_checkpointer((self)),_start_remote_workers((self)),__del__((self))[DUNDER],download_if_needed((self)),close((self)),_wait_for_staging_with_logging((self)),_resolve_remote_root((self))[HOT],_checkpoint_dir((self,step: int)),_raise_invalid_resume_format((self,cause: Exception | None = None)),_remote_key((self,relative_path: Path,remote_root: str | None = None)),save((self,curr_step: int,*,last_step: bool = False)),maybe_wait_for_staging((self)),_process_pending((self,flush: bool = False)),_is_directory_ready_for_upload((self,directory: Path)),_iter_checkpoint_files((self,directory: Path)),_find_local_latest_step((self)),get_s3_checkpoint_wrapper_factory((job_config: MosaicJobConfig,))] streams.py→[_extract_streams((dataset_cfg: dict[str,Any]))→
{_compute_group_indices,_materialize_streams,_resolve_group_stream_names,_aggregate_sampling_groups,_should_concat_sampling_groups,_normalize_sampling_mode},_resolve_group_candidate((entry: Any,*,flattened: dict[str,dict[str,Any]],group: Mapping[str,Any],entry_index: int,))[HOT]→{desloc::tick,desloc::tick,desloc::tick,desloc::tick},_aggregate_sampling_groups((flattened: dict[str,dict[str,Any]],sampling_groups_cfg: Any,mode_raw: Any,))→{_resolve_group_candidate,_collect_group_stream_entries,_normalize_sampling_groups,metrics::setup},_resolve_group_stream_names((flattened: dict[str,dict[str,Any]],sampling_groups_cfg: Any))[HOT]→{_collect_group_stream_entries,_normalize_sampling_groups,desloc::tick},_materialize_streams((flattened: dict[str,dict[str,Any]],*,root_remote: str | None,root_local: str | None,))→{_join_local_path,_join_remote_path,desloc::tick},_join_remote_path((root: str | None,path: str | None))→{_is_uri,_is_uri},_normalize_sampling_groups((config: Any))→{desloc::tick},_collect_group_stream_entries((group: Mapping[str,Any]))→{_flatten_stream_configs},_is_uri((path: str | None)),_join_local_path((root: str | None,path: str | None)),_flatten_stream_configs((streams_cfg: Any)),_normalize_sampling_mode((mode_raw: Any)),_should_concat_sampling_groups((mode: str,sampling_groups_cfg: Any)),_compute_group_indices((group_stream_names: list[list[str]] | None,stream_names: list[str]))[HOT],_select_stream_subset((extraction: StreamExtractionResult,*,dp_rank: int,dp_world_size: int,))] tokenizer.py→[build_mosaic_tokenizer((job_config: MosaicJobConfig,))[HOT]→{desloc::tick}] train.py→[main(void)[ENTRY]→{s3_checkpoint::setup_s3_checkpointing,s3_checkpoint::setup_s3_checkpointing,s3_checkpoint::get_s3_checkpoint_wrapper_factory}] unigram.py→[_maybe_download_unigram_file((remote_uri: str | None,root_remote: str | None,split: str,destination: Path,config: UnigramMetricConfig,))→{_create_remote_unigram_client,_resolve_unigram_remote_path,s3_checkpoint::download_file_from_s3},_load_stream_unigram_counts((stream: Stream,*,root_remote: str | None,dataset_split: str | None,default_split: str,config: UnigramMetricConfig,))→{_materialize_split_cache,_resolve_unigram_cache_path},_build_unigram_metric_for_group((context: UnigramMetricContext,))[HOT]→{_load_stream_unigram_counts,train::main},setup_unigram_metric((# noqa: PLR0913 assignment: StreamAssignment,*,job_config: MosaicJobConfig,split: str,tokenizer: BaseTokenizer,collate_fn: Callable,manager: UnigramMetricManager | None = None,register_unigram_metric: Callable[[PureUnigramCrossEntropy],None] | None = None,))→{_build_unigram_metric_for_group,metrics::get_or_create_unigram_manager},_create_remote_unigram_client((bucket: str,config: UnigramMetricConfig))→{s3_checkpoint::create_remote_up_down},_resolve_unigram_cache_path((stream: Stream,*,root_remote: str | None,dataset_split: str | None,default_split: str,config: UnigramMetricConfig,))[HOT]→{_maybe_download_unigram_file},_resolve_unigram_remote_path((remote_uri: str,*,root_remote: str | None,split: str,))[HOT],_materialize_split_cache((cache_path: Path,split_path: Path))] validate.py→[__init__((self,request: MosaicValidatorRequest))[CTOR,DUNDER]→{dataloader::build_mosaic_validation_dataloader},build_mosaic_validator((**kwargs: Any))[HOT]] 

## DEPENDENCY_PATTERNS

### EDGE_PATTERNS
Contains: 211 edges
Call: 131 edges

### CROSS_CLUSTER_FLOW
DATA_MODELS→UTILITY_LAYER: 3


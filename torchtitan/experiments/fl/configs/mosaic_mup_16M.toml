[job]
dump_folder = "./outputs"
description = "Llama 3 debug training with Mosaic streaming"
print_args = false

[profiling]
enable_profiling = false
save_traces_folder = "profile_trace"
profile_freq = 10
enable_memory_snapshot = false
save_memory_snapshot_folder = "memory_snapshot"

[metrics]
log_freq = 1
disable_color_printing = false
enable_tensorboard = false
save_tb_folder = "tb"
enable_wandb = true
save_for_all_ranks = true

[model]
name = "mosaic_llama3_mup"
flavor = "16M"
# test folder with tokenizer.json, for debug purpose only
hf_assets_path = "./tests/assets/tokenizer"
# converters = ["float8"]


[optimizer]
name = "ADOPT"
lr = 8e-4
eps = 1e-8
betas = [0.9,0.99]
vs = [0.98]
implementation = "foreach"

[fl_metrics.optimizer_monitor]
interval = 1
only_global = true
log_metrics = true

[fl_metrics.activation_monitor]
interval = 1
ignore_module_types = ["dropout", "ln"]

[lr_scheduler]
warmup_steps = 2  # lr scheduler warm up, normally 20% of the train steps
decay_ratio = 0.8  # lr scheduler decay ratio, 80% of the train steps
decay_type = "linear"
min_lr_factor = 0.0

[training]
local_batch_size = 8
seq_len = 2048
max_norm = 1.0  # grad norm clipping
steps = 20
dataset = "c4_test"  # supported datasets: c4_test (2K), c4 (177M)

# Mosaic-specific configurations are now at the root level
[mosaic_dataloader]
name = "text"
num_workers = 8
prefetch_factor = 2
pin_memory = true
persistent_workers = true

[mosaic_dataloader.dataset.common]
max_seq_len = 2048
download_retry = 2
download_timeout = 60
keep_zip = false
partition_algo = "relaxed"
shuffle = true
shuffle_algo = "py1e"
shuffle_seed = 9176
sampling_method = "balanced"
sampling_granularity = 1
batching_method = "random"

[mosaic_dataloader.dataset.train]
split = "train"
root_remote = "s3://smollm-corpus/shared"
root_local = "/nfs-share/datasets/photon/dataset_cache/smollm-corpus-shared"

[mosaic_dataloader.dataset.train.streams.client_streams.stream_0]
local = "fineweb_edu_dedup/client_0"
remote = "fineweb_edu_dedup/client_0"
proportion = 70


[mosaic_dataloader.dataset.val]
# The validation samples are stored under the "train" split on disk.
split = "train"
root_remote = "s3://smollm-corpus/shared"
root_local = "/nfs-share/datasets/photon/dataset_cache/smollm-corpus-shared-val"
subset_num_samples = 512

[mosaic_dataloader.dataset.val.streams.client_streams.stream_0]
local = "fineweb_edu_dedup/client_0"
remote = "fineweb_edu_dedup/client_0"
proportion = 70

[mosaic_tokenizer]
name = "HuggingFaceTB/SmolLM-1.7B"

[mosaic_tokenizer.kwargs]
model_max_length = 2048

[parallelism]
data_parallel_replicate_degree = 1
data_parallel_shard_degree = -1
fsdp_reshard_after_forward = "default" # default / never / always
tensor_parallel_degree = 1
enable_async_tensor_parallel = false
pipeline_parallel_degree = 1
context_parallel_degree = 1

[checkpoint]
enable = true
folder = "checkpoints"
interval = 10
last_save_model_only = false
export_dtype = "float32"
async_mode = "async_with_pinned_mem"  # ["disabled", "async", "async_with_pinned_mem"]

[s3_checkpoint]
enable = true
bucket = "checkpoints"
prefix = ""  # Root of bucket
download_on_start = true
# run_uuid and remote_checkpoint_folder will be set via RUN_UUID environment variable

[activation_checkpoint]
mode = "selective"  # ["none", "selective", "full"]
selective_ac_option = '2'  # 'int' = ac every positive int layer or 'op', ac based on ops policy

[compile]
enable=false
components = ["model", "loss"]

[quantize.linear.float8]
enable_fsdp_float8_all_gather = false
precompute_float8_dynamic_scale_for_fsdp = false
filter_fqns = ["output"]


[validation]
enable = true
dataset = "c4_validation"
freq = 5
steps = 32

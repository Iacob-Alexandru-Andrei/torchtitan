# EMBARGO: LLM-Optimized Codebase Dependency Graph

**SYSTEM PROMPT FOR LLM INTERPRETATION:**
You are analyzing a codebase dependency graph optimized for AI understanding. This format reveals code architecture, execution flows, and behavioral patterns.

## INTERPRETATION KEY

### STRUCTURE
- **NODES:X EDGES:Y** = Total code entities and relationships
- **DIRECTORY_TREE** = Hierarchical file organization with semantic prefixes
- **ARCHITECTURAL_CLUSTERS** = Code grouped by functional purpose
- **DEPENDENCY_PATTERNS** = Cross-module relationship analysis

### BEHAVIORAL NOTATION
- **filename.rs→[...]** = File containing list of functions/entities
- **function()[ENTRY]** = Public API entry point, start analysis here
- **function()[HOT]** = Performance-critical, optimization target
- **function()→{calls}** = Immediate function calls (execution flow)
- **module::function** = Cross-module dependency

### ANALYSIS GUIDANCE
1. **Entry Points**: Start with [ENTRY] functions to understand public APIs
2. **Execution Flow**: Follow →{calls} to trace code execution paths
3. **Hot Paths**: Focus [HOT] functions for performance analysis
4. **Architecture**: Use clusters to understand system organization
5. **Dependencies**: Cross-cluster flows show coupling patterns

### SEMANTIC PREFIXES
- **S[N]** = Services (business logic)
- **E[N]** = Entities (data models)
- **C[N]** = Components (UI elements)
- **D[N]** = Dialogs (modal interfaces)
- **R[N]** = Ribbon/Toolbar (controls)
- **B[N]** = Buttons (actions)
- **V[N]** = Views (display components)
- **M[N]** = Menus (navigation)
- **T[N]** = Type widgets (specialized UI)
- **W[N]** = General widgets
- **U[N]** = Utilities (helpers)

### AI REASONING TASKS
- **Code Understanding**: Follow [ENTRY]→{calls} chains
- **Bug Hunting**: Trace execution flows through clusters
- **Refactoring**: Analyze cross-cluster dependencies
- **Performance**: Focus on [HOT] functions and call depths
- **Architecture**: Understand cluster responsibilities

---

# CODE_GRAPH
NODES:1955 EDGES:997

## DIRECTORY_TREE
ROOT: torchtitan/experiments/
├─ deepseek_v3/ → U[25]
│  ├─ infra/ → U[1]
│  ├─ symm_mem_recipes/ → U[4]
│  ├─ tokenizers/ → U[1]
│  ├─ train_configs/ → U[1]
│  └─ unit_testing/ → U[4]
├─ fl/ → E[13] U[24]
│  ├─ configs/ → U[3]
│  ├─ dataloader/ → U[2]
│  ├─ models/ → E[13]
│  │  ├─ llama3_mup/ → E[9]
│  │  │  ├─ infra/ → E[2]
│  │  │  ├─ model/ → E[4]
│  │  │  ├─ tests/ → E[1]
│  │  │  └─ train_configs/ → E[1]
│  │  ├─ mosaic_llama3/ → E[1]
│  │  └─ mosaic_llama3_mup/ → E[1]
│  └─ optimizers/ → U[8]
├─ flux/ → TST[3] U[19]
│  ├─ dataset/ → U[2]
│  ├─ inference/ → U[1]
│  ├─ infra/ → U[1]
│  ├─ model/ → U[7]
│  ├─ scripts/ → U[1]
│  └─ tests/ → TST[3]
│     ├─ assets/ → TST[1]
│     │  └─ cc12m_test/ → TST[1]
│     └─ unit_tests/ → TST[1]
├─ forge/ → U[5]
├─ kernels/ → TST[1] U[18]
│  ├─ moe/ → TST[1] U[3]
│  │  └─ unit_tests/ → TST[1]
│  ├─ triton_contiguous_group_gemm/ → U[6]
│  └─ triton_mg_group_gemm/ → U[9]
│     └─ torchao_pr/ → U[7]
├─ llama4/ → U[7]
│  ├─ infra/ → U[1]
│  ├─ model/ → U[3]
│  └─ scripts/ → U[2]
├─ multimodal/ → TST[2] U[8]
│  ├─ tests/ → TST[2]
│  └─ tokenizer/ → U[1]
├─ qwen3/ → U[5]
│  ├─ infra/ → U[1]
│  └─ model/ → U[3]
├─ simple_fsdp/ → TST[2] U[7]
│  ├─ deepseek_v3/ → U[3]
│  ├─ llama3/ → U[3]
│  └─ tests/ → TST[2]
└─ vlm/ → U[11]
   ├─ assets/ → U[1]
   ├─ datasets/ → U[5]
   │  └─ utils/ → U[3]
   ├─ infra/ → U[1]
   └─ model/ → U[3]

## ARCHITECTURAL_CLUSTERS

### DATA_MODELS
NODES:108 CALL_DEPTH:4

__init__.py→[get_train_spec(void)→{utils::ensure_mosaic_spec},build_mosaic_mup_optimizers((model_parts: list[nn.Module],optimizer_config: OptimizerConfig | dict[str,Any],parallel_dims: ParallelDims,ft_manager: FTManager | None = None,))[HOT]→{optimizer_builder::build_mosaic_optimizers},get_train_spec(void)→{utils::ensure_mosaic_spec},_get_llama3_mup_spec(void),_update_vocab_sizes((base_spec: TrainSpec,mosaic_spec: TrainSpec)),build_mup_optimizers((model_parts: list[nn.Module],optimizer_config: OptimizerConfig,parallel_dims: ParallelDims,ft_manager: FTManager | None = None,))[HOT],_update_vocab_sizes((base_spec: TrainSpec,mosaic_spec: TrainSpec)),get_train_spec(void)] mup_args.py→[] mup_model.py→[__init__((self,layer_id: int,model_args: TransformerModelArgs # noqa: ARG002))[CTOR,DUNDER]→{math::attention},_precompute_freqs_cis((self))[HOT]→{model::precompute_freqs_cis},init_weights((self,init_std: float)),init_weights((self,init_std: float)),forward((self,x: torch.Tensor,freqs_cis: torch.Tensor,)),init_weights((self)),__init__((self,model_args: TransformerModelArgs))[CTOR,DUNDER],init_weights((self,buffer_device: torch.device | None = None)),get_optimizer_param_groups((self,optimizer_config: dict[str,Any])),forward((self,tokens: torch.Tensor,input_batch: torch.Tensor | None = None,# noqa: ARG002))] parallelize.py→[parallelize_llama_mup((model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,))→{parallelize::parallelize_llama}] state_dict_adapter.py→[__init__((self,model_args: TransformerModelArgs,hf_assets_path: str | None,))[CTOR,DUNDER]] test_mup_model.py→[setUp((self)),test_model_initialization((self))[TEST],test_forward_pass((self))[TEST],test_weight_initialization((self))[TEST]] utils.py→[ensure_mosaic_spec((base_spec_name: str,*,spec_name: str | None = None,dataloader_fn: DataLoaderBuilder | None = None,tokenizer_fn: TokenizerBuilder | None = None,metrics_processor_fn: MetricsProcessorBuilder | None = None,optimizers_fn: OptimizersBuilder | None = None,validator_fn: ValidatorBuilder | None = None,post_transform: PostTransform | None = None,))→{_build_mosaic_spec,train_spec::register_train_spec,train_spec::register_train_spec,__init__::get_train_spec,__init__::get_train_spec},_build_mosaic_spec((base_spec: TrainSpec,*,spec_name: str,dataloader_fn: DataLoaderBuilder | None = None,tokenizer_fn: TokenizerBuilder | None = None,metrics_processor_fn: MetricsProcessorBuilder | None = None,optimizers_fn: OptimizersBuilder | None = None,validator_fn: ValidatorBuilder | None = None,post_transform: PostTransform | None = None,))[HOT]] 
### TESTS
NODES:89 CALL_DEPTH:5

combine.py→[test_token_combine(void)[TEST]] debug.py→[test_small(void)[TEST]→{debug::verify_results,debug::pytorch_reference,cg_forward::cg_grouped_gemm_forward,debug::create_aligned_test_data},test_medium(void)[TEST]→{debug::verify_results,debug::pytorch_reference,cg_forward::cg_grouped_gemm_forward,debug::create_aligned_test_data},test_large(void)[TEST]→{debug::verify_results,debug::pytorch_reference,cg_forward::cg_grouped_gemm_forward,debug::create_aligned_test_data}] dispatch.py→[test_token_dispatch(void)[TEST]] dsgemm_unit_testing.py→[test_m_grouped_gemm_contiguous_with_empty_groups(void)[TEST]→{dsgemm_unit_testing::compute_reference_with_scaling,dsgemm_utils::get_col_major_tma_aligned_tensor,dsgemm_unit_testing::per_block_cast_to_fp8,dsgemm_unit_testing::per_token_cast_to_fp8,dsgemm_unit_testing::create_m_indices_fast},test_m_grouped_gemm_contiguous_all_empty_but_one(void)[TEST]→{dsgemm_unit_testing::compute_reference_with_scaling,dsgemm_utils::get_col_major_tma_aligned_tensor,dsgemm_unit_testing::per_block_cast_to_fp8,dsgemm_unit_testing::per_token_cast_to_fp8,dsgemm_unit_testing::create_m_indices_fast},test_m_grouped_gemm_contiguous_with_scaling_edge_cases(void)[TEST]→{dsgemm_unit_testing::compute_reference_with_scaling,dsgemm_utils::get_col_major_tma_aligned_tensor,dsgemm_unit_testing::per_block_cast_to_fp8,dsgemm_unit_testing::per_token_cast_to_fp8,dsgemm_unit_testing::create_m_indices_fast}] fast_debug_ao.py→[test_multiple_deepseek_configs(void)[TEST]→{reference_utils::analyze_tensor_differences,reference_utils::analyze_tensor_differences,reference_utils::compute_reference_backward,mg_grouped_gemm::grouped_gemm_backward,reference_utils::analyze_tensor_differences,reference_utils::compute_reference_forward},test_backward_pass(void)[TEST]→{reference_utils::analyze_tensor_differences,reference_utils::analyze_tensor_differences,reference_utils::compute_reference_backward,mg_grouped_gemm::grouped_gemm_backward,mg_grouped_gemm::grouped_gemm_forward},test_forward_pass(void)[TEST]→{reference_utils::analyze_tensor_differences,reference_utils::compute_reference_forward,mg_grouped_gemm::grouped_gemm_forward}] indices.py→[test_with_zero_tokens(void)[TEST]→{indices::generate_permute_indices,indices::generate_permute_indices}] integration_tests.py→[main(void)[ENTRY]→{unit_test_cg::run_tests},run_single_test((test_flavor: OverrideDefinitions,full_path: str,output_dir: str))→{main},run_tests((args,test_list: list[OverrideDefinitions]))→{run_single_test},main(void)→{unit_test_cg::run_tests},build_simple_fsdp_test_list(void)[HOT],build_flux_test_list(void)[HOT]] pack_test_dataset.py→[pack_wds_dataset((tar_destination,source_folder,number_of_samples))→{math::rope,math::rope,math::rope,math::rope}] permute_indices_testing.py→[test_fixed_total_experts_varying_ranks((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_fixed_total_experts_varying_ranks((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_different_block_sizes_with_fixed_experts((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_different_block_sizes_with_fixed_experts((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_edge_cases_with_fixed_experts((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_edge_cases_with_fixed_experts((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_max_blocks_with_large_experts((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_max_blocks_with_large_experts((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_extreme_max_blocks_limit((self))[TEST]→
{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},test_extreme_max_blocks_limit((self))[TEST]→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu}] test_create_m_indices.py→[test_create_indices_from_offsets_nosync(void)[TEST]→{dsgemm_utils::create_indices_from_offsets_nosync,dsgemm_utils::create_indices_from_offsets_nosync}] test_flux_dataloader.py→[test_load_dataset((self))[TEST]→{flux_dataset::build_flux_dataloader,flux_dataset::build_flux_dataloader},setUp((self)),tearDown((self))] test_multimodal_model.py→[test_llama_mm_vision_encoder((self))[TEST]] test_numerics.py→[run_simple_fsdp((self,model,inputs,labels,epoch=20))→{simple_fsdp::data_parallel},init_test((self)),get_input((self)),run_fsdp2((self,model,inputs,labels,epoch=20)),test_replicate_convergence((self))[TEST],test_fullyshard_convergence((self))[TEST],test_hybridshard_convergence((self))[TEST]] test_utils.py→[fixed_init_tensor((shape: torch.Size,min_val: Union[float,int] = 0.0,max_val: Union[float,int] = 1.0,nonlinear: bool = False,dtype: torch.dtype = torch.float,))] unit_test_backwards.py→[test_mg_dx((self))[TEST]→{reference_utils::analyze_tensor_differences,reference_utils::compute_reference_backward,mg_grouped_gemm::grouped_gemm_backward,mg_grouped_gemm::grouped_gemm_forward},test_mg_dw((self))[TEST]→{reference_utils::analyze_tensor_differences,reference_utils::compute_reference_backward,mg_grouped_gemm::grouped_gemm_backward,mg_grouped_gemm::grouped_gemm_forward},test_mg_grouped_gemm_backward_bf16((self))[TEST],test_mg_grouped_gemm_backward_deepseek_shapes((self))[TEST]] unit_test_cg.py→[test_forward_deepseek_shapes((self))[TEST],test_backward_deepseek_shapes((self))[TEST],test_forward_performance_deepseek((self))[TEST],test_backward_performance_deepseek((self))[TEST]] unit_test_forwards.py→[test_mg_grouped_gemm_bf16((self))[TEST],test_mg_grouped_gemm_deepseek_shapes((self))[TEST]] 
### UTILITY_LAYER
NODES:1729 CALL_DEPTH:8

__init__.py→[parse_args((self,args: list[str] = sys.argv[1:]))→{config::ensure_mosaic_job_config_types},get_train_spec(void),get_train_spec(void),__init__((self))[CTOR,DUNDER],load_mosaic_job_config((args: list[str] | None = None)),get_train_spec(void),get_train_spec(void),get_train_spec(void),get_train_spec(void)] _decoupled_decay.py→[_compute_decay_factor((lr: float | Tensor,initial_lr: float | Tensor | None))[HOT]] adopt.py→[_multi_tensor_adopt((# noqa: C901,PLR0912,PLR0913,PLR0915 params: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | None,has_complex: bool,beta1: float,beta2: float,lr: float | Tensor,clip_lambda: Callable[[Number | Tensor | Any],float] | None,weight_decay: float,decouple: bool,eps: float,maximize: bool,capturable: bool,differentiable: bool,))→{_decoupled_decay::_compute_decay_factor,_decoupled_decay::_compute_decay_factor},_default_clip_lambda((step: Number | Tensor))→{math::rope},report_per_parameter_metrics((self,param: torch.Tensor,name: str,optimizer_metrics: dict[str,torch.Tensor],))→{_decoupled_decay::_compute_decay_factor},_single_tensor_adopt((# noqa: PLR0913 params: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | None,decouple: bool,clip_lambda: Callable[[Number | Tensor | Any],float] | None,beta1: float,beta2: float,lr: float | Tensor,weight_decay: float,eps: float,maximize: bool,capturable: bool,differentiable: bool,has_complex: bool,# noqa: ARG001))→{_decoupled_decay::_compute_decay_factor},__init__((# noqa: C901,PLR0913 self,params: ParamsT,lr: float | Tensor = 1e-3,betas: tuple[float,float] = (0.9,0.9999),eps: float = 1e-6,clip_lambda: (Callable[[Number | Tensor | Any],float] | None) = _default_clip_lambda,weight_decay: float = 0.0,*,decouple: bool = False,foreach: bool | None = None,maximize: bool = False,capturable: bool = False,differentiable: bool = False,fused: bool | None = None,))[CTOR,DUNDER],__setstate__((self,state: dict))[DUNDER],_init_group((# noqa: PLR0913 self,group: dict,params_with_grad: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],state_steps: list[Tensor],)),_fused_adopt((# noqa: PLR0913 params: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | None,has_complex: bool,beta1: float,beta2: float,lr: float | Tensor,clip_lambda: Callable[[int],float] | None,weight_decay: float,decouple: bool,eps: float,maximize: bool,capturable: bool,differentiable: bool,))] aggmo_adamw.py→[report_per_parameter_metrics((# noqa: D102 self,param: torch.Tensor,name: str,optimizer_metrics: dict[str,torch.Tensor],))→{_decoupled_decay::_compute_decay_factor,aggmo_adopt::_sum_weights,aggmo_adopt::_build_moment_specs},_validate_vs_tuple((self,vs: Sequence[float]))→{aggmo_adopt::_sum_weights,aggmo_adopt::_build_moment_specs},_single_tensor_aggmo_qhadamw((# noqa: C901,PLR0913,PLR0912 params: list[Tensor],grads: list[Tensor],moment_buffers: list[list[Tensor]],exp_avg_sqs: list[Tensor],max_exp_avg_sqs: list[Tensor] | None,state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | Tensor | None,decouple: bool,amsgrad: bool,beta1s: Sequence[float],beta2: float,vs: Sequence[float],lr: float | Tensor,weight_decay: float,eps: float,maximize: bool,capturable: bool,differentiable: bool,has_complex: bool,# noqa: ARG001 grad_coeff: float,))→{_decoupled_decay::_compute_decay_factor,aggmo_adopt::_build_moment_specs},_validate_betas_tuple((self,betas: Sequence[float],vs: Sequence[float]))→{aggmo_adopt::_build_moment_specs},_setup_metric_functions((self,vs: Sequence[float]))→
{aggmo_adopt::_build_moment_specs},_prepare_param_state((# noqa: C901 self,group: dict[str,Any],param: Tensor,moment_specs: Sequence[tuple[float,str]],))→{aggmo_adopt::_is_moment_key},__init__((# noqa: PLR0913 self,params: ParamsT,lr: float | Tensor = 1e-3,betas: tuple[float,...] = (0.9,0.95),vs: tuple[float,...] = (0.7,),eps: float = 1e-8,weight_decay: float = 1e-5,*,amsgrad: bool = False,decouple: bool = True,foreach: bool | None = None,maximize: bool = False,capturable: bool = False,differentiable: bool = False,fused: bool | None = None,))[CTOR,DUNDER],_validate_param_groups((self))] aggmo_adopt.py→[report_per_parameter_metrics((# noqa: D102 self,param: torch.Tensor,name: str,optimizer_metrics: dict[str,torch.Tensor],))→{_sum_weights,_build_moment_specs,_decoupled_decay::_compute_decay_factor},_validate_vs_tuple((self,vs: Sequence[float]))→{_sum_weights,_build_moment_specs},_single_tensor_aggmo_qhadopt((# noqa: C901,PLR0913 params: list[Tensor],grads: list[Tensor],moment_buffers: list[list[Tensor]],exp_avg_sqs: list[Tensor],state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | Tensor | None,decouple: bool,clip_lambda: Callable[[Number | Tensor | Any],float] | None,beta1s: Sequence[float],beta2: float,vs: Sequence[float],lr: float | Tensor,weight_decay: float,eps: float,maximize: bool,capturable: bool,differentiable: bool,has_complex: bool,# noqa: ARG001 grad_coeff: float,))→{_build_moment_specs,_decoupled_decay::_compute_decay_factor},_validate_betas_tuple((self,betas: Sequence[float],vs: Sequence[float]))→{_build_moment_specs},_setup_metric_functions((self,vs: Sequence[float]))→{_build_moment_specs},_prepare_param_state((self,group: dict[str,Any],param: Tensor,moment_specs: Sequence[tuple[float,str]],))→{_is_moment_key},_build_moment_specs((vs: Sequence[float]))[HOT],_is_moment_key((key: str)),_sum_weights((moment_specs: Iterable[tuple[float,str]])),__init__((# noqa: PLR0913 self,params: ParamsT,lr: float | Tensor = 1e-3,betas: tuple[float,...] = (0.999,0.9999),vs: tuple[float,...] = (0.9,),eps: float = 1e-6,clip_lambda: (Callable[[Number | Tensor | Any],float] | None) = _default_clip_lambda,weight_decay: float = 0.0,*,decouple: bool = False,foreach: bool | None = None,maximize: bool = False,capturable: bool = False,differentiable: bool = False,fused: bool | None = None,))[CTOR,DUNDER],_validate_param_groups((self))] args.py→[] attn_mask_utils.py→[_prepare_4d_causal_attention_mask((attention_mask: Optional[torch.Tensor],input_shape: Union[torch.Size,Tuple,List],inputs_embeds: torch.Tensor,past_key_values_length: int,sliding_window: Optional[int] = None,))] autoencoder.py→[forward((self,x))→{swish,swish},__init__((self,params: AutoEncoderParams))[CTOR,DUNDER]→{tiktoken::decode,tiktoken::encode},forward((self,x: Tensor))→{swish},forward((self,z: Tensor))→{swish},swish((x: Tensor)),__init__((self,in_channels: int))[CTOR,DUNDER],attention((self,h_: Tensor)),forward((self,x: Tensor)),__init__((self,in_channels: int,out_channels: int))[CTOR,DUNDER],__init__((self,in_channels: int))[CTOR,DUNDER],forward((self,x: Tensor)),__init__((self,in_channels: int))[CTOR,DUNDER],forward((self,x: Tensor)),__init__((self,resolution: int,in_channels: int,ch: int,ch_mult: list[int],num_res_blocks: int,z_channels: int,))[CTOR,DUNDER],__init__((self,ch: int,out_ch: int,ch_mult: list[int],num_res_blocks: int,in_channels: int,resolution: int,z_channels: int,))[CTOR,DUNDER],__init__((self,sample: bool = True,chunk_dim: int = 1))[CTOR,DUNDER],forward((self,z: Tensor)),encode((self,x: Tensor)),decode((self,z: Tensor)),forward((self,x: Tensor)),load_ae((ckpt_path: str,autoencoder_params: AutoEncoderParams,device: str | torch.device = "cuda",dtype=torch.bfloat16,random_init=False,))] benchmark.py→[benchmark_model_configs(void)→
{mg_grouped_gemm::grouped_gemm_forward,mg_grouped_gemm::grouped_gemm_forward,reference_utils::compute_reference_forward,reference_utils::compute_reference_forward},compute_reference_forward((x,w,m_sizes))[HOT],plot_benchmark_results((results)),compare_mg_implementations(void)] benchmark_kernels.py→[benchmark_quant_kernels((shapes,dtype=torch.bfloat16,warmup=10,iters=100)),print_results_table((results))] callbacks.py→[setup((self,context: CallbackSetupContext)),on_step_end((self,context: CallbackStepContext)),on_validation_end((self,context: CallbackValidationContext)),close((self))] cg_backward.py→[cg_grouped_gemm_backward_weights((grad_output: torch.Tensor,# [M_total,N] inputs: torch.Tensor,# [M_total,K] expert_indices: torch.Tensor,# [M_total] num_experts: int,group_size_m: int = 128,))→{integration_tests::main,integration_tests::main,integration_tests::main},verify_cg_gemm_backward((M_total=1024,N=512,K=512,num_experts=8,group_size_m=128,device="cuda",atol=1e-1,# Absolute tolerance for validation rtol=1e-1,# Relative tolerance for validation))→{cg_grouped_gemm},cg_grouped_gemm_backward_inputs((grad_output: torch.Tensor,# [M_total,N] expert_weights: torch.Tensor,# [num_experts,N,K] expert_indices: torch.Tensor,# [M_total] group_size_m: int = 128,)),cg_grouped_gemm((inputs: torch.Tensor,expert_weights: torch.Tensor,expert_indices: torch.Tensor,group_size_m: int = 128,)),benchmark_cg_gemm_backward((M_total=1024,N=512,K=512,num_experts=8,group_size_m=128,device="cuda",num_runs=10,))] cg_forward.py→[cg_grouped_gemm_forward((inputs: torch.Tensor,# [M_total,K] expert_weights: torch.Tensor,# [num_experts,N,K] expert_indices: torch.Tensor,# [M_total] group_size_m: int = 128,)),cg_grouped_gemm_forward_dynamic((inputs: torch.Tensor,# [M_total,K] expert_weights: torch.Tensor,# [num_experts,N,K] expert_indices: torch.Tensor,# [M_total] group_size_m: int = 128,)),cg_grouped_gemm((inputs: torch.Tensor,expert_weights: torch.Tensor,expert_indices: torch.Tensor,# use_tma: bool = True,group_size_m: int = 128,))] cg_reference.py→[pytorch_reference((inputs: torch.Tensor,expert_weights: torch.Tensor,expert_indices: torch.Tensor,group_size_m: int = 128,))→{integration_tests::main}] check_padding_mm.py→[] checkpoint.py→[load_safetensor_weights((model: torch.nn.Module,weight_map: Dict[str,str],file_location: str,device: torch.device,))→{load_safetensor_file,get_needed_files,metrics::setup,metrics::setup,metrics::setup,metrics::setup},load_weights_from_hf((model: torch.nn.Module,distribution: str,device: torch.device,))→{load_safetensor_weights,get_hf_weight_map_and_path},read_weights_from_json((file_path: str))→{math::rope},get_hf_weight_map_and_path((model_id: str,))→{read_weights_from_json},get_needed_files((state_dict: Dict[str,torch.Tensor],weight_map: Dict[str,str]))→{metrics::setup},load_safetensor_file((full_path: str,device: torch.device))] combine.py→[__init__((self,group_name: str,align: int,in_len,out_len,token_shape,num_ranks,num_local_experts,dtype,device: torch.device,))[CTOR,DUNDER],forward((self,inp: torch.Tensor,out: torch.Tensor,in_splits_offsets: torch.Tensor,out_splits_offsets: torch.Tensor,))] components.py→[build_metrics_processor((job_config: JobConfig,parallel_dims: ParallelDims,model_args: BaseModelArgs | None = None,# noqa: ARG001 tag: str | None = None,))[HOT]] config.py→[_as_dict((value: Mapping[str,Any] | None))→{desloc::tick},ensure_mosaic_job_config_types((job_config: MosaicJobConfig))] convert_hf_to_dcp_with_gpus.py→[_create_fqn_mappings((self,state_dict: dict[str,torch.Tensor]))→{convert_to_titan_fqns,metrics::setup,metrics::setup,metrics::setup,metrics::setup,metrics::setup},_verify_state_dict((state_dict: dict[str,torch.Tensor],path: str,rank: int))→{metrics::setup,math::rope},convert_to_titan_fqns((fqn: str))→{extract_layer_number},_load_metadata((self))→{math::rope},_get_load_assignments((self,state_dict: dict[str,Any]))→
{convert_to_hf_shape},_create_verified_state_dict((pg: dist.ProcessGroup,mesh: DeviceMesh))→{simple_fsdp::_distribute_dtensor},extract_layer_number((s)),convert_to_hf_shape((fqn: str,titan_fqns: list[str],dtensor: DTensor)),convert_to_titan_tensors((fqn: str,full_tensor: torch.Tensor)),__init__((self,process_group: dist.ProcessGroup,path: str,token: Optional[str] = None,loader_every_n_ranks: int = 8,))[CTOR,DUNDER],convert((self,state_dict: dict[str,torch.Tensor])),_load_round((self,assignment: _Assignment)),_reshard_send((self,assignment: _Assignment,loaded_state_dict: dict[str,torch.Tensor],)),_reshard_receive((self,assignment: _Assignment,state_dict: dict[str,torch.Tensor])),_reshard((self,result: dict[str,torch.Tensor],state_dict: dict[str,torch.Tensor],))] convert_meta_to_dcp_with_gpus.py→[_create_fqn_mappings((self,state_dict: dict[str,torch.Tensor]))→{metrics::setup,metrics::setup,metrics::setup,metrics::setup,metrics::setup,metrics::setup},_create_verified_state_dict((pg: dist.ProcessGroup,mesh: DeviceMesh))→{simple_fsdp::_distribute_dtensor},convert_to_titan_fqns((fqn: str)),get_shard_dim((fqn: str)),split_fused_qkv((shards: list[torch.Tensor])),__init__((self,process_group: dist.ProcessGroup,path: str,loader_every_n_ranks: int = 8,))[CTOR,DUNDER],convert((self,state_dict: dict[str,torch.Tensor])),_get_file_path((self,loader_id: int)),_load_metadata((self)),_get_load_assignments((self,state_dict: dict[str,torch.Tensor])),_load_round((self,assignment: _Assignment)),_reshard_send((self,assignment: _Assignment,loaded_state_dict: dict[str,torch.Tensor],)),_reshard_receive((self,assignment: _Assignment,state_dict: dict[str,torch.Tensor])),_reshard((self,results: list[dict[str,torch.Tensor]],state_dict: dict[str,torch.Tensor],)),_verify_state_dict((state_dict: dict[str,torch.Tensor],path: str,rank: int))] custom_args.py→[] dataloader.py→[_extract_streams((dataset_cfg: dict[str,Any]))→{_join_local_path,_join_remote_path,_collect_group_stream_entries,_normalize_sampling_groups,_collect_group_stream_entries,_normalize_sampling_groups},_build_mosaic_dataloader((*,job_config: MosaicJobConfig,tokenizer: BaseTokenizer,dp_world_size: int,dp_rank: int,split: str,default_drop_last: bool,))[HOT]→{_create_streaming_dataset,_setup_unigram_metric,_prepare_dataset_kwargs,_select_stream_subset,_extract_streams,_extract_streams},_join_remote_path((root: str | None,path: str | None))→{_is_uri,_is_uri},_maybe_download_unigram_file((remote_uri: str | None,root_remote: str | None,split: str,destination: Path,config: UnigramMetricConfig,))→{s3_checkpoint::download_file_from_s3,s3_checkpoint::create_remote_up_down},_setup_unigram_metric((assignment: StreamAssignment,*,job_config: MosaicJobConfig,split: str,tokenizer: BaseTokenizer,))→{_build_unigram_metric_for_group,metrics::add_unigram_metric},_build_unigram_metric_for_group((streams: list[Stream] | None,default_split: str,tokenizer: BaseTokenizer,config: UnigramMetricConfig,group_key: str,dataset_root_remote: str | None,dataset_split_remote: str | None,))[HOT]→{_load_stream_unigram_counts,integration_tests::main},_normalize_sampling_groups((config: Any))→{desloc::tick},_collect_group_stream_entries((group: Mapping[str,Any]))→{_flatten_stream_configs},_select_dataset_config((dataset_cfg: Mapping[str,Any] | None,split: str))→{desloc::tick},_load_stream_unigram_counts((stream: Stream,*,root_remote: str | None,dataset_split: str | None,default_split: str,config: UnigramMetricConfig,))→{_maybe_download_unigram_file},_normalize_mosaic_dataloader_config((job_config: MosaicJobConfig,*,split: str,default_drop_last: bool,))→{_select_dataset_config},titan_collate_fn((batch: list[Any]))→{math::rope},build_mosaic_dataloader((*,dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer,job_config: MosaicJobConfig,))[HOT]→
{_build_mosaic_dataloader},build_mosaic_validation_dataloader((*,dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer,job_config: MosaicJobConfig,infinite: bool = False,# noqa: ARG001 - kept for compatibility))[HOT]→{_build_mosaic_dataloader},_is_uri((path: str | None)),_join_local_path((root: str | None,path: str | None)),_flatten_stream_configs((streams_cfg: Any)),_select_stream_subset((extraction: StreamExtractionResult,*,dp_rank: int,dp_world_size: int,)),_prepare_dataset_kwargs((dataset_cfg: dict[str,Any],*,dataset_split_remote: str | None,)),_create_streaming_dataset((*,assignment: StreamAssignment,tokenizer: BaseTokenizer,dataset_config: DatasetFactoryConfig,batch_size: int,split: str,)),__init__((self,*args: Any,**kwargs: Any))[CTOR,DUNDER],__getitem__((self,idx: int))[DUNDER],state_dict((self,num_samples: int | None = None,from_beginning: bool = True)),load_state_dict((self,obj: dict[str,Any])),__init__((self,dataset: StatefulStreamingTextDataset,dp_rank: int,dp_world_size: int,batch_size: int,collate_fn: Callable | None = None,num_workers: int = 0,prefetch_factor: int | None = 2,pin_memory: bool = True,persistent_workers: bool = True,drop_last: bool = True,))[CTOR,DUNDER],state_dict((self)),load_state_dict((self,state_dict: dict[str,Any]))] debug.py→[benchmark_performance(void)→{pytorch_reference,pytorch_reference,create_aligned_test_data,cg_forward::cg_grouped_gemm_forward,cg_forward::cg_grouped_gemm_forward},run_all_tests(void)→{benchmark_performance,debug::test_large,debug::test_medium,debug::test_small},pytorch_reference((inputs: torch.Tensor,expert_weights: torch.Tensor,expert_indices: torch.Tensor,group_size_m: int = 128,))→{integration_tests::main},create_aligned_test_data((batch_size: int,seq_len: int,hidden_dim: int,output_dim: int,num_experts: int,group_size_m: int = 128,device: str = "cuda",dtype: torch.dtype = torch.float16,)),verify_results((output_triton: torch.Tensor,output_reference: torch.Tensor,rtol: float = 1e-2,atol: float = 1e-2,))] decoupled_adamw.py→[report_per_parameter_metrics((self,param: torch.Tensor,name: str,optimizer_metrics: dict[str,torch.Tensor],))→{_decoupled_decay::_compute_decay_factor},__init__((# noqa: PLR0913 self,params: Iterable[torch.Tensor] | Iterable[dict],lr: float = 1e-3,betas: tuple[float,float] = (0.9,0.95),eps: float = 1e-8,weight_decay: float = 1e-5,*,amsgrad: bool = False,decouple: bool = True,foreach: bool | None = None,maximize: bool = False,capturable: bool = False,differentiable: bool = False,fused: bool | None = None,))[CTOR,DUNDER]] desloc.py→[_init_backup_storage((self))→{_extract_local_tensor},save_state((self))→{_extract_local_tensor},restore_state((self))→{_copy_into_tensor},prepare_sync((self))→{_extract_local_tensor},perform_sync((self))→{_copy_into_tensor},__init__((self,*,manager: Any,model: nn.Module,optimizer: Optimizer,param_sync_every: int,optimizer_sync_every: int | list[int] | dict[str,int] | None,backup_device: torch.device | None,pin_memory: bool,name_prefix: str,quorum_timeout_seconds: int,))[CTOR,DUNDER]→{integration_tests::main},_resolve_optimizer_sync_intervals((self,state_keys: Iterable[str]))[HOT]→{math::rope},_lazy_init_optimizer_fragments((self))→
{metrics::setup},_extract_local_tensor((tensor: torch.Tensor)),_copy_into_tensor((param: torch.Tensor,value: torch.Tensor)),__init__((self,sync_every: int))[CTOR,DUNDER],tick((self)),reset((self)),prepare_sync((self)),perform_sync((self)),save_state((self)),restore_state((self)),__init__((self,manager: Any,model: nn.Module,sync_every: int,backup_device: torch.device | None,pin_memory: bool,name_prefix: str,))[CTOR,DUNDER],register_state_dict_fn((self)),__init__((self,manager: Any,model: nn.Module,optimizer: Optimizer,state_key: str,sync_every: int,backup_device: torch.device | None,name_prefix: str,))[CTOR,DUNDER],_init_backup_storage((self)),save_state((self)),restore_state((self)),prepare_sync((self)),perform_sync((self)),register_state_dict_fn((self)),close((self)),_step_post_hook((self,_optimizer: Optimizer,_args: tuple[Any,...],_kwargs: dict[str,Any],)),_sync((self,fragments: list[_BaseFragment])),_prepare_sync((self,fragments: list[_BaseFragment])),_perform_sync((self,fragments: list[_BaseFragment])),__init__((self,model_parts: list[nn.Module],optimizer_cls: type[torch.optim.Optimizer],optimizer_kwargs: dict[str,Any],ft_manager: Any,desloc_config: DesLocConfig,*,use_ft_optimizer: bool = True,param_groups: list[dict[str,Any]] | None = None,))[CTOR,DUNDER],close_desloc((self))] dispatch.py→[__init__((self,group_name: str,align: int,in_len,out_len,token_shape,num_ranks,num_local_experts,dtype,device: torch.device,))[CTOR,DUNDER],forward((self,inp: torch.Tensor,out: torch.Tensor,in_splits: torch.Tensor,out_splits_offsets: torch.Tensor,))] download.py→[print_usage(void)] download_autoencoder.py→[hf_download((repo_id: str,file_path: str,local_dir: str,hf_token: Optional[str] = None))] dsgemm_kernels.py→[groupwise_activation_quant((x: torch.Tensor,block_size: int = 128,switching_size=2048,))→{integration_tests::main,integration_tests::main}] dsgemm_unit_testing.py→[create_m_indices_fast((m_sizes: torch.Tensor)),per_token_cast_to_fp8((x: torch.Tensor)),per_block_cast_to_fp8((x: torch.Tensor)),compute_reference_with_scaling((lhs: torch.Tensor,lhs_scales: torch.Tensor,rhs: torch.Tensor,rhs_scales: torch.Tensor,m_indices: torch.Tensor,num_groups: int,))[HOT]] dsgemm_utils.py→[make_grouped_layout((num_groups: int,x: torch.Tensor,y: torch.Tensor))→{get_col_major_tma_aligned_tensor,dsgemm_unit_testing::per_token_cast_to_fp8,dsgemm_unit_testing::per_block_cast_to_fp8,dsgemm_unit_testing::per_token_cast_to_fp8},construct_grouped((num_groups: int,x: torch.Tensor,y: torch.Tensor,is_masked: bool))→{get_col_major_tma_aligned_tensor,dsgemm_unit_testing::per_token_cast_to_fp8,dsgemm_unit_testing::per_block_cast_to_fp8,dsgemm_unit_testing::per_token_cast_to_fp8},prepare_fp8_input((x))→{get_col_major_tma_aligned_tensor,dsgemm_unit_testing::per_token_cast_to_fp8},per_block_cast_to_fp8((x: torch.Tensor))→{ceil_div,ceil_div},prepare_fp8_weight((w))→{dsgemm_unit_testing::per_block_cast_to_fp8},get_tma_aligned_size((x: int,element_size: int))→{ceil_div},get_col_major_tma_aligned_tensor((x: torch.Tensor))→{get_tma_aligned_size},compare_fp8_tensors((a: torch.Tensor,b: torch.Tensor)),create_indices_from_offsets_nosync((m_offsets: torch.Tensor)),create_m_indices_from_offsets((m_offsets: torch.Tensor)),create_m_indices_from_sizes((m_sizes: torch.Tensor)),get_m_indices((num_groups: int,m: int)),set_num_sms((num_sms: int)),get_num_sms(void),ceil_div((x: int,y: int)),get_m_alignment_for_contiguous_layout(void),per_token_cast_to_fp8((x: torch.Tensor))] engine.py→[close((self))] example_train.py→[batch_generator((self,data_iterable: Iterable[tuple[dict[str,torch.Tensor],torch.Tensor]])),forward_backward_step((self,input_dict: dict[str,torch.Tensor],labels: torch.Tensor)),train_step((self,data_iterator: Iterable[tuple[dict[str,torch.Tensor],torch.Tensor]])),state_dict((self)),load_state_dict((self,state_dict: dict[str,Any])),close((self))] fast_debug_ao.py→[] flux_dataset.py→
[_cc12m_wds_data_processor((sample: dict[str,Any],t5_tokenizer: FluxTokenizer,clip_tokenizer: FluxTokenizer,output_size: int = 256,))→{_process_cc12m_image},_coco_data_processor((sample: dict[str,Any],t5_tokenizer: FluxTokenizer,clip_tokenizer: FluxTokenizer,output_size: int = 256,))→{_process_cc12m_image},__init__((self,dataset_name: str,dataset_path: Optional[str],t5_tokenizer: BaseTokenizer,clip_tokenizer: BaseTokenizer,job_config: Optional[JobConfig] = None,dp_rank: int = 0,dp_world_size: int = 1,infinite: bool = False,))[CTOR,DUNDER]→{_validate_dataset},build_flux_dataloader((dp_world_size: int,dp_rank: int,job_config: JobConfig,# This parameter is not used,keep it for compatibility tokenizer: FluxTokenizer | None,infinite: bool = True,))[HOT]→{tokenizer::build_flux_tokenizer},build_flux_validation_dataloader((dp_world_size: int,dp_rank: int,job_config: JobConfig,# This parameter is not used,keep it for compatibility tokenizer: BaseTokenizer | None,generate_timestamps: bool = True,infinite: bool = False,))[HOT]→{tokenizer::build_flux_tokenizer},_process_cc12m_image((img: PIL.Image.Image,output_size: int = 256,)),_validate_dataset((dataset_name: str,dataset_path: Optional[str] = None)),_get_data_iter((self)),__iter__((self))[DUNDER],load_state_dict((self,state_dict)),state_dict((self)),__init__((self,dataset_name: str,dataset_path: Optional[str],t5_tokenizer: BaseTokenizer,clip_tokenizer: BaseTokenizer,job_config: Optional[JobConfig] = None,dp_rank: int = 0,dp_world_size: int = 1,generate_timesteps: bool = True,infinite: bool = False,))[CTOR,DUNDER],__iter__((self))[DUNDER]] ft_override.py→[] generate.py→[create_model((dist_config: DistConfig))→{checkpoint::load_weights_from_hf},decode((tokenizer,x))→{colorize_chat},colorize_chat((text,user_color=None,assistant_color=None,output_color=None)),create_dist_config((mesh: DeviceMesh)),time_generation((func))] group_gemms.py→[execute((self,contig_tokens,m_sizes,m_offsets,module))→{cg_forward::cg_grouped_gemm_forward,cg_forward::cg_grouped_gemm_forward,cg_forward::cg_grouped_gemm_forward},execute((self,contig_tokens,m_sizes,m_offsets,module))→{mg_grouped_gemm::grouped_gemm_forward,mg_grouped_gemm::grouped_gemm_forward,mg_grouped_gemm::grouped_gemm_forward},__init__((self,custom_activation))[CTOR,DUNDER],arrange_expert_weights((self,all_weights,submod_name,module)),execute((self,contig_tokens,m_sizes,m_offsets,module)),arrange_expert_weights((self,all_weights,submod_name,module)),execute((self,contig_tokens,m_sizes,m_offsets,module)),arrange_expert_weights((self,all_weights,submod_name,module)),arrange_expert_weights((self,all_weights,submod_name,module)),execute((self,contig_tokens,m_sizes,m_offsets,module)),arrange_expert_weights((self,all_weights,submod_name,module)),arrange_expert_weights((self,all_weights,submod_name,module)),execute((self,contig_tokens,m_sizes,m_offsets,module)),__init__((self,custom_activation,use_triton_quant=True))[CTOR,DUNDER],arrange_expert_weights((self,all_weights,submod_name,module)),execute((self,contig_tokens,m_sizes,m_offsets,module))] hf_embedder.py→[__init__((self,version: str,random_init=False,**hf_kwargs))[CTOR,DUNDER],forward((self,batch_tokens: Tensor))] hf_tokenizer.py→[remove_notset_root_handlers(void),__init__((self,tokenizer))[CTOR,DUNDER],encode((self,text,bos=False,eos=False,**kwargs)),__getattr__((self,name))[DUNDER],get_hf_tokenizer((model_id: str))] indices.py→[generate_permute_indices((tokens_per_expert_group: torch.Tensor,experts_per_rank: int,num_ranks: int,max_len: int,alignment: int,use_cpu: bool = False,))→{fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},simple_test(void)→
{generate_permute_indices,generate_permute_indices},fill_indices_wrapper((tokens_per_expert_group: torch.Tensor,start_index_values: torch.Tensor,write_offsets: torch.Tensor,experts_per_rank: int,num_ranks: int,max_len: int,block_size: int = 128,max_blocks: int = 1024,# cap on total number of blocks to launch))→{integration_tests::main},fill_indices_cpu((tokens_per_expert_group: torch.Tensor,start_index_values: torch.Tensor,write_offsets: torch.Tensor,experts_per_rank: int,num_ranks: int,max_len: int,))→{integration_tests::main}] infer.py→[] job_config.py→[] layers.py→[forward((self,ids: Tensor))→{math::rope},forward((self,x: Tensor,pe: Tensor))→{math::attention},forward((self,img: Tensor,txt: Tensor,vec: Tensor,pe: Tensor))→{math::attention},forward((self,x: Tensor,vec: Tensor,pe: Tensor))→{math::attention},__init__((self,dim: int,theta: int,axes_dim: list[int]))[CTOR,DUNDER],timestep_embedding((t: Tensor,dim,max_period=10000,time_factor: float = 1000.0)),__init__((self,in_dim: int,hidden_dim: int))[CTOR,DUNDER],init_weights((self,init_std: float = 0.02)),forward((self,x: Tensor)),__init__((self,dim: int))[CTOR,DUNDER],init_weights((self)),forward((self,q: Tensor,k: Tensor,v: Tensor)),__init__((self,dim: int,num_heads: int = 8,qkv_bias: bool = False))[CTOR,DUNDER],init_weights((self)),__init__((self,dim: int,double: bool))[CTOR,DUNDER],init_weights((self)),forward((self,vec: Tensor)),__init__((self,hidden_size: int,num_heads: int,mlp_ratio: float,qkv_bias: bool = False))[CTOR,DUNDER],init_weights((self)),__init__((self,hidden_size: int,num_heads: int,mlp_ratio: float = 4.0,qk_scale: float | None = None,))[CTOR,DUNDER],init_weights((self)),__init__((self,hidden_size: int,patch_size: int,out_channels: int))[CTOR,DUNDER],init_weights((self)),forward((self,x: Tensor,vec: Tensor))] loss.py→[mse_loss((pred: torch.Tensor,labels: torch.Tensor)),build_mse_loss((job_config: JobConfig))[HOT]] lr_scheduler.py→[_linear_warmup_stable_decay((current_step: int,*,warmup_steps: int,stable_steps: int,decay_steps: int,lr_decay_type: str,min_lr_factor: float,)),build_fl_lr_schedulers((optimizers: OptimizersContainer,lr_scheduler_config: FLLRSchedulerConfig,training_steps: int,))[HOT]] math.py→[attention((q: Tensor,k: Tensor,v: Tensor,pe: Tensor))→{apply_rope},rope((pos: Tensor,dim: int,theta: int)),apply_rope((xq: Tensor,xk: Tensor,freqs_cis: Tensor))] metrics.py→[_build_unigram_payload((self,mesh: DeviceMesh | None))[HOT]→{reset_unigram_metrics,reset_unigram_metrics,collect_unigram_metrics},_add_metrics((# noqa: C901,PLR0912,PLR0915 self,suffix: str,value: torch.Tensor))→{compute_kurtosis,compute_skewness},on_step_end((# noqa: C901,PLR0912,PLR0915 self,context: CallbackStepContext,))→{integration_tests::main,integration_tests::main},__init__((self,*,interval: int = 25,ignore_module_types: Sequence[str] | None = None,gradient_accumulation_steps: int = 1,enabled_metrics: set[str] | None = None,))[CTOR,DUNDER]→{integration_tests::main},_prepare_local_metrics((self))→{integration_tests::main},__init__((self,*,enabled: bool,steps: Sequence[int],new_vs: Sequence[float] | None,new_betas: Sequence[float] | None,reset_momenta: Sequence[str],log_metrics: bool,))[CTOR,DUNDER]→{setup},log((self,step: int,global_avg_loss: float,global_max_loss: float,grad_norm: float,extra_metrics: dict[str,Any] | None = None,))→
{desloc::tick},__init__((self,unigram_probabilities: Tensor,ignore_index: int = -100,*,dist_sync_on_step: bool = False,))[CTOR,DUNDER],update((self,output: Mapping | Tensor,target: Tensor)),compute((self))[HOT],add_unigram_metric((metric: PureUnigramCrossEntropy)),collect_unigram_metrics((*,reset: bool = True)),reset_unigram_metrics(void),update_unigram_metrics((labels: Tensor)),compute_skewness((value: torch.Tensor))[HOT],compute_kurtosis((value: torch.Tensor))[HOT],setup((self,context: CallbackSetupContext)),on_step_end((self,context: CallbackStepContext)),close((self)),_is_metric_enabled((self,metric_key: str)),should_log_step((self,step: int)),register((self,model: torch.nn.Module)),_register_forward_hook((self,module: torch.nn.Module)),_forward_pre_hook((self,module: torch.nn.Module,args: tuple[Any,...],kwargs: dict[str,Any],)),_forward_hook((self,module: torch.nn.Module,inputs: tuple[Any,...],output: Any,)),_recursively_add_metrics((self,suffix: str,values: Any)),finalize((self,step: int,logger: Any,mesh: DeviceMesh | None,)),_reduce_metrics((# noqa: C901,PLR0912 self,metrics: dict[str,float | list[float]],mesh: DeviceMesh | None)),_reset_metrics((self)),__init__((self,interval: int = 10,*,only_global: bool = True,log_optimizer_metrics: bool = True,))[CTOR,DUNDER],setup((self,context: CallbackSetupContext)),_reduce_metrics_across_ranks((self,optimizer_metrics: dict[str,torch.Tensor],mesh: DeviceMesh)),__init__((self,*,interval: int = 1,enabled: bool = True))[CTOR,DUNDER],on_step_end((self,context: CallbackStepContext)),__init__((self,*,interval: int = 0,enabled: bool = False))[CTOR,DUNDER],on_step_end((self,context: CallbackStepContext)),__init__((self,*,interval: int = 0,enabled: bool = False))[CTOR,DUNDER],on_step_end((self,context: CallbackStepContext)),on_step_end((self,context: CallbackStepContext)),_update_group_values((self,param_groups: list[dict[str,Any]],key: str,values: tuple[float,...])),_reset_momenta((self,optimizer_state: dict[Any,dict[str,Any]])),_zero_state_value((self,value: Any)),__init__((self,*args: Any,**kwargs: Any))[CTOR,DUNDER],should_log((self,step: int)),_ensure_callbacks_setup((self)),_run_step_callbacks((self,step: int,mesh: DeviceMesh | None)),_run_validation_callbacks((self,loss: float,step: int)),log_validation((self,loss: float,step: int)),close((self))] mg_grouped_gemm.py→[grouped_gemm_backward((grad_output: torch.Tensor,x: torch.Tensor,w: torch.Tensor,m_sizes: torch.Tensor,use_tma: bool = True,tma_size: int = 128,))→{grouped_gemm_dw_tma,grouped_gemm_dx_tma},grouped_gemm_forward((x: torch.Tensor,w: torch.Tensor,m_sizes: torch.Tensor,tma_size: int = 128,using_fp8: bool = False,)),grouped_gemm_dx_tma((grad_output: torch.Tensor,w: torch.Tensor,m_sizes: torch.Tensor,num_sms: int = 132,tma_size: int = 128,)),grouped_gemm_dw_tma((x: torch.Tensor,grad_output: torch.Tensor,m_sizes: torch.Tensor,num_sms: int = 132,tma_size: int = 128,)),mg_grouped_gemm((x: torch.Tensor,w: torch.Tensor,m_sizes: torch.Tensor,use_tma: bool = True,tma_size: int = 128,using_fp8: bool = False,))] mm_collator.py→[padded_collate((batch: List[Dict[str,List[int]]],padding_idx: int = 0,ignore_idx: int = -100,))] mm_collator_nld.py→[] mm_dataset.py→[_process_obelics_sample((sample: dict[str,Any],image_token: str = "<|image|>"))→{integration_tests::main,utils::load_image},__init__((self,dataset_name: str,dataset_path: Optional[str],tokenizer: BaseTokenizer,image_token: str = "<|image|>",tile_size: int = 448,max_num_tiles: int = 4,seq_len: int = 2048,dp_rank: int = 0,dp_world_size: int = 1,infinite: bool = False,))[CTOR,DUNDER]→{_validate_mm_dataset},__iter__((self))[DUNDER]→
{metrics::setup},_load_obelics_dataset((dataset_path: str)),_validate_mm_dataset((dataset_name: str,dataset_path: str = None)),_get_data_iter((self)),load_state_dict((self,state_dict)),state_dict((self)),build_mm_dataloader((dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer,job_config: JobConfig,infinite: bool = True,))[HOT]] mm_datasets.py→[_process_mm_sample((texts: list[str] | str,images: list[bytes] | bytes,tokenizer: BaseTokenizer,patch_size: int,max_patch_per_image: int,spatial_merge_size: int,special_tokens: SpecialTokens,))→{text::process_text_with_images,image::calculate_image_tokens,image::process_image},_process_obelics_sample((sample: dict[str,Any],tokenizer: HuggingFaceTokenizer,patch_size: int,spatial_merge_size: int,max_patch_per_image: int,special_tokens: SpecialTokens,))→{_process_mm_sample},_process_cc12_wd_sample((sample: dict[str,Any],tokenizer: BaseTokenizer,patch_size: int,spatial_merge_size: int,max_patch_per_image: int,special_tokens: SpecialTokens,))→{_process_mm_sample},__init__((self,dataset_name: str,dataset_path: str | None,tokenizer: BaseTokenizer,batch_size: int,seq_len: int,patch_size: int,spatial_merge_size: int,max_patches_per_image: int,max_images_per_batch: int,packing_buffer_size: int,special_tokens: SpecialTokens,dp_rank: int = 0,dp_world_size: int = 1,infinite: bool = False,))[CTOR,DUNDER]→{mm_dataset::_validate_mm_dataset},_validate_mm_dataset((dataset_name: str,dataset_path: str | None = None)),__iter__((self))[DUNDER],_get_data_iter((self)),load_state_dict((self,state_dict)),state_dict((self)),build_mm_dataloader((dp_world_size: int,dp_rank: int,tokenizer: HuggingFaceTokenizer,job_config: JobConfig,infinite: bool = True,))[HOT]] model.py→[yarn_find_correction_range((low_rot,high_rot,dim,base=10000,max_position_embeddings=2048))→{yarn_find_correction_dim,yarn_find_correction_dim,integration_tests::main,integration_tests::main},_set_cos_sin_cache((self,seq_len,device,dtype))→{yarn_get_mscale,yarn_get_mscale,yarn_linear_ramp_mask,yarn_find_correction_range},apply_rotary_emb((xq: torch.Tensor,xk: torch.Tensor,rope_cache: torch.Tensor))→{rotate_half,rotate_half,reshape_for_broadcast},forward((self,x: torch.Tensor,freqs_cis: torch.Tensor,))→{repeat_kv,repeat_kv,apply_rotary_emb},forward((self,x: torch.Tensor,rope_cache: torch.Tensor,))→{repeat_kv,repeat_kv,apply_rotary_emb},forward((self,x: torch.Tensor,freqs_cis: torch.Tensor,))→{repeat_kv,repeat_kv,apply_rotary_emb},forward((self,x: torch.Tensor,freqs_cis: torch.Tensor,))→{repeat_kv,repeat_kv,apply_rotary_emb},__init__((self,layer_id: int,model_args: TransformerModelArgs,))[CTOR,DUNDER]→{math::rope,math::attention},__init__((self,layer_id: int,model_args: Qwen3ModelArgs))[CTOR,DUNDER]→{math::rope,math::attention},apply_rotary_pos_emb((q,k,cos,sin,position_ids,unsqueeze_dim=1))→{rotate_half,rotate_half},forward((self,hidden_states: torch.Tensor,attention_mask: Optional[torch.Tensor] = None,position_ids: Optional[torch.LongTensor] = None,))→{apply_rotary_pos_emb,attn_mask_utils::_prepare_4d_causal_attention_mask},__init__((self,config: ModelArgs,layer_idx: int))[CTOR,DUNDER]→{math::rope,math::attention},forward((self,x: torch.Tensor,encoder_input: torch.Tensor,mask: Optional[torch.Tensor] = None,))→{repeat_kv,repeat_kv},apply_rotary_emb((xq: torch.Tensor,xk: torch.Tensor,freqs_cis: torch.Tensor,))→{reshape_for_broadcast},forward((self,tokens: torch.Tensor,pixel_values: torch.Tensor,grid_thw: torch.Tensor,special_tokens: SpecialTokens,input_batch: torch.Tensor | None = None,))→{_scatter_img_tokens},forward((self,img: Tensor,img_ids: Tensor,txt: Tensor,txt_ids: Tensor,timesteps: Tensor,y: Tensor,))→{layers::timestep_embedding},apply_rotary_emb((xq: torch.Tensor,xk: torch.Tensor,freqs_cis: torch.Tensor,))→{reshape_for_broadcast},_precompute_rope_cache((self))[HOT]→{precompute_rope_cache},_precompute_freqs_cis((self))[HOT]→{precompute_freqs_cis},__init__((self,config))[CTOR,DUNDER]→
{get_group},__init__((self,model_args: ModelArgs,attn_scale: Optional[nn.Module] = None,mlp_scale: Optional[nn.Module] = None,))[CTOR,DUNDER]→{math::attention},moe_on_device((self,x,topk_ids,topk_weight))→{indices::generate_permute_indices},__init__((self,config: ModelArgs,layer_idx: Optional[int] = None))[CTOR,DUNDER]→{yarn_get_mscale},_precompute_freqs_cis((self,model_args))[HOT]→
{precompute_freqs_cis},__init__((self,model_args: TransformerModelArgs))[CTOR,DUNDER],__init__((self,model_args: DeepSeekV3ModelArgs))[CTOR,DUNDER],init_weights((self,*args,**kwargs)),init_weights((self,*args,**kwargs)),_scatter_img_tokens((h_BSD,tokens_BS,i_NLD,i_mask_NL,img_id)),precompute_freqs_cis((dim: int,end: int,theta: float = 10000.0))[HOT],precompute_rope_cache((dim: int,max_seq_len: int,base: float = 1_000_000.0))[HOT],__init__((self,model_args: FluxModelArgs))[CTOR,DUNDER],__init__((self,in_dim: int,out_dim: int))[CTOR,DUNDER],rotate_half((x: torch.Tensor)),reshape_for_broadcast((freqs_cis: torch.Tensor,x: torch.Tensor)),forward((self,x_NLD: torch.Tensor)),reshape_for_broadcast((rope_cache: torch.Tensor,x: torch.Tensor)),init_weights((self)),__init__((self,model_args: Llama3Siglip2ModelArgs))[CTOR,DUNDER],__init__((self,*args: Any,**kwargs: Any))[CTOR,DUNDER],get_group((dim_name: Optional[str] = None)),init_weights((self,buffer_device=None)),forward((self,x: torch.Tensor)),__init__((self,hidden_size,eps=1e-6))[CTOR,DUNDER],forward((self,hidden_states)),init_weights((self,buffer_device=None)),precompute_freqs_cis((dim: int,end: int,theta: float = 10000.0))[HOT],__init__((self,dim,max_position_embeddings=2048,base=10000,device=None))[CTOR,DUNDER],repeat_kv((x: torch.Tensor,n_rep: int)),repeat_kv((x: torch.Tensor,n_rep: int)),_set_cos_sin_cache((self,seq_len,device,dtype)),reshape_for_broadcast((freqs_cis: torch.Tensor,x: torch.Tensor)),forward((self,x,seq_len=None)),__init__((self,model_args: Qwen3ModelArgs))[CTOR,DUNDER],__init__((self,model_args: TransformerModelArgs,use_rope: bool = True,fixed_block_size: int | None = None,))[CTOR,DUNDER],__init__((self,dim,max_position_embeddings=2048,base=10000,device=None,scaling_factor=1.0,))[CTOR,DUNDER],_set_cos_sin_cache((self,seq_len,device,dtype)),__init__((self,dim,max_position_embeddings=2048,base=10000,device=None,scaling_factor=1.0,))[CTOR,DUNDER],init_weights((self,init_std: float)),init_weights((self,init_std: float)),repeat_kv((x: torch.Tensor,num_rep: int)),_set_cos_sin_cache((self,seq_len,device,dtype)),__init__((self,model_args: ModelArgs))[CTOR,DUNDER],yarn_find_correction_dim((num_rotations,dim,base=10000,max_position_embeddings=2048)),yarn_get_mscale((scale=1,mscale=1)),yarn_linear_ramp_mask((min,max,dim)),init_weights((self,init_std: float)),__init__((self,dim: int,hidden_dim: int,multiple_of: int,ffn_dim_multiplier: float | None,))[CTOR,DUNDER],__init__((self,dim,max_position_embeddings=2048,base=10000,device=None,scaling_factor=1.0,original_max_position_embeddings=4096,beta_fast=32,beta_slow=1,mscale=1,mscale_all_dim=0,))[CTOR,DUNDER],__init__((self,dim: int,hidden_dim: int,))[CTOR,DUNDER],forward((self,x)),forward((self,x)),init_weights((self,init_std: float)),init_weights((self,init_std: float)),__init__((self,dim: int,hidden_dim: int,multiple_of: int,ffn_dim_multiplier: Optional[float],activation: nn.Module = nn.SiLUvoid,))[CTOR,DUNDER],rotate_half((x)),forward((self,x: torch.Tensor,rope_cache: torch.Tensor,)),forward((self,x)),init_weights((self,init_std: float)),__init__((self))[CTOR,DUNDER],forward((self,x: torch.Tensor)),init_weights((self,buffer_device: torch.device)),forward((self,x: torch.Tensor,freqs_cis: torch.Tensor,)),__init__((self,config,hidden_size=None,intermediate_size=None))[CTOR,DUNDER],__init__((self,max_num_tiles: int,emb_dim: int,))[CTOR,DUNDER],init_weights((self,buffer_device: torch.device)),forward((self,x)),__init__((self,model_args: Qwen3ModelArgs))[CTOR,DUNDER],forward((self,x: torch.Tensor,aspect_ratio: torch.Tensor)),__init__((self,config))[CTOR,DUNDER],init_weights((self,buffer_device: torch.device | None = None,)),__init__((self,model_args: TransformerModelArgs))[CTOR,DUNDER],reset_parameters((self)),forward((self,hidden_states)),__init__((self,emb_dim: int,tile_size: int,patch_size: int))[CTOR,DUNDER],init_weights((self,buffer_device: torch.device | None = None,)),forward((self,x: torch.Tensor,*args: Tuple[Any])),forward(
(self,tokens: torch.Tensor,input_batch: torch.Tensor | None = None,)),__init__((self,max_num_tiles: int,emb_dim: int,tile_size: int,patch_size: int))[CTOR,DUNDER],forward((self,tokens: torch.Tensor,input_batch: torch.Tensor | None = None,)),forward((self,x: torch.Tensor,aspect_ratio: torch.Tensor)),__init__((self,in_channels: int,out_channels: int,kernel_size: int,stride: int,bias: bool = False,))[CTOR,DUNDER],forward((self,x: torch.Tensor)),combine_experts((self,submod_name: str)),forward((self,x: torch.Tensor,mask: Optional[torch.Tensor] = None,)),setup_symm_mem((self,dtype: torch.dtype,device: torch.device)),__init__((self,emb_dim: int))[CTOR,DUNDER],forward((self,x: torch.Tensor)),get_send_buf((self)),get_gather_buf((self)),forward((self,hidden_states)),moe_forward((self,x,topk_ids,topk_weight)),sort_tokens((self,x,topk_ids,topk_weights)),__init__((self,model_args: ModelArgs,))[CTOR,DUNDER],_run_group_gemm((self,contig_tokens,m_sizes,m_offsets)),forward((self,images: torch.Tensor,aspect_ratio: Optional[torch.Tensor] = None)),__init__((self,model_args: ModelArgs,))[CTOR,DUNDER],forward((self,x: torch.Tensor,hidden_states: Optional[List[torch.Tensor]] = None,)),_init_rope((self)),__init__((self,model_args: ModelArgs))[CTOR,DUNDER],forward((self,images: torch.Tensor,aspect_ratio: Optional[torch.Tensor] = None)),__init__((self,dim: int,hidden_dim: int,multiple_of: int,ffn_dim_multiplier: Optional[float],))[CTOR,DUNDER],forward((self,x)),init_weights((self,init_std: float)),__init__((self,model_args: ModelArgs))[CTOR,DUNDER],init_weights((self,init_std: float)),forward((self,hidden_states: torch.Tensor,attention_mask: Optional[torch.Tensor] = None,position_ids: Optional[torch.LongTensor] = None,)),__init__((self,model_args: ModelArgs))[CTOR,DUNDER],init_weights((self,init_std: float)),__init__((self,model_args: ModelArgs,))[CTOR,DUNDER],forward((self,x: torch.Tensor,freqs_cis: torch.Tensor,**kwargs: Dict,)),__init__((self,config: ModelArgs))[CTOR,DUNDER],__init__((self,model_args: ModelArgs,))[CTOR,DUNDER],_skip_mask((self,mask: Optional[torch.Tensor])),_init_weights((self,module)),forward((self,x: torch.Tensor,*,encoder_input: Optional[torch.Tensor] = None,encoder_mask: Optional[torch.Tensor] = None,**kwargs: Dict,)),forward((self,tokens: torch.Tensor,attention_mask: Optional[torch.Tensor] = None,position_ids: Optional[torch.LongTensor] = None,)),__init__((self,config))[CTOR,DUNDER],forward((self,tokens: torch.Tensor,attention_mask: Optional[torch.Tensor] = None,position_ids: Optional[torch.LongTensor] = None,)),__init__((self,layer: nn.Module,fusion_layer: nn.Module,fusion_first: bool = True))[CTOR,DUNDER],forward((self,x: torch.Tensor,**kwargs: Dict)),__init__((self,vocab_size: int,fusion_vocab_size: int,embed_dim: int))[CTOR,DUNDER],prepare_inputs_for_generation((self,input_ids,past_key_values=None,attention_mask=None,**kwargs,)),forward((self,input: torch.Tensor)),__init__((self,model_args: ModelArgs))[CTOR,DUNDER],setup_symm_mem((self,dtype: torch.dtype,device: torch.device)),forward((self,tokens: torch.Tensor,*,encoder_input: Optional[torch.Tensor] = None,encoder_mask: Optional[torch.Tensor] = None,))] model_args.py→[] model_config.py→[] moe_kernels.py→[generate_permute_indices((tokens_per_expert_group: torch.Tensor,experts_per_rank: int,num_ranks: int,max_len: int,alignment: int,use_cpu: bool = False,))→{indices::fill_indices_wrapper,permute_indices_testing::fill_indices_cpu},simple_test(void)→{indices::generate_permute_indices,indices::generate_permute_indices},fill_indices_wrapper((tokens_per_expert_group: torch.Tensor,start_index_values: torch.Tensor,write_offsets: torch.Tensor,experts_per_rank: int,num_ranks: int,max_len: int,block_size: int = 128,max_blocks: int = 1024,# cap on total number of blocks to launch))→
{integration_tests::main},fill_indices_cpu((tokens_per_expert_group: torch.Tensor,start_index_values: torch.Tensor,write_offsets: torch.Tensor,experts_per_rank: int,num_ranks: int,max_len: int,))→{integration_tests::main}] optimizer_builder.py→[build_mosaic_optimizers((model_parts: list[torch.nn.Module],optimizer_config: MosaicOptimizerConfig | dict[str,Any],parallel_dims: ParallelDims,ft_manager: FTManager | None = None,param_groups: list[dict[str,Any]] | None = None,))[HOT]→{_build_optimizer_container,_build_optimizer_kwargs,_resolve_optimizer_class,_normalize_mosaic_optimizer_config},_build_optimizer_container((*,model_parts: list[torch.nn.Module],optimizer_cls: type[Optimizer],optimizer_kwargs: dict[str,Any],config: MosaicOptimizerConfig,parallel_dims: ParallelDims,ft_manager: FTManager | None,param_groups: list[dict[str,Any]] | None,))[HOT]→{_build_desloc_container},_resolve_optimizer_class((name: str))[HOT],_normalize_mosaic_optimizer_config((optimizer_config: MosaicOptimizerConfig | dict[str,Any],)),_build_optimizer_kwargs((config: MosaicOptimizerConfig,extra_kwargs: dict[str,Any]))[HOT],_build_desloc_container((*,model_parts: list[torch.nn.Module],optimizer_cls: type[Optimizer],optimizer_kwargs: dict[str,Any],desloc_cfg: DesLocConfig,parallel_dims: ParallelDims,ft_manager: FTManager,param_groups: list[dict[str,Any]] | None,))[HOT]] optimizers.py→[] parallelize.py→[parallelize_vlm((model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,))→{apply_fsdp,apply_fsdp,apply_compile,apply_compile,apply_ac,apply_ac},parallelize_qwen3((model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,))→{apply_fsdp,apply_fsdp,apply_compile,apply_ac,apply_moe_ep_tp,apply_non_moe_tp},parallelize_llama((model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,))→{apply_fsdp,apply_fsdp,apply_compile,apply_ac,apply_moe_ep_tp,apply_non_moe_tp},parallelize_deepseekv3((model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,))→{apply_ac,apply_moe_ep_tp,apply_non_moe_tp,simple_fsdp::data_parallel,simple_fsdp::data_parallel},parallelize_llama((model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,))→{apply_ac,apply_ac,simple_fsdp::data_parallel},parallelize_flux((model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,))→{apply_fsdp,apply_ac},apply_fsdp((model: nn.Module,dp_mesh: DeviceMesh,param_dtype: torch.dtype,reduce_dtype: torch.dtype,cpu_offload: bool = False,)),apply_ac((model: nn.Module,ac_config)),apply_fsdp((model: nn.Module,dp_mesh: DeviceMesh,param_dtype: torch.dtype,reduce_dtype: torch.dtype,pp_enabled: bool,cpu_offload: bool = False,reshard_after_forward_policy: str = "default",)),parallelize_encoders((t5_model: nn.Module,clip_model: nn.Module,parallel_dims: ParallelDims,job_config: JobConfig,)),apply_non_moe_tp((model: nn.Module,tp_mesh: DeviceMesh,loss_parallel: bool,enable_float8_tensorwise_tp: bool,enable_async_tp: bool,)),apply_non_moe_tp((model: nn.Module,tp_mesh: DeviceMesh,loss_parallel: bool,enable_float8_tensorwise_tp: bool,)),apply_fsdp((model: nn.Module,dp_mesh: DeviceMesh,param_dtype: torch.dtype,reduce_dtype: torch.dtype,pp_enabled: bool,cpu_offload: bool = False,reshard_after_forward_policy: str = "default",ep_degree: int = 1,dp_mod_ep_mesh: DeviceMesh | None = None,gradient_divide_factor: int | None = None,)),apply_moe_ep_tp((model: nn.Module,tp_mesh: DeviceMesh | None,ep_mesh: DeviceMesh | None,ep_tp_mesh: DeviceMesh | None,etp_enabled: bool,)),apply_compile((model: nn.Module,compile_config: CompileConfig))] parallelize_deepseek.py→[get_group((dim_name: Optional[str] = None)),parallelize_deepseek((# model: nn.Module,world_mesh: DeviceMesh,device: torch.device,model_args,rank: int,# parallel_dims: ParallelDims,# job_config: JobConfig,))] permute_indices_testing.py→
[fill_indices_cpu((tokens_per_expert_group: torch.Tensor,start_index_values: torch.Tensor,write_offsets: torch.Tensor,experts_per_rank: int,num_ranks: int,max_len: int,))→{integration_tests::main},fill_indices_cpu((tokens_per_expert_group: torch.Tensor,start_index_values: torch.Tensor,write_offsets: torch.Tensor,experts_per_rank: int,num_ranks: int,max_len: int,))→{integration_tests::main},setUp((self)),setUp((self)),create_test_data((self,experts_per_rank: int,num_ranks: int,token_range: Tuple[int,int] = (1,16),alignment: int = 32,)),create_test_data((self,experts_per_rank: int,num_ranks: int,token_range: Tuple[int,int] = (1,16),alignment: int = 32,))] qhadamw.py→[report_per_parameter_metrics((self,param: torch.Tensor,name: str,optimizer_metrics: dict[str,torch.Tensor],))→{_decoupled_decay::_compute_decay_factor},_single_tensor_qhadamw((# noqa: PLR0913 params: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],max_exp_avg_sqs: list[Tensor],state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | None,decouple: bool,amsgrad: bool,beta1: float,beta2: float,v1: float,lr: float | Tensor,weight_decay: float,eps: float,maximize: bool,capturable: bool,differentiable: bool,has_complex: bool,# noqa: ARG001))→{_decoupled_decay::_compute_decay_factor},_multi_tensor_qhadamw((# noqa: C901,PLR0913,PLR0912,PLR0915 params: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],max_exp_avg_sqs: list[Tensor],state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | None,has_complex: bool,amsgrad: bool,beta1: float,beta2: float,v1: float,lr: float | Tensor,weight_decay: float,decouple: bool,eps: float,maximize: bool,capturable: bool,differentiable: bool,))→{_decoupled_decay::_compute_decay_factor},__init__((# noqa: PLR0913,PLR0912,C901 self,params: ParamsT,lr: float | Tensor = 1e-3,betas: tuple[float,float] = (0.9,0.95),vs: tuple[float,...] = (0.7,),eps: float = 1e-8,weight_decay: float = 1e-5,*,amsgrad: bool = False,decouple: bool = True,foreach: bool | None = None,maximize: bool = False,capturable: bool = False,differentiable: bool = False,fused: bool | None = None,))[CTOR,DUNDER],__setstate__((self,state: dict))[DUNDER],_init_group((# noqa: PLR0913 self,group: dict,params_with_grad: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],max_exp_avg_sqs: list[Tensor],state_steps: list[Tensor],)),_fused_qhadamw((*args: Any,**kwargs: Any,))] qhadopt.py→[_multi_tensor_qhadopt((# noqa: C901,PLR0912,PLR0913,PLR0915 params: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | None,has_complex: bool,beta1: float,beta2: float,v1: float,lr: float | Tensor,clip_lambda: Callable[[Number | Tensor | Any],float] | None,weight_decay: float,decouple: bool,eps: float,maximize: bool,capturable: bool,differentiable: bool,))→{_decoupled_decay::_compute_decay_factor,_decoupled_decay::_compute_decay_factor},_default_clip_lambda((step: Number | Tensor))→{math::rope},report_per_parameter_metrics((self,param: torch.Tensor,name: str,optimizer_metrics: dict[str,torch.Tensor],))→{_decoupled_decay::_compute_decay_factor},_single_tensor_qhadopt((# noqa: PLR0913 params: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | None,decouple: bool,clip_lambda: Callable[[Number | Tensor | Any],float] | None,beta1: float,beta2: float,v1: float,lr: float | Tensor,weight_decay: float,eps: float,maximize: bool,capturable: bool,differentiable: bool,has_complex: bool,# noqa: ARG001))→
{_decoupled_decay::_compute_decay_factor},__init__((# noqa: C901,PLR0913,PLR0912 self,params: ParamsT,lr: float | Tensor = 1e-3,betas: tuple[float,float] = (0.999,0.9999),vs: tuple[float,...] = (0.9,),eps: float = 1e-6,clip_lambda: (Callable[[Number | Tensor | Any],float] | None) = _default_clip_lambda,weight_decay: float = 0.0,*,decouple: bool = False,foreach: bool | None = None,maximize: bool = False,capturable: bool = False,differentiable: bool = False,fused: bool | None = None,))[CTOR,DUNDER],__setstate__((self,state: dict))[DUNDER],_init_group((# noqa: PLR0913 self,group: dict,params_with_grad: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],state_steps: list[Tensor],)),_fused_qhadopt((# noqa: PLR0913 params: list[Tensor],grads: list[Tensor],exp_avgs: list[Tensor],exp_avg_sqs: list[Tensor],state_steps: list[Tensor],grad_scale: Tensor | None,found_inf: Tensor | None,*,initial_lr: float | None,has_complex: bool,beta1: float,beta2: float,v1: float,lr: float | Tensor,clip_lambda: Callable[[int],float] | None,weight_decay: float,decouple: bool,eps: float,maximize: bool,capturable: bool,differentiable: bool,))] reference_utils.py→[compute_reference_backward((x,w,m_sizes,grad_output))[HOT]→{compute_reference_forward},compute_reference_forward((x,w,m_sizes))[HOT],analyze_tensor_differences((actual,expected,name))] s3_checkpoint.py→[__init__((self,checkpointer: CheckpointManager,config: S3CheckpointingConfig,job_config: MosaicJobConfig,*,enable_uploads: bool = True,))[CTOR,DUNDER]→{create_remote_up_down,metrics::setup,metrics::setup,metrics::setup},_download_step((self,step: int,remote_path: str))→{download_file_from_s3,download_file_from_s3,integration_tests::main},__getattr__((self,name: str))[DUNDER]→{math::rope,math::rope},install_onto_checkpointer((self))→{math::rope},_upload_step((self,step: int,directory: Path))→{upload_file_to_s3},_write_latest_marker((self,step: int))→{upload_file_to_s3},_read_remote_latest_step((self,remote_root: str | None = None))→{download_file_from_s3},setup_s3_checkpointing((checkpointer: CheckpointManager,job_config: MosaicJobConfig,*,install: bool = True,))→{get_s3_checkpoint_wrapper_factory},download_file_from_s3((remote_up_down: RemoteUploaderDownloader,remote_file_name: str,local_file_name: Path | str,)),upload_file_to_s3((remote_up_down: RemoteUploaderDownloader,remote_file_name: str,local_file_name: Path,)),create_remote_up_down((# noqa: PLR0913 bucket_name: str,prefix: str,num_attempts: int,client_config: dict[str,Any],*,num_concurrent_uploads: int = 1,upload_staging_folder: str | None = None,use_procs: bool = True,)),attach_to_trainer((self,trainer: Any)),_start_remote_workers((self)),__del__((self))[DUNDER],download_if_needed((self)),close((self)),_wait_for_staging_with_logging((self)),_resolve_remote_root((self))[HOT],_checkpoint_dir((self,step: int)),_raise_invalid_resume_format((self,cause: Exception | None = None)),_remote_key((self,relative_path: Path,remote_root: str | None = None)),save((self,curr_step: int,*,last_step: bool = False)),maybe_wait_for_staging((self)),_process_pending((self,flush: bool = False)),_is_directory_ready_for_upload((self,directory: Path)),_iter_checkpoint_files((self,directory: Path)),_find_local_latest_step((self)),get_s3_checkpoint_wrapper_factory((job_config: MosaicJobConfig,))] sampling.py→[denoise((device: torch.device,dtype: torch.dtype,model: FluxModel,img_width: int,img_height: int,denoising_steps: int,clip_encodings: torch.Tensor,t5_encodings: torch.Tensor,enable_classifier_free_guidance: bool = False,empty_t5_encodings: torch.Tensor | None = None,empty_clip_encodings: torch.Tensor | None = None,classifier_free_guidance_scale: float | None = None,))→
{get_schedule,utils::unpack_latents,utils::pack_latents,utils::create_position_encoding_for_latents,utils::generate_noise_latent},generate_image((device: torch.device,dtype: torch.dtype,job_config: JobConfig,model: FluxModel,prompt: str | list[str],autoencoder: AutoEncoder,t5_tokenizer: BaseTokenizer,clip_tokenizer: BaseTokenizer,t5_encoder: FluxEmbedder,clip_encoder: FluxEmbedder,))→{denoise,utils::preprocess_data,utils::preprocess_data},get_schedule((num_steps: int,image_seq_len: int,base_shift: float = 0.5,max_shift: float = 1.15,shift: bool = True,))→{time_shift,get_lin_function},time_shift((mu: float,sigma: float,t: Tensor)),get_lin_function((x1: float = 256,y1: float = 0.5,x2: float = 4096,y2: float = 1.15)),save_image((name: str,output_dir: str,x: torch.Tensor,add_sampling_metadata: bool,prompt: str,))] siglip2.py→[forward((self,pixels_NLD: torch.Tensor,grid_hw: torch.Tensor))→{resize_positional_embeddings},__init__((self,args: Siglip2ModelArgs))[CTOR,DUNDER]→{math::attention},resize_positional_embeddings((pos_embs_HWD: torch.Tensor,spatial_shapes_N2: torch.Tensor,max_length: int,)),__init__((self,args: Siglip2ModelArgs))[CTOR,DUNDER],init_weights((self)),__init__((self,args: Siglip2ModelArgs))[CTOR,DUNDER],forward((self,x: torch.Tensor)),init_weights((self)),__init__((self,args: Siglip2ModelArgs))[CTOR,DUNDER],forward((self,x: torch.Tensor)),init_weights((self)),forward((self,x: torch.Tensor)),init_weights((self)),__init__((self,args: Siglip2ModelArgs))[CTOR,DUNDER],forward((self,pixel_values_NLD: torch.FloatTensor,pixel_masks_NL: torch.BoolTensor,grid_hw: torch.LongTensor,)),init_weights((self))] simpleMoE.py→[measure_performance((model: nn.Module,batch_size: int,seq_len: int,vocab_size: int,num_batches: int,device: str,))→{generate_sample_data,generate_sample_data,generate_sample_data},forward_mg_gemm((self,x: torch.Tensor))→{mg_grouped_gemm::grouped_gemm_forward,mg_grouped_gemm::grouped_gemm_forward},train_epoch((model: nn.Module,optimizer: torch.optim.Optimizer,batch_size: int,seq_len: int,vocab_size: int,num_batches: int,device: str,load_balance_coef: float = 0.01,))→{compute_load_balancing_loss,generate_sample_data},compare_methods((args))→{measure_performance,measure_performance},train_model((args))→{evaluate,train_epoch},evaluate((model: nn.Module,batch_size: int,seq_len: int,vocab_size: int,num_batches: int,device: str,))→{generate_sample_data},__init__((self,input_dim: int,num_experts: int,top_k: int = 2))[CTOR,DUNDER],forward((self,x: torch.Tensor)),__init__((self,input_dim: int,hidden_dim: int,output_dim: int))[CTOR,DUNDER],forward((self,x: torch.Tensor)),__init__((self,input_dim: int,hidden_dim: int,output_dim: int,num_experts: int,top_k: int = 2,use_mg_gemm: bool = False,))[CTOR,DUNDER],forward_manual_loop((self,x: torch.Tensor)),forward((self,x: torch.Tensor)),__init__((self,vocab_size: int,embed_dim: int,hidden_dim: int,num_experts: int,top_k: int = 2,use_mg_gemm: bool = False,))[CTOR,DUNDER],forward((self,x: torch.Tensor)),compute_load_balancing_loss((router_logits: torch.Tensor,num_experts: int))[HOT],generate_sample_data((batch_size: int,seq_len: int,vocab_size: int,device: str = "cuda"))] simple_fsdp.py→[data_parallel((model,device_mesh,mode="replicate",ac_mode: str = "none",mp_policy: Optional[MixedPrecisionPolicy] = None,shard_dim: int = 0,))→{_register_parametrization,desloc::tick},_register_parametrization((module: nn.Module,param_names: List[str],parametrization: nn.Module))→{math::rope},_distribute_dtensor((tensor: DTensor,device_mesh: DeviceMesh,dp_placements: Sequence[Placement],)),fsdp_policy(void),__init__((self,device_mesh,param_sharding,mode,regional_ac,mp_policy,))[CTOR,DUNDER],replicate_compute((self,x))[HOT],forward((self,x))] state_dict_adapter.py→[__init__((self,model_args: FluxModelArgs,hf_assets_path: str | None))[CTOR,DUNDER]→
{math::rope},__init__((self,model_args: TransformerModelArgs,hf_assets_path: str | None))[CTOR,DUNDER],__init__((self,model_args: Qwen3ModelArgs,hf_assets_path: str | None))[CTOR,DUNDER],to_hf((self,state_dict: dict[str,Any])),to_hf((self,state_dict: dict[str,Any])),from_hf((self,hf_state_dict: dict[str,Any])),from_hf((self,hf_state_dict: dict[str,Any])),_swap_scale_shift((self,weight)),to_hf((self,state_dict: dict[str,Any])),from_hf((self,hf_state_dict: dict[str,Any]))] test_create_m_indices.py→[] tiktoken.py→[encode((self,s: str,*,bos: bool,eos: bool,allowed_special: Optional[Union[Literal["all"],AbstractSet[str]]] = None,disallowed_special: Optional[Union[Literal["all"],Collection[str]]] = None,))→{metrics::setup,math::rope},encode_multimodal((self,sample: Mapping[str,Any]))→{metrics::setup},__init__((self,model_path: str))[CTOR,DUNDER],decode((self,t: Sequence[int])),build_tiktoken_tokenizer((job_config: JobConfig))[HOT]] tma_autotuning.py→[__init__((self,tma_size: int = 128))[CTOR,DUNDER],init_tma_descriptor((self,name: str)),fill_1d_tma_descriptor((self,name: str,ptr: int,dim: int,block_dim: int,element_size: int)),fill_2d_tma_descriptor((self,name: str,ptr: int,dim1: int,dim0: int,block_dim1: int,block_dim0: int,element_size: int,)),get_tma_descriptor_kernel_param((self,name: str)),early_config_prune((configs,named_args,dtsize=None,dtype=None,**kwargs))] tma_cuda_autotune.py→[early_config_prune((configs,args,**kwargs))→{integration_tests::main},__init__((self,tma_size: int = 128))[CTOR,DUNDER],init_tma_descriptor((self,name: str)),fill_1d_tma_descriptor((self,name: str,ptr: int,dim: int,block_dim: int,element_size: int)),fill_2d_tma_descriptor((self,name: str,ptr: int,dim1: int,dim0: int,block_dim1: int,block_dim0: int,element_size: int,)),get_tma_descriptor_kernel_param((self,name: str))] tokenizer.py→[build_mosaic_tokenizer((job_config: MosaicJobConfig,))[HOT]→{desloc::tick},__init__((self,model_path: str = "t5-small",max_length: int = 77,**hf_kwargs))[CTOR,DUNDER],_pad_and_chunk_tokens((self,tokens: List[int],max_length: int,pad_token: int)),get_vocab_size((self)),encode((self,text: str | list[str])),decode((self,t: List[int])),__init__((self,model_path: str = "t5-small",max_length: int = 77,**hf_kwargs))[CTOR,DUNDER],get_vocab_size((self)),encode((self,s: str | list[str],)),decode((self,t: List[int])),build_flux_tokenizer((job_config: JobConfig))[HOT]] train.py→[main(void)[ENTRY]→{s3_checkpoint::setup_s3_checkpointing,s3_checkpoint::setup_s3_checkpointing,s3_checkpoint::get_s3_checkpoint_wrapper_factory,utils::ensure_mosaic_spec},forward_backward_step((self,input_dict: dict[str,torch.Tensor],labels: torch.Tensor))→{utils::unpack_latents,utils::pack_latents,utils::create_position_encoding_for_latents,utils::preprocess_data},__init__((self,job_config: JobConfig))[CTOR,DUNDER]→{parallelize::parallelize_encoders,autoencoder::load_ae}] train_ds_dev.py→[run_full_model((mesh: DeviceMesh,))] train_ds_real.py→[run_full_model((config: JobConfig,))→{next_batch,components::build_metrics_processor,mm_dataset::build_mm_dataloader,hf_tokenizer::get_hf_tokenizer,parallelize_deepseek::parallelize_deepseek},cross_entropy_loss((pred: torch.Tensor,labels: torch.Tensor)),next_batch((data_iterator: Iterable,metrics_processor))] train_spec.py→[get_train_spec((name: str))→{_transform_train_spec,_transform_train_spec},_transform_train_spec((original_spec: TrainSpec)),register_train_spec((train_spec: ForgeTrainSpec))] transform.py→[__call__((self,image: torch.Tensor))[DUNDER]→{utils::tile_crop,utils::resize_with_pad,utils::get_canvas_best_fit},__init__((self,*,image_mean: Optional[List[float]] = None,image_std: Optional[List[float]] = None,possible_resolutions: Optional[List[Tuple[int,int]]] = None,tile_size: int = 224,max_num_tiles: Optional[int] = 4,dtype: torch.dtype = torch.bfloat16,resample: str = "bilinear",resize_to_max_canvas: bool = False,))[CTOR,DUNDER]→
{utils::find_supported_resolutions}] triton_barrier.py→[] triton_on_device_all_to_all_v.py→[_on_device_all_to_all_v((output: torch.Tensor,output_splits: torch.Tensor,input: torch.Tensor,input_splits: torch.Tensor,group: dist.ProcessGroup = dist.group.WORLD,BLOCKS_PER_REMOTE_RANK=8,UNROLL_FACTOR: int = 8,BLOCK_SIZE: int = 16384,))] triton_utils.py→[] unit_test_backwards.py→[_run_grouped_gemm_backward_test((self,shape: Tuple[int,int,int,int],device: torch.device,dtype: torch.dtype = torch.bfloat16,atol: float = 1e-5,rtol: float = 1.6e-2,))→{reference_utils::analyze_tensor_differences,reference_utils::analyze_tensor_differences,reference_utils::compute_reference_backward,mg_grouped_gemm::grouped_gemm_backward,reference_utils::analyze_tensor_differences,reference_utils::compute_reference_forward},setUp((self))] unit_test_cg.py→[benchmark_forward((self,M_total,K,N,num_experts,group_size_m,num_runs=10))→{integration_tests::main,cg_forward::cg_grouped_gemm_forward,cg_forward::cg_grouped_gemm_forward},benchmark_backward((self,M_total,K,N,num_experts,group_size_m,num_runs=5))→{integration_tests::main,cg_backward::cg_grouped_gemm,cg_backward::cg_grouped_gemm},verify_forward((self,M_total,K,N,num_experts,group_size_m,print_stats=False))→{integration_tests::main,cg_forward::cg_grouped_gemm_forward},verify_backward((self,M_total,K,N,num_experts,group_size_m,print_stats=False))→{integration_tests::main,cg_backward::cg_grouped_gemm},run_tests((run_benchmarks=False))] unit_test_forwards.py→[_run_grouped_gemm_test((self,shape: Tuple[int,int,int,int],device: torch.device,dtype: torch.dtype = torch.bfloat16,atol: float = 1e-5,rtol: float = 1.6e-2,))→{mg_grouped_gemm::grouped_gemm_forward},setUp((self))] utils.py→[resize_with_pad((image: torch.Tensor,target_size: Tuple[int,int],resample: torchvision.transforms.InterpolationMode,max_size: Optional[int] = None,))→{_pad_image_top_left,_get_max_res_without_distortion,integration_tests::main,integration_tests::main,integration_tests::main,integration_tests::main},_get_max_res_without_distortion((image_size: Tuple[int,int],target_size: Tuple[int,int],))→{integration_tests::main,integration_tests::main},_get_factors((n: int))→{metrics::setup},find_supported_resolutions((max_num_tiles: int,tile_size: int))→{_get_factors},preprocess_data((# arguments from the recipe device: torch.device,dtype: torch.dtype,*,# arguments from the config autoencoder: Optional[AutoEncoder],clip_encoder: FluxEmbedder,t5_encoder: FluxEmbedder,batch: dict[str,Tensor],)),tile_crop((image: torch.Tensor,tile_size: int)),generate_noise_latent((bsz: int,height: int,width: int,device: str | torch.device,dtype: torch.dtype,seed: int | None = None,)),create_position_encoding_for_latents((bsz: int,latent_height: int,latent_width: int,position_dim: int = 3)),pack_latents((x: Tensor)),_pad_image_top_left((image: torch.Tensor,target_size: Tuple[int,int],)),unpack_latents((x: Tensor,latent_height: int,latent_width: int)),get_canvas_best_fit((image: torch.Tensor,possible_resolutions: torch.Tensor,resize_to_max_canvas: bool)),load_image((image_loc: Union[Path,str]))] validate.py→[__init__((self,job_config: JobConfig,dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer,parallel_dims: ParallelDims,loss_fn: LossFunction,validation_context: Generator[None,None,None],maybe_enable_amp: Generator[None,None,None],metrics_processor: MetricsProcessor | None = None,pp_schedule: _PipelineSchedule | None = None,pp_has_first_stage: bool | None = None,pp_has_last_stage: bool | None = None,))[CTOR,DUNDER]→
{tokenizer::build_flux_tokenizer,flux_dataset::build_flux_validation_dataloader},__init__((self,job_config: MosaicJobConfig,dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer,parallel_dims: ParallelDims,loss_fn: LossFunction,validation_context: Generator[None,None,None],maybe_enable_amp: Generator[None,None,None],metrics_processor: MetricsProcessor,pp_schedule: _PipelineSchedule | None = None,pp_has_first_stage: bool | None = None,pp_has_last_stage: bool | None = None,))[CTOR,DUNDER]→{dataloader::build_mosaic_validation_dataloader},build_mosaic_validator((job_config: MosaicJobConfig,dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer,parallel_dims: ParallelDims,loss_fn: LossFunction,validation_context: Generator[None,None,None],maybe_enable_amp: Generator[None,None,None],metrics_processor: MetricsProcessor,pp_schedule: _PipelineSchedule | None = None,pp_has_first_stage: bool | None = None,pp_has_last_stage: bool | None = None,))[HOT],flux_init((self,device: torch.device,_dtype: torch.dtype,autoencoder: AutoEncoder,t5_encoder: FluxEmbedder,clip_encoder: FluxEmbedder,)),build_flux_validator((job_config: JobConfig,dp_world_size: int,dp_rank: int,tokenizer: BaseTokenizer,parallel_dims: ParallelDims,loss_fn: LossFunction,validation_context: Generator[None,None,None],maybe_enable_amp: Generator[None,None,None],metrics_processor: MetricsProcessor | None = None,pp_schedule: _PipelineSchedule | None = None,pp_has_first_stage: bool | None = None,pp_has_last_stage: bool | None = None,))[HOT]] 
### UTILS
NODES:29 CALL_DEPTH:5

image.py→[_smart_resize((height: int,width: int,factor: int,# should be equal patch_size * merge_size max_patch_per_image: int,min_patch_per_image: int = 1,))→{integration_tests::main,integration_tests::main,integration_tests::main,integration_tests::main},_resize_image_by_patch_count((image: Image.Image,max_patch_per_image: int,patch_size: int,merge_size: int,min_patch_per_image: int = 1,))→{_smart_resize,_smart_resize,_smart_resize,integration_tests::main},process_image((image: str | bytes | Image.Image,patch_size: int = 16,merge_size: int = 1,max_patch_per_image: int = 256,min_patch_per_image: int = 1,))→{_resize_image_by_patch_count},calculate_image_tokens((image: Image.Image | torch.Tensor,patch_size: int,spatial_merge_size: int,)),convert_to_patches((pixel_values: torch.Tensor,patch_size: int,temporal_patch_size: int = 1,)),pad_patches((patches: torch.Tensor,grids: torch.Tensor,max_patches: int,)),pad_empty_images_to_target_batch_size((patches: torch.Tensor,grids: torch.Tensor,max_images: int,))] packing.py→[__init__((self,max_seq_length: int,buffer_size: int = 100,batch_size: int = 8,))[CTOR,DUNDER],_pack_buffered_samples((self)),add_sample((self,sample: dict[str,Any])),has_batch_ready((self)),get_next_batch((self))] text.py→[pad_text_batch((input_ids: torch.Tensor,labels: torch.Tensor,seq_len: int,padding_idx: int = 0,ignore_idx: int = -100,)),pad_input_ids_and_labels_to_target_batch_size((input_ids: torch.Tensor,labels: torch.Tensor,target_batch_size: int,padding_idx: int = 0,ignore_idx: int = -100,)),process_text_with_images((text: list[str],image_tokens: list[tuple[int,int,int]],# [(total,width,height),...] tokenizer,special_tokens,add_eos: bool = True,))] 

## DEPENDENCY_PATTERNS

### EDGE_PATTERNS
Contains: 505 edges
Call: 492 edges

### CROSS_CLUSTER_FLOW
DATA_MODELS→UTILITY_LAYER: 8
TESTS→UTILITY_LAYER: 83
UTILITY_LAYER→DATA_MODELS: 1
UTILITY_LAYER→TESTS: 57
UTILITY_LAYER→UTILS: 3
UTILS→TESTS: 5

